<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>100 Days Of Kubernetes</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><li class="part-title">Overview</li><li class="chapter-item expanded "><a href="overview.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="contributors.html"><strong aria-hidden="true">2.</strong> Contributors</a></li><li class="chapter-item expanded affix "><li class="part-title">Let's get started</li><li class="chapter-item expanded "><a href="start/intro-to-k8s.html"><strong aria-hidden="true">3.</strong> Introduction to Kubernetes</a></li><li class="chapter-item expanded "><a href="start/architecture-overview.html"><strong aria-hidden="true">4.</strong> Kubernetes Architecture</a></li><li class="chapter-item expanded "><a href="start/pods.html"><strong aria-hidden="true">5.</strong> Kubernetes Pods</a></li><li class="chapter-item expanded "><a href="start/set-up-cluster.html"><strong aria-hidden="true">6.</strong> Setup Kubernetes Cluster</a></li><li class="chapter-item expanded "><a href="start/run-pod.html"><strong aria-hidden="true">7.</strong> Run Pods</a></li><li class="chapter-item expanded "><a href="start/replicas.html"><strong aria-hidden="true">8.</strong> Replica Set</a></li><li class="chapter-item expanded "><a href="start/deployment.html"><strong aria-hidden="true">9.</strong> Kubernetes Deployment</a></li><li class="chapter-item expanded "><a href="start/namespaces.html"><strong aria-hidden="true">10.</strong> Namespaces</a></li><li class="chapter-item expanded "><a href="start/configmaps.html"><strong aria-hidden="true">11.</strong> ConfigMaps</a></li><li class="chapter-item expanded affix "><li class="part-title">Networking</li><li class="chapter-item expanded "><a href="networking/service.html"><strong aria-hidden="true">12.</strong> Kubernetes Service</a></li><li class="chapter-item expanded "><a href="networking/ingress.html"><strong aria-hidden="true">13.</strong> Kubernetes Ingress</a></li><li class="chapter-item expanded "><a href="networking/servicemesh.html"><strong aria-hidden="true">14.</strong> Service Mesh</a></li><li class="chapter-item expanded affix "><li class="part-title">Storage</li><li class="chapter-item expanded "><a href="storage/volumes.html"><strong aria-hidden="true">15.</strong> Kubernetes Volumes</a></li><li class="chapter-item expanded affix "><li class="part-title">Kubernetes Clusters Overview and Use</li><li class="chapter-item expanded "><a href="kubernetes-clusters/kind.html"><strong aria-hidden="true">16.</strong> KinD Cluster</a></li><li class="chapter-item expanded "><a href="kubernetes-clusters/k3s.html"><strong aria-hidden="true">17.</strong> K3s and K3sup</a></li><li class="chapter-item expanded affix "><li class="part-title">Templating & IaC</li><li class="chapter-item expanded "><a href="templating/kustomize.html"><strong aria-hidden="true">18.</strong> Kustomize</a></li><li class="chapter-item expanded "><a href="templating/terraform.html"><strong aria-hidden="true">19.</strong> Terraform</a></li><li class="chapter-item expanded "><a href="templating/crossplane.html"><strong aria-hidden="true">20.</strong> Crossplane</a></li><li class="chapter-item expanded affix "><li class="part-title">Helm</li><li class="chapter-item expanded "><a href="helm/partone.html"><strong aria-hidden="true">21.</strong> Helm Part 1</a></li><li class="chapter-item expanded "><a href="helm/partwo.html"><strong aria-hidden="true">22.</strong> Helm Part 2</a></li><li class="chapter-item expanded "><a href="helm/partthree.html"><strong aria-hidden="true">23.</strong> Helm Part 3</a></li><li class="chapter-item expanded affix "><li class="part-title">Cloud Native Tools and Platforms</li><li class="chapter-item expanded "><a href="tools/k9s.html"><strong aria-hidden="true">24.</strong> k9s</a></li><li class="chapter-item expanded "><a href="tools/knative.html"><strong aria-hidden="true">25.</strong> Knative</a></li><li class="chapter-item expanded "><a href="tools/argo.html"><strong aria-hidden="true">26.</strong> GitOps and Argo</a></li><li class="chapter-item expanded "><a href="tools/linkerd.html"><strong aria-hidden="true">27.</strong> Linkerd</a></li><li class="chapter-item expanded affix "><li class="part-title">Observability</li><li class="chapter-item expanded "><a href="observaility/prometheus.html"><strong aria-hidden="true">28.</strong> Prometheus</a></li><li class="chapter-item expanded "><a href="observability/prometheus-exporter.html"><strong aria-hidden="true">29.</strong> Prometheus Exporter</a></li><li class="chapter-item expanded affix "><li class="part-title">Advanced Topics</li><li class="chapter-item expanded "><a href="advanced/operators.html"><strong aria-hidden="true">30.</strong> Kubernetes Operators</a></li><li class="chapter-item expanded affix "><li class="part-title">Tutorials and Practice</li><li class="chapter-item expanded "><a href="tutorials/serverless.html"><strong aria-hidden="true">31.</strong> Serverless</a></li><li class="chapter-item expanded "><a href="tutorials/ingress-from-scratch.html"><strong aria-hidden="true">32.</strong> Ingress from scratch</a></li><li class="chapter-item expanded "><a href="tutorials/istio-from-scratch.html"><strong aria-hidden="true">33.</strong> Istio from scratch</a></li><li class="chapter-item expanded "><a href="tutorials/deploy-to-civo.html"><strong aria-hidden="true">34.</strong> Deploy to CIVO</a></li><li class="chapter-item expanded affix "><li class="part-title">Troubleshooting K8s</li><li class="chapter-item expanded "><a href="troubleshooting/crashloopbackoff.html"><strong aria-hidden="true">35.</strong> CrashLoopBackOff</a></li><li class="chapter-item expanded affix "><li class="part-title">Glossary</li><li class="chapter-item expanded "><a href="glossary/terminologyprimer.html"><strong aria-hidden="true">36.</strong> Terminology Primer</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">100 Days Of Kubernetes</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        <a href="https://github.com/100daysofkubernetes/100DaysOfKubernetes" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="welcome-to-100-days-of-kubernetes"><a class="header" href="#welcome-to-100-days-of-kubernetes">Welcome to 100 Days Of Kubernetes!</a></h1>
<p>100 Days of Kubernetes is the challenge in which we aim to learn something new related to Kubernetes each day across 100 Days!!!</p>
<blockquote>
<p>You Can Learn Anything </p>
</blockquote>
<p>A lot of times it is just about finding the right resources and the right learning path. </p>
<h2 id="what-can-you-find-in-this-book"><a class="header" href="#what-can-you-find-in-this-book">What can you find in this book</a></h2>
<p>This book provides a list of resources from across the cloud native space to learn about and master Kubernetes. Whether you are just getting started with Kubernetes or you are already using Kubernetes, I am sure that you will find a way to use the resources or contribute :)</p>
<h2 id="just-a-note-of-caution"><a class="header" href="#just-a-note-of-caution">Just a note of caution</a></h2>
<p>The notes in this book depend on the community to improve and become accurate. Everything detailed so far is someone's personal understanding at each point in time learning about the respective topics. </p>
<p>Help us improve the notes. </p>
<h2 id="100-days-of-kubernetes"><a class="header" href="#100-days-of-kubernetes">100 Days Of Kubernetes</a></h2>
<p>So what is this challenge all about? The idea (and credit) goes to two existing communities :</p>
<ol>
<li><a href="https://www.100daysofcode.com/">100DaysOfCode</a></li>
<li><a href="https://www.100daysofcloud.com/">100DaysOfCloud</a></li>
</ol>
<p>The idea is that you publicly commit to learning something new. This way, the community can support and motivate you. Every day that you are doing the challenge, just post a tweet highlighting what you learned with the #100DaysOfKubernetes hashtag. </p>
<p>Additionally, creating your own content based on what you are learning can be highly valuable to the community. For example, once you wrote a blog post on Kubernetes ReplicaSets or similar, you could add it to this book.</p>
<p>The goal is to make this a community-driven project.</p>
<h3 id="where-to-get-started"><a class="header" href="#where-to-get-started">Where to get started</a></h3>
<p><strong>Fork the example repository</strong>
We suggest you to fork the journey repository. Every day that you work on the challenge, you can make changes to the repository to detail what you have been up to.
The progress will then be tracked on your GitHub.</p>
<p><strong>Tweet about your progress</strong>
Share your learnings and progress with the <a href="https://twitter.com/search?q=%23100DaysOfKubernetes&amp;src=typed_query&amp;f=live">#100DaysOfKubernetes</a> on Twitter.</p>
<p><strong>Join the community</strong>
We have a channel in this <a href="https://discord.gg/bs2sy3Ppm7">Discord channel</a> -- come say hi, ask questions, and contribute!</p>
<h2 id="contribute"><a class="header" href="#contribute">Contribute</a></h2>
<p>You can find more information on contributions in the <a href="https://github.com/100daysofkubernetes/100DaysOfKubernetes">README</a> of this GitHub repository.</p>
<h2 id="structure-of-the-book"><a class="header" href="#structure-of-the-book">Structure of the book</a></h2>
<p>The book is divided into several higher-level topics. Each topic has several sub-topics that are individual pages or chapters.</p>
<p>Those chapters have a similar structure:</p>
<p><strong>Title</strong></p>
<p>The title of the page</p>
<p><strong>100Days Resources</strong></p>
<p>This section highlights a list of community resources specific to the topics that is introduced.
Additionally, this is where you can include your own content, videos and blog articles, from your 100DaysOfKubernetes challenge.</p>
<p><strong>Learning Resources</strong></p>
<p>A list of related learning resources. Different to '100Days Resources', these do not have to be specific to 100DaysOfKubernetes.</p>
<p><strong>Example Notes</strong></p>
<p>This section provides an introduction to the topics. The goal is to advance each topics over time. 
When you are first time learning about a topic, it is usually best to take your own notes but sometimes having a starting point and examples is helpful.</p>
<h2 id="list-of-example-notes-by-the-community"><a class="header" href="#list-of-example-notes-by-the-community">List of Example Notes by the Community</a></h2>
<ul>
<li><a href="https://devops.anaisurl.com/kubernetes">Anais Urlichs' public Notion</a></li>
<li><a href="https://github.com/rishabkumar7/LearningKubernetes">Rishab Kumar's GitHub repository</a></li>
</ul>
<h1 id="about-the-contributors"><a class="header" href="#about-the-contributors">About the contributors</a></h1>
<p>If you have contributed to this book, please add yourself to the list :)</p>
<h4 id="anais-urlichs"><a class="header" href="#anais-urlichs">Anais Urlichs</a></h4>
<ul>
<li><a href="https://github.com/AnaisUrlichs">GitHub</a></li>
<li><a href="https://www.youtube.com/c/AnaisUrlichs">YouTube</a></li>
<li><a href="https://twitter.com/urlichsanais">Twitter</a></li>
<li><a href="https://devops.anaisurl.com">DevOps</a></li>
</ul>
<h1 id="what-is-kubernetes-and-why-do-we-want-it"><a class="header" href="#what-is-kubernetes-and-why-do-we-want-it">What is Kubernetes and why do we want it?</a></h1>
<h1 id="100days-resources"><a class="header" href="#100days-resources">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/fCpv7xSEyEI">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources"><a class="header" href="#learning-resources">Learning Resources</a></h1>
<ul>
<li><a href="https://youtu.be/VnvRFRk_51k">What is Kubernetes</a></li>
<li><a href="https://youtu.be/umXEmn3cMWY">Kubernetes architecture explained</a></li>
<li><a href="https://youtu.be/kBF6Bvth0zw">Container Orchestration Explained</a></li>
</ul>
<h1 id="example-notes"><a class="header" href="#example-notes">Example Notes</a></h1>
<p>Kubernetes builds on Borg and the lessons learned by Google for operating Borg for over 15 years. </p>
<p>Borg was used by Google internally to manage its systems.</p>
<p>For more information here is the full paper <a href="https://research.google/pubs/pub43438/">https://research.google/pubs/pub43438/</a></p>
<p><strong>What is Kubernetes?</strong></p>
<p>Kubernetes is an open-source container Orchestration Framework.</p>
<p>At its root, it manages containers — to manage applications that are made of of containers- physical machines, virtual machines, hybrid environments</p>
<p>According to the <a href="https://kubernetes.io/">kubernetes.io</a> website, Kubernetes is:</p>
<p><em>&quot;an open-source system for automating deployment, scaling, and management of containerized applications&quot;</em>.</p>
<p><strong>What problems does it solve</strong></p>
<ul>
<li>Following the trend from Monolithic to Microservices — traditionally, an application would be a Monolithic application — which requires the hardware to scale with the application. In comparison, Kubernetes deploys a large number of small web servers.</li>
<li>Containers are the perfect host for small self-contained applications</li>
<li>Applications comprised of 100s of containers — managing those with scripts can be really difficult and even impossible</li>
<li>Kubernetes helps us with the following: connecting containers across multiple hosts, scaling them, deploying applications without downtime, and service discovery among several other aspects</li>
</ul>
<p>The benefits of splitting up your application from Monolithic into Microservices is that they are easier to maintain. For instance:</p>
<p>Instead of a large Apache web server with many httpd daemons responding to page requests, there would be many nginx servers, each responding.</p>
<p>Additionally, it allows us to separate matters of concerns within our application i.e. decoupling the architecture based on responsibilies.</p>
<p>Additionally, Kubernetes is an essential part of </p>
<ul>
<li>Continuous Integration</li>
<li>Continuous Delivery</li>
</ul>
<p><strong>Orchestration tools such as Kubernetes offer:</strong></p>
<ul>
<li>High availability</li>
<li>Scalability: Applications have higher performance e.g. load time</li>
<li>Disaster Recovery: The architecture has to have a way to back-up the data and restore the state of the application at any point in time</li>
</ul>
<p><strong>How does the architecture actually look like:</strong></p>
<ul>
<li>You have a master node: Runs several Kubernetes processes that are necessary to run the container's processes — e.g. an <strong>API server</strong> ⇒ the entry point to the Kubernetes cluster (UI, API, CLI); then it needs to have a <strong>Controller Manager</strong> ⇒ keeps track of what is happening e.g. detects when a pod dies ⇒ detects state changes; and lastly, it contains a <strong>Scheduler</strong> ⇒ this ensures the Pod placement (more on pods later); <strong>etcd database</strong> ⇒ key-value storage that holds the state of the Kubernetes cluster at any point in time; and the last thing that is needed is the <strong>Virtual Network</strong> that spans across all the nodes in the cluster</li>
<li>And worker nodes: contains Containers of different applications; here is where the actual work is happening</li>
</ul>
<p>Note that worker nodes are usually much bigger because they are running the containers. The master node will only run a selection of processes. </p>
<p>Each node will have multiple pods with containers running on them. 3 processes have to be present on all nodes</p>
<ul>
<li>Container Runtime e.g. Docker</li>
<li>Kubelet; which is a process of Kubernetes itself that interacts with both, the container runtime and the node — it is responsible for taking our configuration and starting a pod inside the node</li>
</ul>
<p>Usually a Kubernetes cluster has several nodes running in parallel.</p>
<ul>
<li>nodes communicate with services between each other. Thus, the third process that has to be installed on every node is Kube Proxy that forwards requests between nodes and pods— the communication works in a performant way with low overhead. Requests are forwarded to the closest pod.</li>
</ul>
<p><strong>Kubernetes Concepts — Pods</strong></p>
<p>Pod ⇒ the smallest unit that you, as a Kubernetes user will configure</p>
<ul>
<li>Pods are a wrapper of a container; on each worker node, we will have multiple containers</li>
<li>Usually, you would have one application, one container per pod</li>
<li>Each Pod has its own IP address — thus, each pod is its own self-containing server. This allows pods to communicate with each other — through their internal IP addresses</li>
</ul>
<p>Note that we don't actually create containers inside the Kubernetes cluster but we work with the pods that are an abstraction layer over the containers. Pods manage the containers without our intervention</p>
<p>In case a pod dies, it can be configured to recreate (as per restart policy) and it will get a new IP address. </p>
<p>A service is used as an alternative for a pods IP address. Resulting, there is a service that sits in front of each pod that abstracts away the dynamic IP address. Resulting, the lifecycle of a pod and the IP address are not tied to each other-</p>
<p>If a pod behind the service dies, it gets recreated.</p>
<p>A service has two main functions:</p>
<ul>
<li>Providing an IP address to the pod(s)</li>
<li>It is a Loadbalancer (what the hack is that 😆 — more on this later)</li>
</ul>
<p><strong>How do we create those components</strong></p>
<ul>
<li>All configuration goes through the master node — UI, API, CLI, all talk to the APi server within the master node — they send the configuration request to the API server</li>
<li>The configuration is usually in YAML format ⇒ a blue print for creating pods. The Kubernetes agents convert the YAML to JSON prior to persistence to the database.</li>
<li>The configuration of the requirements are in a declarative format. This will allow it to compare the desired state to the actual state (more on this later)</li>
</ul>
<p><strong>Developer Workflow:</strong></p>
<ul>
<li>Create Docker images based on your application</li>
<li>Use Docker and Kubernetes</li>
<li>A CI pipeline to build, test, and verify Docker images</li>
<li>&quot;You must be able to perform rolling updates and rollbacks, and eventually tear down the resource when no longer needed.&quot; — the course</li>
</ul>
<p>This requires flexible and easy to use network storage.</p>
<h1 id="kubernetes-architecture"><a class="header" href="#kubernetes-architecture">Kubernetes Architecture</a></h1>
<h1 id="100days-resources-1"><a class="header" href="#100days-resources-1">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/oqWgMc9yYcc">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-1"><a class="header" href="#learning-resources-1">Learning Resources</a></h1>
<ul>
<li><a href="https://youtu.be/mNK14yXIZF4">How Kubernetes deployments work</a></li>
</ul>
<h1 id="example-notes-1"><a class="header" href="#example-notes-1">Example Notes</a></h1>
<p>We can divide the responsibilities within a Kubernetes cluster between a main node and worker nodes. Note that in small clusters we may have one node that takes the responsibilities of both.</p>
<h2 id="main-node"><a class="header" href="#main-node">Main Node</a></h2>
<p>Where does the orchestration from Kubernetes come in? These are some characteristics that make up Kubernetes as a container orchestration system:</p>
<ul>
<li>Managed by several operators and controllers — will look at operators and controllers later one. <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/">Operators</a> make use of custom resources to manage an application and their components.</li>
<li>&quot;Each controller interrogates the kube-apiserver for a particular object state, modifying the object until the declared state matches the current state.&quot; In short, <a href="https://kubernetes.io/docs/concepts/architecture/controller/">controllers</a> are used to ensure a process is happening in the desired way.</li>
<li>&quot;The <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a> is a controller which deploys and restarts containers, Docker by default, until the requested number of containers is running.&quot; In short, its purpose is to ensure a specific number of pods are running.</li>
</ul>
<p>Note that those concepts are details in further sections of the book.</p>
<p>There are several other API objects which can be used to deloy pods. A DaemonSet will ensure that a single pod is deployed on every node. These are often used for logging and metrics. A StatefulSet can be used to deploy pods in a particular order, such that following pods are only deployed if previous pods report a ready status.</p>
<p>API objects can be used to know</p>
<ul>
<li>
<p>What containerized applications are running (and on which nodes)</p>
</li>
<li>
<p>The resources available to those applications</p>
</li>
<li>
<p>The policies around how those applications behave, such as restart policies, upgrades, and fault-tolerance</p>
</li>
<li>
<p><strong>kube-apiserver</strong></p>
<ul>
<li>Provides the front-end to the cluster's shared state through which all components interact</li>
<li>Is central to the operation of the Kubernetes cluster.</li>
<li>Handles internal and external traffic</li>
<li>The only agent that connects to the etcd database</li>
<li>Acts as the master process for the entire cluster</li>
<li>Provides the out-wards facing state for the cluster's state</li>
<li>Each API call goes through three steps: 
authentication, authorization, and several admission controllers.</li>
</ul>
</li>
<li>
<p><strong>kube-scheduler</strong></p>
<ul>
<li>The Scheduler sees a request for running a container and will run the container in the best suited node</li>
<li>When a new pod has to be deployed, the kube-scheduler determines through an algorithm to which node the pod should be deployed</li>
<li>If the pod fails to be deployed, the kube-scheduler will try again based on the resources available across nodes</li>
<li>A user could also determine which node the pod should be deployed to —this can be done through a custom scheduler</li>
<li>Nodes that meet scheduling requirements are called feasible nodes.</li>
<li>You can find more <a href="https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/scheduler.go">details about the scheduler on GitHub.</a></li>
</ul>
</li>
<li>
<p><strong>etcd Database</strong></p>
<ul>
<li>The state of the cluster, networking, and other persistent information is kept in an etcd database</li>
<li>etcd is a consistent and highly-available key value store used as Kubernetes' backing store for all cluster data</li>
<li>Note that this database does not change; previous entries are not modified and new values are appended at the end.</li>
<li>Once an entry can be deleted, it will be labelled for future removal by a compaction process. It works with curl and other HTTP libraries and provides reliable watch queries.</li>
<li>Requests to update the database are all sent through the kube api-server; each request has its own version number which allows the etcd to distinguish between requests. If two requests are sent simultaneously, the second request would then be flagged as invalid with a 409 error and the etcd will only update as per instructed by the first request.</li>
<li>Note that it has to be specifically configured</li>
</ul>
</li>
<li>
<p><strong>Other Agents</strong></p>
<ul>
<li>The kube-controller-manager is a core control loop daemon which 
interacts with the kube-apiserver to determine the state of the cluster.
If the state does not match, the manager will contact the necessary 
controller to match the desired state. 
It is also responsible to interact with third-party cluster management and reporting.</li>
<li>The cluster has several controllers in 
use, such as endpoints, namespace, and replication. The full list has 
expanded as Kubernetes has matured. Remaining in beta as of 
v1.16, the cloud-controller-manager interacts with agents outside of the
cloud. It handles tasks once handled by kube-controller-manager. This 
allows faster changes without altering the core Kubernetes control 
process. Each kubelet must use the <strong>--cloud-provider-external</strong> settings passed to the binary.</li>
</ul>
</li>
<li>
<p>There are several add-ons which have become essential to a typical 
production cluster, such as DNS services. Others are third-party 
solutions where Kubernetes has not yet developed a local component, such
as cluster-level logging and resource monitoring.</p>
</li>
</ul>
<p>&quot;Each node in the cluster runs two processes: a kubelet and a kube-proxy.&quot;</p>
<p>Kubelet: handles requests to the containers, manages resources and looks after the local nodes</p>
<p>The Kube-proxy creates and manages networking rules — to expose container on the network</p>
<ul>
<li>
<p><strong>kubelet</strong></p>
<p>Each node has a container runtime e.g. the Docker engine installed. The kubelet is used to interact with the Docker Engine and to ensure that the containers that need to run are actually running.</p>
<p>Additionally, it does a lot of the work for the worker nodes; such as accepting API calls for the Pods specifications that are either provided in JSON or YAML. </p>
<p>Once the specifications are received, it will take care of configuring the nodes until the specifications have been met</p>
<p>Should a Pod require access to <strong>storage</strong>, <strong>Secrets</strong> or <strong>ConfigMaps</strong>, the kubelet will ensure access or creation. It also sends back status to the kube-apiserver for eventual persistence.</p>
</li>
<li>
<p><strong>kube-proxy</strong></p>
<p>The kube-proxy is responsible for managing the network connectivity to containers. To do that is has iptables. </p>
<p>&quot;<a href="https://en.wikipedia.org/wiki/Iptables">iptables</a> is a user-space utility program that allows a system administrator to configure the IP packet filter rules of the Linux kernel firewall, implemented as different Netfilter modules.&quot;</p>
<p>Additional options are the use of namespaces to monitor services and endpoints, or ipvs to replace the use of iptables</p>
</li>
</ul>
<p>To easily manage thousands of Pods across hundreds of nodes can be a difficult task to manage. To make management easier, we can use labels, arbitrary strings which become part of the object metadata. These can then be used when checking or changing the state of objects without having to know individual names or UIDs. Nodes can have taints to discourage Pod assignments, unless the Pod has a toleration in its metadata.</p>
<p><strong>Multi-tenancy</strong></p>
<p>When multiple-users are able to access the same cluster</p>
<p><strong>Additional security measures can be taken through either of the following:</strong></p>
<ol>
<li>
<p><strong>Namespaces</strong>: Namespaces can be used to &quot;divide the cluster&quot;; additional permissions can be set on each namespace; note that two objects cannot have the same name in the same namespace</p>
</li>
<li>
<p><strong>Context</strong>: A combination of user, cluster name and namespace; this allows you to restrict the cluster between permissions and restrictions. This information is referenced by ~/.kube/config</p>
<p>Can be checked with</p>
<pre><code class="language-jsx">kubectl config view
</code></pre>
</li>
<li>
<p><strong>Resource Limits:</strong> Provide a way to limit the resources that are provided for a specific pod</p>
</li>
<li>
<p><strong>Pod Security Policies</strong>: &quot;A policy to limit the ability of pods to elevate permissions or modify the node upon which they are scheduled. This wide-ranging limitation may prevent a pod from operating properly. The use of PSPs may be replaced by Open Policy Agent in the future.&quot;</p>
</li>
<li>
<p><strong>Network Policies:</strong> The ability to have an inside-the-cluster firewall. 
Ingress and Egress traffic can be limited according to namespaces and labels as well as typical network traffic characteristics.</p>
</li>
</ol>
<h1 id="kubernetes-pods"><a class="header" href="#kubernetes-pods">Kubernetes Pods</a></h1>
<h1 id="100days-resources-2"><a class="header" href="#100days-resources-2">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/fCpv7xSEyEI">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-2"><a class="header" href="#learning-resources-2">Learning Resources</a></h1>
<ul>
<li><a href="https://www.vmware.com/topics/glossary/content/kubernetes-pods">https://www.vmware.com/topics/glossary/content/kubernetes-pods</a></li>
<li><a href="https://cloud.google.com/kubernetes-engine/docs/concepts/pod">https://cloud.google.com/kubernetes-engine/docs/concepts/pod</a></li>
<li><a href="https://kubernetes.io/docs/concepts/workloads/pods/">https://kubernetes.io/docs/concepts/workloads/pods/</a></li>
</ul>
<h1 id="example-notes-2"><a class="header" href="#example-notes-2">Example Notes</a></h1>
<p><strong>Overview</strong></p>
<p>Pods are the smallest unit in a Kubernetes cluster; which encompass one or more application ⇒ it represents processes running on a cluster. Pods are used to manage your application instance.</p>
<p>Here is a quick summary of what a Pod is and its responsibilities:</p>
<ul>
<li>In our nodes, and within our Kubernetes cluster, the smallest unit that we can work with are pods.</li>
<li>Containers are part of a larger object, which is the pod. We can have one or multiple containers within a pod.</li>
<li>Each container within a pod share an IP address, storage and namespace — each container usually has a distinct role inside the pod.</li>
<li>Note that pods usually operate on a higher level than containers; they are more of an abstraction of the processes within a container than the container itself.</li>
<li>A pod can also run multiple containers; all containers are started in parallel ⇒ this makes it difficult to know which process started before another.</li>
<li>Usually, one pod is used per container process; reasons to run two containers within a pod might be logging purposes.</li>
<li><em>initContainers</em> can be used to ensure some containers are ready before others in a pod. To support a single process running in a container, you may need logging, a proxy, or special adapter. These tasks are often handled by other containers in the same Pod.</li>
<li>Usually each pod has one IP address.</li>
<li>You may find the term <em>sidecar</em> for a container dedicated to performing a helper task, like handling logs and responding to requests, as the primary application container may have this ability.</li>
</ul>
<p><strong>Running multiple containers in one pod</strong></p>
<p>An example for running multiple containers within a pod would be an app server pod that contains three separate containers: the app server itself, a monitoring adapter, and a logging adapter. Resulting, all containers combines will provide one service.</p>
<p>In this case, the logging and monitoring container should be shared across all projects within the organisation.</p>
<p><strong>Replica sets</strong></p>
<p>Each pod is supposed to run a single instance of an application. If you want to scale your application horizontally, you can create multiple instance of that pod.</p>
<p>It is usually not recommended to create pods manually but instead use multiple instances of the same application; these are then identical pods, called replicas.</p>
<p>Such a set of replicated Pods are created and managed by a controller, such as a Deployment.</p>
<p><strong>Connection</strong></p>
<p>All the pods in a cluster are connected. Pods can communicate through their unique IP address. If there are more containers within one pod, they can communicate over localhost.</p>
<p><strong>Pods are not forever</strong></p>
<p>Pods are not &quot;forever&quot;; instead, they easily die in case of machine failure or have to be terminated for machine maintenance. When a pod fails, Kubernetes automatically (unless specified otherwise) spins it up again.</p>
<p>Additionally, a controller can be used to ensure that the pod is &quot;automatically&quot; healing. In this case, the controlled will monitor the stat of the pod; in case the desired state does not fit the actual state; it will ensure that the actual state is moved back towards the desired state.</p>
<p>It is considered good practice to have one process per pod; this allows for easier analysis and debugging.</p>
<p><strong>Each pod has:</strong></p>
<ul>
<li>a unique IP address (which allows them to communicate with each other)</li>
<li>persistent storage volumes (as required) (more on this later on another day)</li>
<li>configuration information that determine how a container should run</li>
</ul>
<p><strong>Pod lifecycle</strong></p>
<p>(copied from Google)</p>
<p>Each Pod has a PodStatus API object, which is represented by a Pod's status field. Pods publish their phase to the status: phase field. The phase of a Pod is a high-level summary of the Pod in its current state.</p>
<p>When you run
<code>kubectl get pod</code> <a href="https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#get">Link</a> 
to inspect a Pod running on your cluster, a Pod can be in one of the following
possible phases:</p>
<ul>
<li><strong>Pending:</strong> Pod has been created and accepted by the cluster, but one or more
of its containers are not yet running. This phase includes time spent being
scheduled on a node and downloading images.</li>
<li><strong>Running:</strong> Pod has been bound to a node, and all of the containers have been
created. At least one container is running, is in the process of starting, or
is restarting.</li>
<li><strong>Succeeded:</strong> All containers in the Pod have terminated successfully.
Terminated Pods do not restart.</li>
<li><strong>Failed:</strong> All containers in the Pod have terminated, and at least one
container has terminated in failure. A container &quot;fails&quot; if it exits with a
non-zero status.</li>
<li><strong>Unknown:</strong> The state of the Pod cannot be determined.</li>
</ul>
<p><strong>Limits</strong></p>
<p>Pods by themselves do not have a memory or CPU limit. However, you can set limits to control the amount of CPU or memory your Pod can use on a node. A limit is the maximum amount of CPU or memory that Kubernetes guarantees to a Pod.</p>
<p><strong>Termination</strong></p>
<p>Once the process of the pod is completed, it will terminate. Alternatively, you can also delete a pod.</p>
<h1 id="setup-your-first-kubernetes-cluster"><a class="header" href="#setup-your-first-kubernetes-cluster">Setup your first Kubernetes Cluster</a></h1>
<h1 id="100days-resources-3"><a class="header" href="#100days-resources-3">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/uU-8Zcst5Qk">Video by Anais Urlichs</a></li>
<li><a href="https://medium.com/backbase/kubernetes-in-local-the-easy-way-f8ef2b98be68?sk=1cf6c2f31d82d836a2a75503b2fb17be">Kubernetes cluster in your local machine using Docker Desktop</a></li>
</ul>
<h1 id="learning-resources-3"><a class="header" href="#learning-resources-3">Learning Resources</a></h1>
<ul>
<li>What is Kubernetes: <a href="https://youtu.be/VnvRFRk_51k">https://youtu.be/VnvRFRk_51k</a></li>
<li>Kubernetes architecture explained: <a href="https://youtu.be/umXEmn3cMWY">https://youtu.be/umXEmn3cMWY</a></li>
</ul>
<h1 id="example-notes-3"><a class="header" href="#example-notes-3">Example Notes</a></h1>
<p>Today, I will get started with the <a href="http://leanpub.com/the-devops-2-3-toolkit">book</a>: The DevOps 2.3 Toolkit; and will work my way through the book.</p>
<p><strong>Chapter 1</strong> provides an introduction to Kubernetes; I will use it to optimise the notes from the previous day.</p>
<p><strong>Chapter 2</strong> provides a walkthrough on how to set-up a local Kubernetes cluster using minikube or microk8s. Alternatively, <a href="https://kind.sigs.k8s.io/docs/user/quick-start/">kind</a> could also be used to create a local Kubernetes cluster or if you have Docker Desktop you could use directly the single node cluster <a href="https://medium.com/backbase/kubernetes-in-local-the-easy-way-f8ef2b98be68?sk=1cf6c2f31d82d836a2a75503b2fb17be">included</a>.</p>
<p>Prerequisites</p>
<ol>
<li>Have Docker installed (if not go ahead and do it): <a href="https://docs.docker.com/">https://docs.docker.com/</a></li>
<li>Install kubectl </li>
</ol>
<p><strong>Here is how to install kubectl</strong></p>
<p>If you have the Homebrew package manager installed, you can use that:</p>
<pre><code class="language-jsx">brew install kubectl
</code></pre>
<p>On Linux, the commands are going to be:</p>
<pre><code class="language-jsx">curl -LO [https://storage.googleapis.com/kubernetes-release/release/$](https://storage.googleapis.com/kubernetes-release/release/$)(curl -s https:/\
/storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubec\
tl

chmod +x ./kubectl

sudo mv ./kubectl /usr/local/bin/kubectl
</code></pre>
<p>To make sure you have kubectl installed, you can run </p>
<pre><code class="language-jsx">kubectl version --output=yaml
</code></pre>
<p><strong>Install local cluster</strong></p>
<p>To install minikube, you require a virtualisation technology such as VirtualBox. If you are on windows, you might want to use hyperv instead. Minikube provides a single node instance that you can use in combination with kubectl.</p>
<p>It supports DNS, Dashboards, CNI, NodePorts, Config Maps, etc. It also supports multiple hypervisors, such as Virtualbox, kvm, etc.</p>
<p>In my case I am going to be using <a href="https://microk8s.io/">microk8s</a> since I had several issues getting started with minikube. However, please don't let this put you off. Please look for yourself into each tool and decide which one you like the best.</p>
<p>Microk8s provides a lightweight Kubernetes installation on your local machine. Overall, it is much easier to install on Linux using snap since it does not require any virtualization tools. </p>
<pre><code class="language-jsx">sudo snap install microk8s --classic
</code></pre>
<p>However, also the Windows and Mac installation are quite straightforward so have a look at those on their website.</p>
<p>Make sure that kubectl has access directly to your </p>
<p>If you have multiple clusters configured, you can switch between them using your kubectl commands</p>
<p>To show the different clusters available:</p>
<pre><code>kubectl config get-contexts
</code></pre>
<p>to switch to a different cluster:</p>
<pre><code>kubectl config use-context &lt;name of the context&gt;
</code></pre>
<p>Once we are connected to the right cluster, we can ask kubectl to show us our nodes</p>
<pre><code>kubectl get nodes
</code></pre>
<p>Or you could see the current pods that are running on your cluster — if it is a new cluster, you likely don't have any pods running.</p>
<pre><code>kubectl get pods
</code></pre>
<p>In the case of minikube and microk8s, we have only one node</p>
<h1 id="running-pods"><a class="header" href="#running-pods">Running Pods</a></h1>
<h1 id="100days-resources-4"><a class="header" href="#100days-resources-4">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/S39JmiFsheM">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-4"><a class="header" href="#learning-resources-4">Learning Resources</a></h1>
<ul>
<li><a href="https://youtu.be/wlYESb124xM">How Pods and the Pod Lifecycle work in Kubernetes</a></li>
<li><a href="https://youtu.be/5cNrTU6o3Fw">Pods and Containers - Kubernetes Networking | Container Communication inside the Pod</a></li>
</ul>
<h1 id="example-notes-4"><a class="header" href="#example-notes-4">Example Notes</a></h1>
<h2 id="practical-example"><a class="header" href="#practical-example">Practical Example</a></h2>
<p>Fork the following repository: <a href="https://github.com/vfarcic/k8s-specs">https://github.com/vfarcic/k8s-specs</a></p>
<pre><code>git clone https://github.com/vfarcic/k8s-specs.git

cd k8s-specs
</code></pre>
<p>Create a mongo DB database </p>
<pre><code>kubectl run db --image mongo \
--generator &quot;run-pod/v1&quot;
</code></pre>
<p>If you want to confirm that the pod was created do: kubectl get pods</p>
<p>Note that if you do not see any output right away that is ok; the mongo image is really big so it might take a while to get the pod up and running.</p>
<p>Confirm that the image is running in the cluster</p>
<pre><code>docker container ls -f ancestor=mongo
</code></pre>
<p>To delete the pod run</p>
<pre><code>kubectl delete pod db
</code></pre>
<p>Delete the pod above since it was not the best way to run the pod.</p>
<ul>
<li>Pods should be created in a declarative format. However, in this case, we created it in an imperative way — BAD!</li>
</ul>
<p>To look at the pod definition:</p>
<pre><code>cat pod/db.yml
</code></pre>
<pre><code>apiVersion: v1 // means the version 1 of the Kubernetes pod API; API version and kind has to be provided -- it is mandatory
kind: Pod
metadata: // the metadata provides information on the pod, it does not specifiy how the pod behaves
name: db
labels:
type: db
vendor: MongoLabs // I assume, who has created the image
spec:
containers:
- name: db
image: mongo:3.3 // image name and tag
command: [&quot;mongod&quot;]
args: [&quot;--rest&quot;, &quot;--httpinterface&quot;] // arguments, defined in an array
</code></pre>
<p>In the case of controllers, the information provided in the metadata has a practical purpose. However, in this case, it merely provides descriptive information.</p>
<p>All arguments that can be used in pods are defined in <a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#pod-v1-core">https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#pod-v1-core</a></p>
<p>With the following command, we can create a pod that is defined in the pod.yml file </p>
<pre><code>kubectl create -f pod/db.yml
</code></pre>
<p>to view the pods (in json format)</p>
<pre><code>kubectl get pods -o json
</code></pre>
<p>We can see that the pod went through several stages (stages detailed in the video on pods)</p>
<p>In the case of microk8s, both master and worker nodes run on the same machine.</p>
<p>To verify that the database is running, we can go ahead an run</p>
<pre><code>kubectl exec -it db sh // this will start a terminal inside the running container
echo 'db.stats()'
exit
</code></pre>
<p>Once we do not need a pod anymore, we should delete it</p>
<pre><code>kubectl delete -f pod/db.yml
</code></pre>
<ul>
<li>Kubernetes will first try to stop a pod gracefully; it will have 30s to shut down.</li>
<li>After the &quot;grace period&quot; a kill signal is sent</li>
</ul>
<p>Additional notes</p>
<ul>
<li>Pods cannot be split across nodes</li>
<li>Storage within a pod (volumes) can be accessed by all the containers within a pod</li>
</ul>
<h3 id="run-multiple-containers-with-in-a-pod"><a class="header" href="#run-multiple-containers-with-in-a-pod">Run multiple containers with in a pod</a></h3>
<p>Most pods should be made of a single container; multiple containers within one pod is not common nor necessarily desirable </p>
<p>Look at </p>
<pre><code>cat pod/go-demo-2.yml
</code></pre>
<p>of the closed repository (the one cloned at the beginning of these notes)</p>
<p>The yml defines the use of two containers within one pod</p>
<pre><code>kubectl create -f pod/go-demo-2.yml

kubectl get -f pod/go-demo-2.yml
</code></pre>
<p>To only retrieve the names of the containers running in the pod</p>
<pre><code>kubectl get -f pod/go-demo-2.yml \
-o jsonpath=&quot;{.spec.containers[*].name}&quot;
</code></pre>
<p>Specify the name of the container for which we want to have the logs</p>
<pre><code>kubectl logs go-demo-2 -c db
</code></pre>
<ul>
<li>livenessProbes are used to check whether a container should be running</li>
</ul>
<p>Have a look at </p>
<pre><code>cat pod/go-demo-2-health.yml
</code></pre>
<p>within the cloned repository.</p>
<p>Create the pod</p>
<pre><code>kubectl create \
-f pod/go-demo-2-health.yml
</code></pre>
<p>wait a minute and look at the output</p>
<pre><code>kubectl describe \
-f pod/go-demo-2-health.yml
</code></pre>
<h1 id="kubernetes-replicaset"><a class="header" href="#kubernetes-replicaset">Kubernetes ReplicaSet</a></h1>
<h1 id="100days-resources-5"><a class="header" href="#100days-resources-5">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/TBkZaPDX7Us">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-5"><a class="header" href="#learning-resources-5">Learning Resources</a></h1>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">Kubernetes ReplicaSet official documentation</a></li>
</ul>
<h1 id="example-notes-5"><a class="header" href="#example-notes-5">Example Notes</a></h1>
<p><strong>ReplicaSets</strong></p>
<p>It is usually not recommended to create pods manually but instead use multiple instances of the same application; these are then identical pods, called replicas. You can specify the desired number of replicas within the ReplicaSet.</p>
<p>A ReplicaSet ensures that a certain number of pods are running at any point in time. If there are more pods running than the number specified by the ReplicaSet, the ReplicaSet will kill the pods.</p>
<p>Similarly, if any pod dies and the total number of pods is fewer than the defined number of pods, the ReplicaSet will spin up more pods.</p>
<p>Each pod is supposed to run a single instance of an application. If you want to scale your application horizontally, you can create multiple instances of that pod.</p>
<p>The pod ReplicaSet is used for scaling pods in your Kubernetes cluster.</p>
<p><strong>Such a set of replicated Pods are created and managed by a controller, such as a Deployment.</strong></p>
<p>As long as the primary conditions are met: enough CPU and memory is available in the cluster, the ReplicaSet is self-healing; it provides fault tolerance and high availibility.</p>
<p>It's only purpose is to ensure that the specified number of replicas of a service is running.</p>
<p>All pods are managed through Controllers and Services. They know about the pods that they have to manage through the in-YAML defined Labels within the pods and the selectors within the Controllers/Services. Remember the metadata field from one of the previous days — in the case of ReplicaSets, these labels are used again.</p>
<h2 id="some-practice"><a class="header" href="#some-practice">Some Practice</a></h2>
<p>Clone the following repository: <a href="https://github.com/vfarcic/k8s-specs">https://github.com/vfarcic/k8s-specs</a> and enter into the root folder</p>
<pre><code class="language-jsx">cd k8s-specs
</code></pre>
<p>Looking at the following example</p>
<pre><code class="language-jsx">cat rs/go-demo-2.yml
</code></pre>
<ul>
<li>The selector is used to specify which pods should be included in the replicaset</li>
<li>ReplicaSets and Pods are decoupled</li>
<li>If the pods that match the replicaset, it does not have to do anything</li>
<li>Similar to how the ReplicaSet would scale pods to match the definition provided in the yaml, it will also terminate pods if there are too many</li>
<li>the spec.template.spec defines the pod</li>
</ul>
<p>Next, create the pods</p>
<pre><code class="language-jsx">kubectl create -f rs/go-demo-2.yml
</code></pre>
<p>We can see further details of your running pods through the kubectl describe command</p>
<pre><code class="language-jsx">kubectl describe -f rs/go-demo-2.yml
</code></pre>
<p>To list all the pods, and to compare the labels specified in the pods match the ReplicaSet</p>
<pre><code class="language-jsx">kubectl get pods --show-labels
</code></pre>
<p>You can call the number of replicasets by running</p>
<pre><code class="language-jsx">kubectl get replicasets
</code></pre>
<p>ReplicaSets are named using the same naming convention as used for pods.</p>
<h3 id="difference-between-replicaset-and-replication-controller"><a class="header" href="#difference-between-replicaset-and-replication-controller">Difference between ReplicaSet and Replication Controller</a></h3>
<p>They both serve the same purpose — the Replication Controller is being deprecated.</p>
<h3 id="operating-replicasets"><a class="header" href="#operating-replicasets">Operating ReplicaSets</a></h3>
<p>You can delete a ReplicaSet without deleting the pods that have been created by the replicaset</p>
<pre><code class="language-jsx">kubectl delete -f rs/go-demo-2.yml \
--cascade=false
</code></pre>
<p>And then the ReplicaSet can be created again</p>
<pre><code class="language-jsx">kubectl create -f rs/go-demo-2.yml \
--save-config
</code></pre>
<p>the —save-config flag ensures that our configurations are saved, which allows us to do more specific tasks later on.</p>
<h1 id="kubernetes-deployments"><a class="header" href="#kubernetes-deployments">Kubernetes Deployments</a></h1>
<h1 id="100days-resources-6"><a class="header" href="#100days-resources-6">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/qt76R2G4h-0">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-6"><a class="header" href="#learning-resources-6">Learning Resources</a></h1>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Kubernetes Deployments official documentation</a></li>
</ul>
<h1 id="example-notes-6"><a class="header" href="#example-notes-6">Example Notes</a></h1>
<p>This little exercise will be based on the following application: <a href="https://github.com/anais-codefresh/react-article-display">https://github.com/anais-codefresh/react-article-display</a></p>
<p>Then we will create a deployment</p>
<pre><code class="language-jsx">apiVersion: apps/v1
kind: Deployment
metadata:
  name: react-application
spec:
  replicas: 2
  selector:
    matchLabels:
      run: react-application
  template:
    metadata:
      labels:
        run: react-application
    spec:
      containers:
      - name: react-application
        image: anaisurlichs/react-article-display:master
        ports:
          - containerPort: 80
        imagePullPolicy: Always
</code></pre>
<p>More information on Kubernetes deployments</p>
<ul>
<li>A deployment is a Kubernetes object that makes it possible to manage multiple, identical pods</li>
<li>Using deployments, it is possible to automate the process of creating, modifying and deleting pods — it basically manages the lifecycle of your application</li>
<li>Whenever a new object is created, Kubernetes will ensure that this object exist</li>
<li>If you try to set-up pods manually, it can lead to human error. On the other hand, using deployments is a better way to prevent human errors. </li>
<li>The difference between a deployment and a service is that a deployment ensures that a set of pods keeps running by creating pods and replacing broken prods with the resource defined in the template. In comparison, a service is used to allow a network to access the running pods.</li>
</ul>
<p><a href="https://www.redhat.com/en/topics/containers/what-is-kubernetes-deployment">Deployments allow you to</a></p>
<ul>
<li>Deploy a replica set or pod</li>
<li>Update pods and replica sets</li>
<li>Rollback to previous deployment versions</li>
<li>Scale a deployment</li>
<li>Pause or continue a deployment</li>
</ul>
<p>Create deployment</p>
<pre><code class="language-jsx">kubectl create -f deployment.yaml
</code></pre>
<p>Access more information on the deployment </p>
<pre><code class="language-jsx">kubectl describe deployment &lt;deployment name&gt;
</code></pre>
<p>Create the service yml</p>
<pre><code class="language-jsx">apiVersion: v1
kind: Service
metadata:
  name: react-application
  labels:
    run: react-application
spec:
  type: NodePort
  ports:
  - port: 8080
    targetPort: 80
    protocol: TCP
    name: http
  selector:
    run: react-application
</code></pre>
<p>Creating the service with kubectl expose</p>
<pre><code class="language-jsx">kubectl expose deployment/my-nginx
</code></pre>
<p>This will create a service that is highly similar to our in yaml defined service. However, if we want to create the service based on our yaml instead, we can run:</p>
<pre><code class="language-jsx">kubectl create -f my-pod-service.yml
</code></pre>
<h3 id="how-is-the-service-and-the-deployment-linked"><a class="header" href="#how-is-the-service-and-the-deployment-linked">How is the Service and the Deployment linked?</a></h3>
<p>The targetPort in the service yaml links to the container port in the deployment. Thus, both have to be, for example, 80. </p>
<p>We can then create the deployment and service based on the yaml, when you look for &quot;kubectl get service&quot;, you will see the created service including the Cluster-IP. Take that cluster IP and the port that you have defined in the service e.g. 10.152.183.79:8080 basically <Cluster IP>:<port defined in service> and you should be able to access the application through NodePort. However, note that anyone will be able to access this connection. You should be deleting these resources afterwards.</p>
<pre><code class="language-jsx">kubectl get service
</code></pre>
<p><img src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3f2ba295-c81d-4836-8b46-eadbc70b2eb7/Screenshot_from_2021-01-06_21-54-40.png" alt="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3f2ba295-c81d-4836-8b46-eadbc70b2eb7/Screenshot_from_2021-01-06_21-54-40.png" /></p>
<p>Alternatively, for more information of the service</p>
<pre><code class="language-jsx">kubectl get svc &lt;service name&gt; -o yaml
</code></pre>
<p>-o yaml: the data should be displayed in yaml format</p>
<p>Delete the resources by</p>
<pre><code class="language-jsx">kubectl delete service react-application
kubectl delete deployment react-application

// in this case, your pods are still running, so you would have to remove them individually 
</code></pre>
<p>Note: replace react-application with the name of your service.</p>
<h1 id="namespaces"><a class="header" href="#namespaces">Namespaces</a></h1>
<h1 id="100days-resources-7"><a class="header" href="#100days-resources-7">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/GD-CweSXsfY">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-7"><a class="header" href="#learning-resources-7">Learning Resources</a></h1>
<ul>
<li><a href="https://youtu.be/K3jNo4z5Jx8">TechWorld with Nana explanation on Namespaces</a></li>
</ul>
<h1 id="example-notes-7"><a class="header" href="#example-notes-7">Example Notes</a></h1>
<p>In some cases, you want to divide your resources, provide different access rights to those resources and more. This is largely driven by the fear that something could happen to your precious production resources.</p>
<p>However, with every new cluster, the management complexity will scale — the more clusters you have, the more you have to manage — basically, the resource overhead of many small clusters is higher than of one big one. Think about this in terms of houses, if you have one big house, you have to take care of a lot but having several small houses, you have x the number of everything + they will be affected by different conditions. </p>
<h2 id="practical"><a class="header" href="#practical">Practical</a></h2>
<p>Let's get moving, you only learn by doing.</p>
<p>Clone this repository <a href="https://github.com/vfarcic/k8s-specs">https://github.com/vfarcic/k8s-specs</a> </p>
<pre><code class="language-jsx">cd k8s-specs
</code></pre>
<p>and then, we will use this application</p>
<pre><code class="language-jsx">cat ns/go-demo-2.yml
</code></pre>
<p>Then we do a nasty work-around to specify the image tag used in the pod</p>
<pre><code class="language-jsx">IMG=vfarcic/go-demo-2

TAG=1.0

cat ns/go-demo-2.yml \
| sed -e \
&quot;s@image: $IMG@image: $IMG:$TAG@g&quot; \
| kubectl create -f -
</code></pre>
<p>When the -f argument is followed with a
dash (-), kubectl uses standard input (stdin) instead of a file.</p>
<p>To confirm that the deployment was successful </p>
<pre><code class="language-jsx">kubectl rollout status \
2 deploy go-demo-2-api
</code></pre>
<p>Which will get us the following output</p>
<pre><code class="language-jsx">hello, release 1.0!
</code></pre>
<p>Almost every service are Kubernetes Objects. </p>
<pre><code class="language-jsx">kubectl get all
</code></pre>
<p>Gives us a full list of all the resources that we currently have up and running.</p>
<p>The system-level objects within our cluster are usually not visible, only the objects that we created.</p>
<p>Within the same namespace, we cannot have twice the same object with exactly the same name. However, we can have the same object in two different namespaces. </p>
<p>Additionally, you could specify within a cluster permissions, quotas, policies, and more — will look at those sometimes later within the challenge.</p>
<p>We can list all of our namespaces through</p>
<pre><code class="language-jsx">kubectl get ns
</code></pre>
<p>Create a new namespaces</p>
<pre><code class="language-jsx">kubectl create namespace testing
</code></pre>
<p>Note that you could also use 'ns' for 'namespace'</p>
<p>Kubernetes puts all the resources needed to execute Kubernetes commands into the kube-system namespace</p>
<pre><code class="language-jsx">kubectl --namespace kube-system get all
</code></pre>
<p>Now that we have a namespace testing, we can use it for new deployments — however, specifying the namespace with each command is annoying. What we can do instead is</p>
<pre><code class="language-jsx">kubectl config set-context testing \
 --namespace testing \
 --cluster docker-desktop \
 --user docker-desktop
</code></pre>
<p>In this case, you will have to change the command according to your cluster. The created context uses the same cluster as before — just a different namespace.</p>
<p>You can view the config with the following command</p>
<pre><code class="language-jsx">kubectl config view
</code></pre>
<p>Once we have a new context, we can switch to that one</p>
<pre><code class="language-jsx">kubectl config use-context testing
</code></pre>
<p>Once done, all of our commands will be automatically executed in the testing namespace.</p>
<p>Now we can deploy the same resource as before but specify a different tag.</p>
<pre><code class="language-jsx">TAG=2.0

DOM=go-demo-2.com

cat ns/go-demo-2.yml \
| sed -e \
&quot;s@image: $IMG@image: $IMG:$TAG@g&quot; \
| sed -e \
&quot;s@host: $DOM@host: $TAG\.$DOM@g&quot; \
| kubectl create -f -
</code></pre>
<p>to confirm that the rollout has finished</p>
<pre><code class="language-jsx">kubectl rollout status \
deployment go-demo-2-api
</code></pre>
<p>Now we can send requests to the different namespaces </p>
<pre><code class="language-jsx">curl -H &quot;Host: 2.0.go-demo-2.com&quot; \
2 &quot;http://$(minikube ip)/demo/hello&quot;
</code></pre>
<h3 id="deleting-resources"><a class="header" href="#deleting-resources">Deleting resources</a></h3>
<p>It can be really annoying to have to delete all objects one by one. What we can do instead is to delete all resources within a namespace all at once </p>
<pre><code class="language-jsx">kubectl delete ns testing
</code></pre>
<p>The real magic of namespaces is when we combine those with authorization logic, which we are going to be looking at in later videos.</p>
<h1 id="configmaps"><a class="header" href="#configmaps">ConfigMaps</a></h1>
<h1 id="100days-resources-8"><a class="header" href="#100days-resources-8">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/gQgGN12hfNY">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-8"><a class="header" href="#learning-resources-8">Learning Resources</a></h1>
<ul>
<li><a href="https://matthewpalmer.net/kubernetes-app-developer/articles/ultimate-configmap-guide-kubernetes.html">https://matthewpalmer.net/kubernetes-app-developer/articles/ultimate-configmap-guide-kubernetes.html</a></li>
</ul>
<h1 id="example-notes-8"><a class="header" href="#example-notes-8">Example Notes</a></h1>
<p>ConfigMaps make it possible to keep configurations separately from our application images by injecting configurations into your container. The content/injection might be configuration <strong>files or variables.</strong></p>
<p>It is a type of Volume. </p>
<p>ConfigMaps=Mount a source to a container</p>
<p>It is a directory or file of configuration settings.</p>
<p>Environment variables can be used to configure new applications. They are great unless our application is too complex.</p>
<p>If the application configuration is based on a file, it is best to make the file part of our Docker image. </p>
<p>Additionally, you want to use ConfigMap with caution. If you do not have any variations between configurations of your app, you do not need a ConfigMap. ConfigMaps let you easily fall into the trap of making specific configuration — which makes it harder to move the application and to automate its set-up. Resulting, if you do use ConfigMaps, you would likely have one for each environment.</p>
<p>So what could you store within a ConfigMap</p>
<blockquote>
<p>A ConfigMap stores configuration settings for your code. Store connection strings, public credentials, hostnames, and URLs in your ConfigMap.</p>
</blockquote>
<p><a href="https://matthewpalmer.net/kubernetes-app-developer/articles/ultimate-configmap-guide-kubernetes.html">Source</a></p>
<p>So make sure to not store any sensitive information within ConfigMap.</p>
<p>We first create a ConfigMap with</p>
<pre><code class="language-jsx">kubectl create cm my-config \
--from-file=cm/prometheus-conf.yml
</code></pre>
<p>Taking a look into that resource</p>
<pre><code class="language-jsx">**kubectl describe cm my-config**
</code></pre>
<p>ConfigMap is another volume that, like other volumes, need to mount</p>
<pre><code class="language-jsx">cat cm/alpine.yml
</code></pre>
<p>The volume mount section is the same, no matter the type of volume that we want to mount.</p>
<p>We can create a pod and make sure it is running</p>
<pre><code class="language-jsx">kubectl create -f cm/alpine.yml

kubectl get pods
</code></pre>
<p>And then have a look inside the pod</p>
<pre><code class="language-jsx">kubectl exec -it alpine -- \
ls /etc/config
</code></pre>
<p>You will then see a single file that is correlated to the file that we stored in the ConfigMap</p>
<p>To make sure the content of both files is indeed the same, you can use the following command</p>
<pre><code class="language-jsx">kubectl exec -it alpine -- \
cat /etc/config/prometheus-conf.yml
</code></pre>
<p>The —from-file argument in the command at the beginning can be used with files as well as directories.</p>
<p>In case we want to create a ConfigMap with a directory</p>
<pre><code class="language-jsx">kubectl create cm my-config \
--from-file=cm
</code></pre>
<p>and have a look inside</p>
<pre><code class="language-jsx">kubectl describe cm my-config
</code></pre>
<p>The create a pod that mounts to the ConfigMap</p>
<pre><code class="language-jsx">kubectl create -f cm/alpine.yml

kubectl exec -it alpine -- \
ls /etc/config
</code></pre>
<p>Make sure to delete all the files within your cluster afterwards</p>
<pre><code class="language-jsx">kubectl delete -f cm/alpine.yml

kubectl delete cm my-config
</code></pre>
<p>Furthermore, like every other Kubernetes resource, you can define ConfigMaps through Kubernetes YAML files. This actually (probably the easiest way) — write the ConfigMap in YAML and mount it as a Volume</p>
<p>We can show one of our existing ConfigMaps in YAML</p>
<pre><code class="language-jsx">kubectl get cm my-config -o yaml
</code></pre>
<p>Additionally we can take a look at this file within our repository that has both a Deployment object and a ConfigMap</p>
<pre><code class="language-jsx">cat cm/prometheus.yml
</code></pre>
<pre><code class="language-jsx">kind: ConfigMap 
apiVersion: v1 
metadata:
  name: example-configmap 
data:
  # Configuration values can be set as key-value properties
  database: mongodb
  database_uri: mongodb://localhost:27017
  
  # Or set as complete file contents (even JSON!)
  keys: | 
    image.public.key=771 
    rsa.public.key=42
</code></pre>
<p>And then create the ConfigMap like any other resource</p>
<pre><code class="language-jsx">kubectl apply -f config-map.yaml
</code></pre>
<pre><code class="language-jsx">kind: Pod 
apiVersion: v1 
metadata:
  name: pod-using-configmap 

spec:
  # Add the ConfigMap as a volume to the Pod
  volumes:
    # `name` here must match the name
    # specified in the volume mount
    - name: example-configmap-volume
      # Populate the volume with config map data
      configMap:
        # `name` here must match the name 
        # specified in the ConfigMap's YAML 
        name: example-configmap

  containers:
    - name: container-configmap
      image: nginx:1.7.9
      # Mount the volume that contains the configuration data 
      # into your container filesystem
      volumeMounts:
        # `name` here must match the name
        # from the volumes section of this pod
        - name: example-configmap-volume
           mountPath: /etc/config
</code></pre>
<h1 id="kubernetes-service"><a class="header" href="#kubernetes-service">Kubernetes Service</a></h1>
<h1 id="100days-resources-9"><a class="header" href="#100days-resources-9">100Days Resources</a></h1>
<ul>
<li>Video by Anais Urlichs <a href="https://youtu.be/4_felE4hMog">One</a> and <a href="https://youtu.be/vxA4IP3K7Z4">Two</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-9"><a class="header" href="#learning-resources-9">Learning Resources</a></h1>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/service/">Official Documentation</a></li>
<li><a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/">https://kubernetes.io/docs/reference/kubectl/cheatsheet/</a></li>
<li><a href="https://katacoda.com/courses/kubernetes">https://katacoda.com/courses/kubernetes</a></li>
<li>Guides and interactive tutorial within the Kubernetes docs <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/">https://kubernetes.io/docs/tutorials/kubernetes-basics/expose/expose-intro/</a></li>
<li>Kubernetes by example <a href="https://kubernetesbyexample.com/">https://kubernetesbyexample.com/</a> created by OpenShift</li>
</ul>
<h1 id="example-notes-9"><a class="header" href="#example-notes-9">Example Notes</a></h1>
<p>Pods are formed, destroyed and never repaired. You would not repair an existing, running pod but rather deploy a new, healthy one.</p>
<p>Controllers, along with the Scheduler inside your Kubernetes cluster are making sure that pods are behaving correctly, they are monitoring the pods. </p>
<p>So far, only containers within the same pod can talk to each other through localhost. This prevents us from scaling our application. Thus we want to enable communication between pods. This is done with Kubernetes Services.</p>
<blockquote>
<p>Kubernetes Services provide addresses through which associated Pods can be accessed.</p>
</blockquote>
<p>A service is usually created on top of an existing deployment.</p>
<h3 id="follow-along"><a class="header" href="#follow-along">Follow along</a></h3>
<p>Clone the following repository: <a href="https://github.com/vfarcic/k8s-specs">https://github.com/vfarcic/k8s-specs</a></p>
<p>And prepare our minikube, microk8s or whatever you are using as your local cluster 🙂</p>
<pre><code class="language-jsx">cd k8s-specs

git pull

minikube start --vm-driver=virtualbox

kubectl config current-context
</code></pre>
<p>For this exercise, we are going to create the following ReplicaSet, similar to what we have done in the previous video. The definition will look as follows:</p>
<pre><code class="language-jsx">cat svc/go-demo-2-rs.yml
</code></pre>
<p>Now create the ReplicaSet:</p>
<pre><code class="language-jsx">kubectl create -f svc/go-demo-2-rs.yml

// get the state of it
kubectl get -f svc/go-demo-2-rs.yml
</code></pre>
<p>Before continuing with the next exercises, make sure that both replicas are ready</p>
<p>With the kubectl expose command, we can tell Kubernetes that we want to expose a resource as service in our cluster</p>
<pre><code class="language-jsx">kubectl expose rs go-demo-2 \
 --name=go-demo-2-svc \
 --target-port=28017 \ // this is the port that the MongoDB interface is listening to
 --type=NodePort
</code></pre>
<p>We can have by default three different types of Services</p>
<p><strong>ClusterIP</strong></p>
<p>ClusterIP is used by default. It exposes the service only within the cluster. By default you want to be using ClusterIP since that prevents any external communication and makes your cluster more secure. </p>
<p><strong>NodePort</strong> </p>
<p>Allows the outside world to access the node IP</p>
<p><strong>and LoadBalancer</strong></p>
<p>The LoadBalancer is only useful when it is combined with the LoadBalancer of your cloud provider.</p>
<p>The process when creating a new Service is something like this:</p>
<ol>
<li>First, we tell our API server within the master node in our cluster it should create a new service — in our case, this is done through kubectl commands.</li>
<li>Within our cluster, inside the master node, we have an endpoint controller. This controller will watch our API server to see whether we want to create a new Service. Once it knows that we want to create a new API server, it will create an endpoint object.</li>
<li>The kube-proxy watches the cluster for services and enpoints that it can use to configure the access to our cluster. It will then make a new entry in its iptable that takes note of the new information.</li>
<li>The Kube-DNS realises that there is a new service and will add the db’s record to the dns server (skydns).</li>
</ol>
<p>Taking a look at our newly created service:</p>
<pre><code class="language-jsx">kubectl describe svc go-demo-2-svc
</code></pre>
<ul>
<li>All the pods in the cluster can access the targetPort</li>
<li>The NodePort automatically creates the clusterIP</li>
<li>Note that if you have multiple ports defined within a service, you have to name those ports</li>
</ul>
<p>Let’s see whether the Service indeed works.
PORT=$(kubectl get svc go-demo-2-svc <br />
-o jsonpath=&quot;{.spec.ports[0].nodePort}&quot;)</p>
<p>IP=$(minikube ip)</p>
<p>open &quot;http://$IP:$PORT&quot;</p>
<h3 id="creating-services-in-a-declarative-format"><a class="header" href="#creating-services-in-a-declarative-format">Creating Services in a Declarative format</a></h3>
<pre><code class="language-jsx">cat svc/go-demo-2-svc.yml
</code></pre>
<ul>
<li>The service is of type NodePort - making it available within the cluster</li>
<li>TCP is used as default protocol</li>
<li>The selector is used by the service to know which pods should receive requests (this works the same way as the selector within the ReplicaSet)</li>
</ul>
<p>With the following command, we create the service and then get the sevrice</p>
<pre><code class="language-jsx">kubectl create -f svc/go-demo-2-svc.yml

kubectl get -f svc/go-demo-2-svc.yml
</code></pre>
<p>We can look at our endpoint through</p>
<pre><code class="language-jsx">kubectl get ep go-demo-2 -o yaml
</code></pre>
<ul>
<li>The subset responds to two pods, each pod has its own IP address</li>
<li>Requests are distributed between these two nodes</li>
</ul>
<p>Make sure to delete the Service and ReplicaSet at the end</p>
<pre><code class="language-jsx">
kubectl delete -f svc/go-demo-2-svc.yml

kubectl delete -f svc/go-demo-2-rs.yml
</code></pre>
<h1 id="ingress"><a class="header" href="#ingress">Ingress</a></h1>
<h1 id="100days-resources-10"><a class="header" href="#100days-resources-10">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/rNvFvGVzT5o">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-10"><a class="header" href="#learning-resources-10">Learning Resources</a></h1>
<ul>
<li><a href="https://github.com/AnaisUrlichs/ingress-example">Ingress Tutorial by Anais Urlichs</a></li>
</ul>
<h1 id="example-notes-10"><a class="header" href="#example-notes-10">Example Notes</a></h1>
<p>Ingress is responsible for managing the external access to our cluster. Whereby it manages</p>
<ul>
<li>forwarding rules based on paths and domains</li>
<li>SSl termination</li>
<li>and several other features.</li>
</ul>
<p>The API provided by Ingress allows us to replace an external proxy with a loadbalancer.</p>
<p>Things we want to resolve using Ingress</p>
<ul>
<li>Not having to use a fixed port — if we have to manage multiple clusters, we would have a hard time managing all those ports</li>
<li>We need standard HTTPS(443) or HTTP (80) ports through a predefined path</li>
</ul>
<p>When we open an application, the request to the application is first received by the service and LoadBalancer, which is the responsible for forwarding the request to either of the pods it is responsible for.</p>
<p>To make our application more secure, we need a place to store the application's HTTPS certificate and forwarding. Once this is implemented, we have a mechanism that accepts requests on specific ports and forwards them to our Kubernetes Service.</p>
<p>The Ingress Controller can be used for this. </p>
<p>Unlike other Kubernetes Controllers, it is not part of our cluster by default but we have to install it separately.</p>
<p>If you are using minikube, you can check the available addons through </p>
<pre><code class="language-jsx">minikube addons list
</code></pre>
<p>And enable ingress (in case it is not enabled) </p>
<pre><code class="language-jsx">minikube addons enable ingress
</code></pre>
<p>If you are on microk8s, you can enable ingress through</p>
<pre><code class="language-jsx">microk8s enable ingress
</code></pre>
<p>You can check whether it is running through — if the pods are running for nginx-ingress, they will be listed</p>
<pre><code class="language-jsx">kubectl get pods --all-namespaces | grep nginx-ingress
</code></pre>
<p>If you receive an empty output, you might have to wait a little bit longer for Ingress to start.</p>
<p>Here is the YAML definition of our Ingress resource</p>
<pre><code class="language-jsx">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: react-application
  annotations:
    kubernetes.io/ingress.class: &quot;nginx&quot;
    ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;
spec:
  rules:
  - http:
      paths:
      - path: /demo
        pathType: ImplementationSpecific
        backend:
          service:
            name: react-application
            port:
              number: 8080
</code></pre>
<ul>
<li>The annotation section is used to provide additional information to the Ingress controller.</li>
<li>The path is the path after the</li>
</ul>
<p>You can find a list of annotations and the controllers that support them on this page: <a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md">https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/nginx-configuration/annotations.md</a></p>
<p>We have to set the ssl redirect to false since we do not have an ssl certificate.</p>
<p>You can create the resource through</p>
<pre><code class="language-jsx">kubectl create \
-f &lt;name of your file&gt;
</code></pre>
<p>If your application's service is set to NodePort, you will want to change it back into ClusterIP since there is no need anymore for NodePort.</p>
<p>What happens when we create a new Ingress resource?</p>
<ol>
<li>kubectl will send a request to the API Server of our cluster requesting the creation of a new Ingress resource</li>
<li>The ingress controller is consistently checking the cluster to see if there is a new ingress resource</li>
<li>Once it sees that there is a new ingress resource, it will configure its loadbalancer </li>
</ol>
<p>Ingress is a kind of service that runs on all nodes within your cluster. As long as requests match any of the rules defined within Ingress, Ingress will forward the request to the respective service.</p>
<p>To view the ingress running inside your cluster, use</p>
<pre><code class="language-jsx">kubectl get ing
</code></pre>
<p>Note that it might not work properly on microk8s.</p>
<h1 id="service-mesh"><a class="header" href="#service-mesh">Service Mesh</a></h1>
<h1 id="100days-resources-11"><a class="header" href="#100days-resources-11">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/4LU-XaQ7zzA">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-11"><a class="header" href="#learning-resources-11">Learning Resources</a></h1>
<ul>
<li><a href="https://docs.microsoft.com/en-us/azure/aks/servicemesh-about">Microsoft Introduction to Service Mesh</a></li>
<li><a href="https://www.nginx.com/blog/what-is-a-service-mesh/">Nginx explanation on Service Mesh</a></li>
</ul>
<h1 id="example-notes-11"><a class="header" href="#example-notes-11">Example Notes</a></h1>
<p><strong>The goal is:</strong></p>
<ol>
<li>Higher Portability: Deploy it wherever</li>
<li>Higher Agility: Update whenever </li>
<li>Lower Operational Management: Invest low cognitive </li>
<li>Lower Security Risk</li>
</ol>
<p><strong>How do Services find each other?</strong></p>
<ul>
<li>Answering this question allows us to break down the value of Service Mesh — different Services have to find each other.</li>
<li>If one service fails, the traffic has to be routed to another service so that requests don't fail</li>
</ul>
<p>Service-discovery can become the biggest bottleneck.</p>
<p>Open platform, independent service mesh.</p>
<p>In its simplest form a service mesh is a network of your microservices, managing the traffic between services. This allows it to manage the different interactions between your microservices.</p>
<p>A lot of the responsibilities that a service mesh has could be managed on an application basis. However, with the service mesh takes that logic out of the application specific services and manages those on an infrastructure basis.</p>
<p><strong>Why do you need Service Mesh?</strong></p>
<p>Istio is a popular solution for managing communication between microservices.</p>
<p>When we move from monolithic to microservice application, we run into several issues that we did not have before. It will need the following setup</p>
<ol>
<li>Each microservice has its own business logic — all service endpoints must be configured</li>
<li>Ensure Security standards with firewall rules set-up — every service inside the cluster can talk to every other service if we do not have any additional security inside — for more important applications this is not secure enough. This may result in a lot of complicated configuration.</li>
</ol>
<p>To better manage the application configuration, everything but the business logic could be packed into its own Sidecar Proxy, which would then be responsible to</p>
<ul>
<li>Handle the networking logic</li>
<li>Act as a Proxy</li>
<li>Take care of third-party applications</li>
<li>Allow cluster operators to configure everything easily</li>
<li>Enable developers to focus on the actual business logic</li>
</ul>
<p>A service mesh will have a control plane that will inject this business logic automatically into every service. Once done, the microservices can talk to each other through proxies.</p>
<p>Core feature of service mesh: <strong>Traffic Splitting:</strong></p>
<ul>
<li>When you spin up a new service in response to a high number of requests, you only want to forward about 10% of the traffic to the new Service to make sure that it really works before distributing the traffic between all services. This may also be referred to as Canary Deployment</li>
</ul>
<p>&quot;In a service mesh, requests are routed 
between microservices through proxies in their own infrastructure layer.
For this reason, individual proxies that make up a service mesh are 
sometimes called “sidecars,” since they run <em>alongside</em> each service, rather than <em>within</em> them. Taken together, these “sidecar” proxies—decoupled from each service—form a mesh network.&quot;</p>
<p><img src="https://www.redhat.com/cms/managed-files/service-mesh-1680.png" alt="https://www.redhat.com/cms/managed-files/service-mesh-1680.png" /></p>
<p>&quot;A sidecar proxy sits alongside a microservice and routes requests to 
other proxies. Together, these sidecars form a mesh network.&quot;</p>
<p><strong>Service Mesh is just a paradigm and Istio is one of the implementations</strong></p>
<p>Istio allows Service A and Service B to communicate to each other. Once your microservices scale, you have more services, the service mesh becomes more complicated — it becomes more complicated to manage the connection between different services. That's where Istio comes in.</p>
<p>It runs on </p>
<ul>
<li>Kubernetes</li>
<li>Nomad</li>
<li>Console</li>
</ul>
<p>I will focus on Kubernetes.</p>
<p><strong>Features</strong></p>
<ul>
<li>Load Balancing: Receive some assurance of load handling — enabled some level of abstraction that enables services to have their own IP addresses.</li>
<li>Fine Grain Control: to make sure to have rules, fail-overs, fault connection</li>
<li>Access Control: Ensure that the policies are correct and enforceable</li>
<li>Visibility: Logging and graphing</li>
<li>Security: It manages your TSL certificates</li>
</ul>
<p>Additionally, Service Mesh makes it easier to discover problems within your microservice architecture that would be impossible to discover without.</p>
<p><strong>Components</strong> — Connect to the Control  Plane API within Kubernetes — note that this is the logic of Istio up to version 1.5. The latest versions only deal with Istiod.</p>
<ol>
<li>Pilot: Has A/B testing, has the intelligence how everything works, the driver of Istio</li>
<li>Cit: Allows Service A and Service B to talk to each other</li>
</ol>
<p><strong>How do we configure Istio?</strong></p>
<ol>
<li>You do not have to modify any Kubernetes Deployment and Service YAML files</li>
<li>Istio is configured separately from application configuration</li>
<li>Since Istio is implemented through Kubernetes Custom Resource Definitions (CRD), it can be easily extended with other Kubernetes-based plug-ins</li>
<li>It can be used like any other Kubernetes object</li>
</ol>
<p>The Istio-Ingress Gateway is an entry-point to our Kubernetes cluster. It runs as a pod in our cluster and acts as a LoadBalancer.</p>
<h2 id="service-mesh-interface"><a class="header" href="#service-mesh-interface">Service Mesh Interface</a></h2>
<p>With different projects and companies creating their own Service Mesh, the need for standards and specifications arise. One of those standards is provided by the Service Mesh Interface (SMI). In its most basic form, SMI provides a list of Service Mesh APIs. Separately SMI is currently a CNCF sandbox project.</p>
<p>SMI provides a standard interface for Service Mesh on Kubernetes</p>
<ul>
<li>Provides a basic set of features for the most common use cases</li>
<li>Flexible to support new use case over time</li>
</ul>
<p>Website with more information</p>
<p><strong>SMI covers the following</strong></p>
<ul>
<li>Traffic policy – apply policies like identity and transport encryption across services</li>
<li>Traffic telemetry – capture key metrics like error rate and latency between services</li>
<li>Traffic management – shift traffic between different services</li>
</ul>
<h2 id="other-service-mesh-examples"><a class="header" href="#other-service-mesh-examples">Other Service Mesh Examples</a></h2>
<ol>
<li>Gloo Mesh: Enterprise version of Istio Service Mesh but also has a Gloo Mesh open source version.</li>
<li><a href="https://linkerd.io/">Linkerd</a>: Its main advantage is that it is lighter than Istio itself. Note that Linkerd was origially developed by Buoyant. Linkerd specifically, is run through an <a href="https://linkerd.io/2019/10/03/linkerds-commitment-to-open-governance/">open governance model.</a></li>
<li><a href="https://www.nginx.com/products/nginx-service-mesh/">Nginx service mesh</a>: Focused on the data plane and security policies; platform agnostic; traffic orchestration and management</li>
</ol>
<h1 id="kubernetes-volumes"><a class="header" href="#kubernetes-volumes">Kubernetes Volumes</a></h1>
<h1 id="100days-resources-12"><a class="header" href="#100days-resources-12">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/qnlgJkntS1k">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-12"><a class="header" href="#learning-resources-12">Learning Resources</a></h1>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-volume-storage/</a></li>
<li><a href="https://codeburst.io/kubernetes-storage-by-example-part-1-27f44ae8fb8b">https://codeburst.io/kubernetes-storage-by-example-part-1-27f44ae8fb8b</a></li>
<li><a href="https://youtu.be/0swOh5C3OVM">https://youtu.be/0swOh5C3OVM</a></li>
</ul>
<h1 id="example-notes-12"><a class="header" href="#example-notes-12">Example Notes</a></h1>
<p>We cannot store data within our containers — if our pod crashes and restarts another container based on that container image, all of our state will be lost. <strong>Kubernetes does not give you data-persistence out of the box.</strong></p>
<p><strong>Volumes are references to files and directories</strong> made accessible to containers that form a pod. So, they basically keep track of the state of your application and if one pod dies the next pod will have access to the Volume and thus, the previously recorded state.</p>
<p>There are over 25 different Volume types within Kubernetes — some of which are specific to hosting providers e.g. AWS</p>
<p>The difference between volumes is the way that files and  directories are created.</p>
<p>Additionally, Volumes can also be used to access other Kubernetes resources such as to access the Docker socket.</p>
<p>The problem is that the storage has to be available across all nodes. When a pod fails and is restarted, it might be started on a different node.</p>
<p>Overall, Kubernetes Volumes have to be highly error-resistant — and even survive a crash of the entire cluster.</p>
<p>Volumes and Persistent Volumes are created like other Kubernetes resources, through YAML files.</p>
<p>Additionally, we can differentiate between <strong>remote and local volumes</strong> — each volume type has its own use case. </p>
<p>Local volumes are tied to a specific node and do not survive cluster disasters. Thus, you want to use remote volumes whenever possible.</p>
<pre><code class="language-jsx">apiVersion: v1
kind: Pod
metadata:
  name: empty-dir
spec:
  containers:
    - name: busybox-a
      command: ['tail', '-f', '/dev/null']
      image: busybox
      volumeMounts:
        - name: cache
          mountPath: /cache
    - name: busybox-b
      command: ['tail', '-f', '/dev/null']
      image: busybox
      volumeMounts:
        - name: cache
          mountPath: /cache
  volumes:
    - name: cache
      emptyDir: {}
</code></pre>
<p>Create the resource:</p>
<pre><code>kubectl apply -f empty-dir
</code></pre>
<p>Write to the file:</p>
<pre><code>kubectl exec empty-dir --container busybox-a -- sh -c &quot;echo \&quot;Hello World\&quot; &gt; /cache/hello.txt&quot;
</code></pre>
<p>Read what is within the file</p>
<pre><code>kubectl exec empty-dir --container busybox-b -- cat /cache/hello.txt
</code></pre>
<p>However, to ensure that the data will be saved beyond the creation and deletion of pods, we need Persistent volumes. Ephemeral volume types only have the lifetime of a pod — thus, they are not of much use if the pod crashes.</p>
<p>A persistent volume will have to take the same storage as the physical storage. </p>
<p>Storage in Kubernetes is an external plug-in to our cluster. This way, you can also have multiple different storage resources. </p>
<p>The storage resources is defined within the PersistentVolume YAML</p>
<p>A hostPath volume mounts a file or directory from the host node’s 
filesystem into your Pod. This is not something that most Pods will 
need, but it offers a powerful escape hatch for some applications.</p>
<p><em>— Kubernetes — <a href="https://kubernetes.io/docs/concepts/storage/volumes/">Volumes</a></em></p>
<pre><code class="language-jsx">apiVersion: v1
kind: Pod
metadata:
  name: host-path
spec:
  containers:
    - name: busybox
      command: ['tail', '-f', '/dev/null']
      image: busybox
      volumeMounts:
        - name: data
          mountPath: /data
  volumes:
  - name: data
    hostPath:
      path: /data
</code></pre>
<p>Create the volume</p>
<pre><code class="language-jsx">kubectl apply -f empty-dir
</code></pre>
<p>Read from the volume</p>
<pre><code class="language-jsx">kubectl exec host-path -- cat /data/hello.txt
</code></pre>
<h1 id="kind"><a class="header" href="#kind">KinD</a></h1>
<h1 id="100days-resources-13"><a class="header" href="#100days-resources-13">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/Bdw5saYQMvY">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-13"><a class="header" href="#learning-resources-13">Learning Resources</a></h1>
<ul>
<li><a href="https://kind.sigs.k8s.io/">Official Documentation</a></li>
</ul>
<h1 id="example-notes-13"><a class="header" href="#example-notes-13">Example Notes</a></h1>
<h2 id="setting-up-kind-on-windows-and-cluster-comparison"><a class="header" href="#setting-up-kind-on-windows-and-cluster-comparison">Setting up Kind on Windows and Cluster Comparison</a></h2>
<p>These are the docs that I used to set-up all of my resources on Windows:</p>
<p><strong>Setting up the Ubuntu in Windows</strong></p>
<ul>
<li><a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10#step-4---download-the-linux-kernel-update-package">https://docs.microsoft.com/en-us/windows/wsl/install-win10#step-4---download-the-linux-kernel-update-package</a></li>
<li><a href="https://github.com/microsoft/WSL/issues/4766">https://github.com/microsoft/WSL/issues/4766</a></li>
</ul>
<p><strong>Use WSL in Code</strong></p>
<ul>
<li><a href="https://docs.docker.com/docker-for-windows/wsl/">https://docs.docker.com/docker-for-windows/wsl/</a></li>
<li><a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-wsl">https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-wsl</a></li>
</ul>
<p><strong>Kubectl installation</strong></p>
<ul>
<li><a href="https://devkimchi.com/2018/06/05/running-kubernetes-on-wsl/">https://devkimchi.com/2018/06/05/running-kubernetes-on-wsl/</a></li>
</ul>
<p><strong>Create Kind cluster</strong></p>
<ul>
<li><a href="https://kubernetes.io/blog/2020/05/21/wsl-docker-kubernetes-on-the-windows-desktop/">https://kubernetes.io/blog/2020/05/21/wsl-docker-kubernetes-on-the-windows-desktop/</a></li>
</ul>
<p>If you want to run microk8s on WSL, you have to get a snap workaround described here</p>
<ul>
<li><a href="https://github.com/microsoft/WSL/issues/5126">https://github.com/microsoft/WSL/issues/5126</a></li>
</ul>
<h2 id="running-a-local-cluster"><a class="header" href="#running-a-local-cluster">Running a local cluster</a></h2>
<p>Especially when you are just getting started, you might want to spin up a local cluster on your machine. This will allow you to run tests, play around with the resources and more without having to worry much about messing something up :) — If you watched any of my previous videos, you will have already a good understand of how much I enjoy trying out different things — setting things up just to try something out — gain a better understanding — and then to delete everything in the next moment.</p>
<p>Also, you might have seen that I already have two videos on microk8s —mainly since the minikube set-up on my Ubuntu did not properly work. Now that I am on Windows, I might actually have more options. So let's take a look at those and see how they compare.</p>
<h2 id="kubernetes-in-docker"><a class="header" href="#kubernetes-in-docker">Kubernetes in Docker</a></h2>
<p>When you install Docker for Desktop (or however you call it) you can enable Kubernetes:</p>
<p>The set-up will take a few seconds but then you have access to a local cluster both from the normal Windows terminal (I am still new to Windows, so excuse any terminology that is off) or through the WSL.</p>
<pre><code class="language-jsx">kubectl config get-contexts
</code></pre>
<p>and you will see a display of the different clusters that you have set-up. This will allow you to switch to a different cluster:</p>
<pre><code class="language-jsx">kubectl config use-context &lt;context name&gt;
</code></pre>
<p>Below are several other options highlighted.</p>
<h2 id="minikube"><a class="header" href="#minikube"><strong>minikube</strong></a></h2>
<p>minikube is probably the best known of the three; maybe because it is the oldest. When you are using minikube, it will spin up a VM that runs a single Kubernetes node. To do so it needs hypervisor. Now, if you have never interacted with much virtualisation technology, you might think of a hypervisor as something like this:</p>
<p>I assure you, it is not. So what is a Hypervisor then? A Hypervisor is basically a form of software, firmware, or hardware that is used to set-up virtual environments on your machine.</p>
<p>Running minikube, you can spin up multiple nodes as well, each will be running their own VM (Virtual Machine).</p>
<p>For those of you, who are really into Dashboards, minikube provides a Dashboard, too! Personally, I am not too much of a fan but that is your choice. If you would like some sort of Dashboard, I highly recommend you k9s. <a href="https://youtu.be/5Nhbl6LwP2o">Here is my introductory video if you are curious.</a></p>
<p>This is what the minikube Dashboard looks like — just your usual UI 😊</p>
<p>Now how easy is the installation of minikube? Yes, I mentioned that I had some problems installing minikube on Ubuntu and thus, went with microk8s. At that time, I did not know about kind yet. Going back to the original question, if you are using straight up Windows it is quite easy to install, if you are using Linux in Windows however, it might be a bit different — tbh I am a really impatient person, so don't ask me.</p>
<p><a href="https://minikube.sigs.k8s.io/docs/start/">Documentation</a></p>
<h2 id="what-is-kind"><a class="header" href="#what-is-kind">What is kind?</a></h2>
<p>Kind is quite different to minikube, instead of running the nodes in VMs, it will run nodes as Docker containers. Because of that, it is supposed to start-up faster — I am not sure how to test this, they are both spinning up the cluster in an instance and that is good enough for me.</p>
<p>However, note that kind requires more space on your machine to run than etiher microk8s or minikube. In fact, microk8s is actually the smallest of the three. </p>
<p><a href="https://brennerm.github.io/posts/minikube-vs-kind-vs-k3s.html">Like detailed in this article, you can</a></p>
<ul>
<li>With 'kind load docker-image my-app:latest' the image is available for use in your cluster</li>
</ul>
<p>Which is an additional feature.</p>
<p>If you decide to use kind, you will get the most out of it if you are fairly comfortable to use YAML syntax since that will allow you to define different cluster types. </p>
<p><a href="https://kind.sigs.k8s.io/docs/user/quick-start/">Documentation</a></p>
<p><a href="https://kubernetes.io/blog/2020/05/21/wsl-docker-kubernetes-on-the-windows-desktop/">The documentation that I used to install it</a></p>
<h2 id="microk8s"><a class="header" href="#microk8s">microk8s</a></h2>
<p>Microk8s is in particular useful if you want to run a cluster on small devices; it is better tested in ubuntu than the other tools. Resulting, you can install it with snap super quickly! In this case, it will basically run the cluster separate from the rest of the stuff on your computer. </p>
<p>It also allows for multi-node clusters, however, I did not try that yet, so I don't know how well that actually works. </p>
<p>Also note that if you are using microk8s on MacOS or Windows, you will need a hypevisor of sorts. Running it on Ubuntu, you do not.</p>
<p><a href="https://microk8s.io/">Documentation</a></p>
<p>My video</p>
<p><a href="https://youtu.be/uU-8Zcst5Qk">https://youtu.be/uU-8Zcst5Qk</a></p>
<h2 id="direct-comparison"><a class="header" href="#direct-comparison">Direct comparison</a></h2>
<p>This article by Max Brenner offers a really nice comparison between the different tools with a comparison table <a href="https://brennerm.github.io/posts/minikube-vs-kind-vs-k3s.html">https://brennerm.github.io/posts/minikube-vs-kind-vs-k3s.html</a></p>
<h1 id="k3s-and-k3sup"><a class="header" href="#k3s-and-k3sup">K3s and K3sup</a></h1>
<h1 id="100days-resources-14"><a class="header" href="#100days-resources-14">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/fRcOSqD9p6w">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-14"><a class="header" href="#learning-resources-14">Learning Resources</a></h1>
<ul>
<li><a href="https://github.com/k3s-io/k3s/blob/master/README.md">Overview README</a></li>
<li><a href="https://k3s.io/">Website</a></li>
<li><a href="https://rancher.com/docs/k3s/latest/en/">Documentation</a></li>
</ul>
<h1 id="example-notes-14"><a class="header" href="#example-notes-14">Example Notes</a></h1>
<p>When you are choosing a Kubernetes distribution, the most obvious one is going to be K8s, which is used by major cloud vendors and many more. However, if you just want to play around on your local machine with Kubernetes, test some tools or learn about Kubernetes resources, you would likely go with something like minikube, microk8s or kind. </p>
<p>Now we just highlighted two use cases for different types of Kubernetes clusters. What about use cases where you want to run Kubernetes on really small devices, such as raspberry pis? What about IoT devices? Generally, devices where you want to run containers effectively without consuming too much resources.</p>
<p>In those cases, you could go with K3s.</p>
<h2 id="what-is-k3s"><a class="header" href="#what-is-k3s">What is K3s?</a></h2>
<p>In short, k3s is half the size in terms of memory footprint than &quot;normal Kubernetes&quot;. The origin of k3s is Rio, which was developed by Rancher. However, they then decided to branch it out of Rio into its own tool — which became K3s. It was more of a figure it out by doing it.</p>
<p>The important aspect of k3s is that it was oriented around production right from the beginning.</p>
<p>You want to be able to run Kubernetes in highly resource constraint environments — which is not always possible with pure Kubernetes. </p>
<p>K3s is currently a CNCF sandbox project — the development is led by Rancher, which provides Kubernetes as a service (?)</p>
<ul>
<li>Instead of Docker, it runs Containerd — Note that Kubernetes itself is also moving to Containerd as its container runtime. This does not mean that you will not be able to run Docker containers. If I just scared you, please watch this video to clarify.</li>
</ul>
<p><strong>Designed based on the following goals:</strong></p>
<ol>
<li>Lightweight: Show work on small resource environment</li>
<li>Compatibility: You should be able to use most of the tools you can use with &quot;normal k8s&quot;</li>
<li>Ethos: Everything you need to use k3s is built right in</li>
</ol>
<p><em>Btw: K3s is described as the second most popular Kubernetes distribution (Read in a Medium post, please don't quote me on this)</em></p>
<p>How does k3s differ from &quot;normal&quot; Kubernetes?</p>
<p><a href="https://youtu.be/FmLna7tHDRc">https://youtu.be/FmLna7tHDRc</a></p>
<p><strong>Do you have questions? We have answers!</strong></p>
<p>Now I recorded last week an episode with Alex Ellis, who built k3sup, which can be used to deploy k3s. Here are some of the questions that I had before the live recording that are answered within the recording itself:</p>
<ul>
<li>Let's hear a bit about the background of k3sup — how did it come about?</li>
<li>How are both pronounced?</li>
<li>How would you recommend learning about k3s — let's assume you are complete new, where do you start?</li>
<li>Walking through the k3s architecture <a href="https://k3s.io/">https://k3s.io/</a></li>
<li>What is the difference between k8s and k3s</li>
<li>When would I prefer to use k8s over k3s</li>
<li>What can I NOT do with k3s? Or what would I NOT want to do?</li>
<li>Do I need a VM to run k3s? It is mentioned in some blog posts — let's assume I do not have a raspberry pi — I have a VM, you can set them up quite easily; why would I run K3s on a VM?</li>
<li>So we keep discussing that this is great for a Kubernetes homelab or IoT devices — is that not a bit of an overkills to use Kubernetes with it?</li>
<li>Is the single node k3s similar to microk8s — having one instance that is both worker node</li>
</ul>
<p><a href="https://youtu.be/_1kEF-Jd9pw">https://youtu.be/_1kEF-Jd9pw</a></p>
<p><strong>Use cases for k3s:</strong></p>
<ul>
<li>Single node clusters</li>
<li>Edge</li>
<li>IoT</li>
<li>CI</li>
<li>Development Environments and Test Environments</li>
<li>Experiments, useful for academia</li>
<li>ARM</li>
<li>Embedding K8s</li>
<li>Situations where a PhD in K8s clusterology is infeasible</li>
</ul>
<h3 id="install-k3s"><a class="header" href="#install-k3s">Install k3s</a></h3>
<p>There are three ways for installing k3s*</p>
<ul>
<li>The quick way shown below directly with k3s</li>
<li>Supposedly easier way with k3s up</li>
<li>The long way that is detailed over several docs pages <a href="https://rancher.com/docs/k3s/latest/en/installation/">https://rancher.com/docs/k3s/latest/en/installation/</a></li>
</ul>
<p>*Actually there are several more ways to install k3s like highlighted in this video:</p>
<p><a href="https://youtu.be/O3s3YoPesKs">https://youtu.be/O3s3YoPesKs</a></p>
<p>This is the installation script:</p>
<pre><code class="language-jsx">curl -sfL https://get.k3s.io | sh -
</code></pre>
<p>Note that this might take up to 30 sec. Once done, you should be able to run</p>
<pre><code class="language-jsx">k3s kubectl get node
</code></pre>
<p>What it does</p>
<blockquote>
<p>The K3s service will be configured to automatically restart after node reboots or if the process crashes or is killed
Additional utilities will be installed, including kubectl, crictl, ctr, <a href="http://k3s-killall.sh/">k3s-killall.sh</a>, and <a href="http://k3s-uninstall.sh/">k3s-uninstall.sh</a>
A kubeconfig file will be written to /etc/rancher/k3s/k3s.yaml and the kubectl installed by K3s will automatically use it</p>
</blockquote>
<p>Once this is done, you have to set-up the worker nodes that are used by k3s</p>
<pre><code class="language-jsx">curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken sh -
</code></pre>
<h3 id="once-installed-access-the-cluster"><a class="header" href="#once-installed-access-the-cluster">Once installed, access the cluster</a></h3>
<p>Leverage the KUBECONFIG environment variable:</p>
<pre><code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
kubectl get pods --all-namespaces
helm ls --all-namespaces

</code></pre>
<p>If you do not set KUBECONFIG as an environment variable,</p>
<h3 id="installing-with-k3sup"><a class="header" href="#installing-with-k3sup">Installing with k3sup</a></h3>
<p>The following is taken from the k3sup <a href="https://github.com/alexellis/k3sup">READM</a>E file</p>
<pre><code class="language-jsx">$ curl -sLS https://get.k3sup.dev | sh
$ sudo install k3sup /usr/local/bin/
$ k3sup --help
</code></pre>
<p>There is a lot to unpack... WHAT DOES THIS SENTENCE MEAN?</p>
<blockquote>
<p>This tool uses ssh to install k3s to a remote Linux host. You can also use it to join existing Linux hosts into a k3s cluster as agents.</p>
</blockquote>
<p>If you want to get an A to Z overview, watch the following video</p>
<p><a href="https://youtu.be/2LNxGVS81mE">https://youtu.be/2LNxGVS81mE</a></p>
<p>If you are still wondering about what k3s, have a look at this video</p>
<p><a href="https://youtu.be/-HchRyqNtkU">https://youtu.be/-HchRyqNtkU</a></p>
<p><strong>How is Kubernetes modified?</strong></p>
<ul>
<li>This is not a Kubernetes fork</li>
<li>Added rootless support</li>
<li>Dropped all third-party storage drivers, completely CSI is supported and preferred</li>
</ul>
<p>Following this tutorial; note that there are many others that you could use — have a look at their documentation:</p>
<p><a href="https://rancher.com/blog/2020/k3s-high-availability">https://rancher.com/blog/2020/k3s-high-availability</a></p>
<h1 id="kustomize"><a class="header" href="#kustomize">Kustomize</a></h1>
<h1 id="100days-resources-15"><a class="header" href="#100days-resources-15">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/Gc-nO3oxxk8">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-15"><a class="header" href="#learning-resources-15">Learning Resources</a></h1>
<ul>
<li>Their <a href="https://kustomize.io/">webiste</a> has lots of amazing videos</li>
</ul>
<h1 id="example-notes-15"><a class="header" href="#example-notes-15">Example Notes</a></h1>
<p>Configuration Management for Kubernetes</p>
<p>— &quot;A template free way to customize application configuration that simplifies the use of off-the-shelf applications&quot;</p>
<p>When I create a YAML, the manifests go into the API server of the main node; the cluster then aims to create the resources within the cluster to match the desired state defined in the YAML</p>
<p>Different to what we have seen before in the videos, YAML can get super complex! Additionally, there are several aspects of the state of our deployment that we want to frequently change. Including:</p>
<ul>
<li>Namespaces, Labels, Container Registry, Tags and more</li>
</ul>
<p>Then, we have resources and processes that we want to change a bit less frequently, such as</p>
<ul>
<li>Management Parameters</li>
<li>Environment-specific processes and resources</li>
<li>Infrastructure mapping</li>
</ul>
<p>Kustomize allows you to specify different values of your Kubernetes resources for different situations. </p>
<p>To make your YAML resources more dynamic and to apply variations between environments, you can use Kustomize.</p>
<h2 id="lets-get-started-using-kustomize"><a class="header" href="#lets-get-started-using-kustomize">Let's get started using Kustomize</a></h2>
<p>Install Kustomize — just reading about it is not going to help us.</p>
<p><a href="https://kubectl.docs.kubernetes.io/installation/">Here is their official documentation</a> </p>
<p>However, their options did not work for the Linux installation, which I also need on WSL — this one worked:</p>
<p><a href="https://weaveworks-gitops.awsworkshop.io/20_weaveworks_prerequisites/15_install_kustomize.html">https://weaveworks-gitops.awsworkshop.io/20_weaveworks_prerequisites/15_install_kustomize.html</a> </p>
<p>Kustomize is part of kubectl so it should work without additional installation using 'kubectl -k' to specify that you want to use kustomize.</p>
<p>Next, scrolling through their documentation, they provide some amazing resources with examples on how to use kubectl correctly — but I am looking for kustomize example</p>
<p>Have a look at their guides if you are curious <a href="https://kubectl.docs.kubernetes.io/guides/">https://kubectl.docs.kubernetes.io/guides/</a> </p>
<p>So with kustomize, we want to have our YAML tempalte and then customize the values provided to that resource manifest. However, each directory that is referenced within kustomized must have its own kustomization.yaml file.</p>
<p>First, let's set-up a deployment and a service, like we did in one of the previous days.</p>
<p>The Deployment:</p>
<pre><code class="language-jsx">apiVersion: apps/v1
kind: Deployment
metadata:
  name: react-application
spec:
  replicas: 2
  selector:
    matchLabels:
      run: react-application
  template:
    metadata:
      labels:
        run: react-application
    spec:
      containers:
      - name: react-application
        image: anaisurlichs/react-article-display:master
        ports:
          - containerPort: 80
        imagePullPolicy: Always
				env:
            - name: CUSTOM_ENV_VARIABLE
              value: Value defined by Kustomize ❤️
</code></pre>
<p>The service</p>
<pre><code class="language-jsx">apiVersion: v1
kind: Service
metadata:
  name: react-application
  labels:
    run: react-application
spec:
  type: NodePort
  ports:
  - port: 8080
    targetPort: 80
    protocol: TCP
    name: http
  selector:
    run: react-application
</code></pre>
<p>Now we want to customize that deployment with specific values.</p>
<p>Set-up a file called 'kustomization.yaml'</p>
<pre><code class="language-jsx">**resources:
  - deployment.yaml
  - service.yaml**
</code></pre>
<p>Within this file, we will specify specific values that we will use within our Deployment resource. From a Kubernetes perspective, this is just another Kuberentes resource.</p>
<p>One thing worth mentioning here is that Kustomize allows us to combine manifests from different repositories.</p>
<p>So once you want to apply the kustomize resources, you can have a look at the changed resources:</p>
<pre><code class="language-jsx">kustomize build .
</code></pre>
<p>this will give you the changed YAML files with whatever you had defined in your resources.</p>
<p>Now you can forward those resources into another file:</p>
<pre><code class="language-jsx">kustomize build . &gt; mydeployment.yaml
</code></pre>
<p>Now if you have kubectl running, you could specify the resource that you want to use through :</p>
<pre><code class="language-jsx">kubectl create -k .
</code></pre>
<p>-k refers here to - -kustomize, you could use that flag instead if you wanted to.</p>
<p>This will create our Deployment and service — basically all of the resources that have been defined in your kustomization.yaml file</p>
<pre><code class="language-jsx">kubectl get pods -l &lt;label specifying your resources&gt;
</code></pre>
<p>A different kustomization.yaml</p>
<p>Use this within one environment</p>
<pre><code class="language-jsx">resources:
- ../../base
namePrefix: dev-
namespace: development
commonLabels:
  environment: development

</code></pre>
<p>And this file for the other environment</p>
<pre><code class="language-jsx">resources:
- ../../base
namePrefix: prod-
namespace: production
commonLabels:
  environment: production
  sre-team: blue
</code></pre>
<p>Create the new file that includes kustomize and the original resources</p>
<pre><code class="language-jsx">kustomize build . &gt; mydeployment.yaml
</code></pre>
<p>Check-out the video for more 🙂</p>
<h2 id="resources"><a class="header" href="#resources">Resources</a></h2>
<p>For a comprehensive overview: <a href="https://www.youtube.com/watch?v=Twtbg6LFnAg&amp;t=137s">https://www.youtube.com/watch?v=Twtbg6LFnAg&amp;t=137s</a></p>
<p>And another hands-on walkthrough: <a href="https://youtu.be/ASK6p2r-Yrk">https://youtu.be/ASK6p2r-Yrk</a></p>
<p>A really good blog post on Kustomize: <a href="https://blog.scottlowe.org/2019/09/13/an-introduction-to-kustomize/">https://blog.scottlowe.org/2019/09/13/an-introduction-to-kustomize/</a></p>
<h1 id="terraform"><a class="header" href="#terraform">Terraform</a></h1>
<h1 id="100days-resources-16"><a class="header" href="#100days-resources-16">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/pbrsrKFhohk">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-16"><a class="header" href="#learning-resources-16">Learning Resources</a></h1>
<ul>
<li><a href="https://youtu.be/UleogrJkZn0">https://youtu.be/UleogrJkZn0</a></li>
<li><a href="https://youtu.be/HmxkYNv1ksg">https://youtu.be/HmxkYNv1ksg</a></li>
<li><a href="https://youtu.be/l5k1ai_GBDE">https://youtu.be/l5k1ai_GBDE</a></li>
<li><a href="https://learn.hashicorp.com/tutorials/terraform/kubernetes-provider?in=terraform/use-case">https://learn.hashicorp.com/tutorials/terraform/kubernetes-provider?in=terraform/use-case</a></li>
<li><a href="https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/guides/getting-started">https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/guides/getting-started</a></li>
</ul>
<h1 id="example-notes-16"><a class="header" href="#example-notes-16">Example Notes</a></h1>
<p>With Terraform, you have a DevOps first approach. It is an open-source tools, original developed by <a href="https://www.hashicorp.com/">HashiCorp</a></p>
<p>It is an infrastructure provisioning tool that allows you to store your infrastructure set-up as code.</p>
<p>Normally, you would have:</p>
<ol>
<li>Provisioning the Infrastructure
<ol>
<li>Usually has top be done within a specific order</li>
</ol>
</li>
<li>Deploying the application</li>
</ol>
<p>It has a strong open community and is pluggable by design — meaning that it is used by vast number of organisations</p>
<p>Allows you to manage your infrastructure as code in a declarative format. </p>
<p>What would be the alternative?</p>
<ul>
<li>Pressing buttons in a UI — the problem with this is that it is not repeatable across machines</li>
</ul>
<p>Especially important in the microservices world! It would otherwise be more time-consuming and error-prone to set-up all the resources needed for every microservice.</p>
<p>What is a declarative approach?</p>
<p>Current state vs. desired state</p>
<p>Define what the end-result should be.</p>
<p>When you are first defining your resources, this is less needed. However, once you are adding or changing resources, this is a lot more important. It does not require you to know how many resources and what resources are currently available but it will just compare what is the current state and what has to happen to create the desired state.</p>
<p><strong>Different stages of using Terraform:</strong></p>
<ol>
<li>Terraform file that defines our resources e.g. VM, Kubernetes cluster, VPC</li>
<li>Plan Phase — Terraform command: Compares the desired state to what currently exists on our cluster
if everything looks good:</li>
<li>Apply Phase: Terraform is using your API token to spin up your resources</li>
</ol>
<p>When you defined your Terraform resources, you define a provider: </p>
<p>Connects you to a cloud provider/infrastructure provider. A provider can also be used to spin up platforms and manage SAAS offerings</p>
<p>So, beyond IAAS, you can also manage other platforms and resources through Terraform.</p>
<p>You can configure your provider in either of the following ways:</p>
<ol>
<li>Use cloud-specific auth plugins (for example, <code>eks get-token</code>, <code>az get-token</code>, <code>gcloud config</code>)</li>
<li>Use <a href="https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/guides/getting-started#provider-setup">oauth2 token</a></li>
<li>Use <a href="https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs#statically-defined-credentials">TLS certificate credentials</a></li>
<li>Use <code>kubeconfig</code> file by setting <strong>both</strong> <code>[config_path](https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs#config_path)</code> and <code>[config_context](https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs#config_context)</code></li>
<li>Use <a href="https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs#statically-defined-credentials">username and password (HTTP Basic Authorization)</a></li>
</ol>
<h2 id="lets-try-out-terraform"><a class="header" href="#lets-try-out-terraform">Let's try out Terraform</a></h2>
<ol>
<li><a href="https://learn.hashicorp.com/tutorials/terraform/install-cli?in=terraform/aws-get-started">Install Terraform</a></li>
<li>Initialise cloud provider</li>
<li>Run &quot;terraform init&quot;</li>
<li>Set-up a security group</li>
<li>Make sure that you set up the file</li>
<li>Try whether or not it works with &quot;terraform plan&quot; . Here we would expect something to add but not necessarily something to change or to destroy.</li>
<li>Don't display your credentials openly, you would probably want to use something like Ansible vault or similar to store your credentials, do not store in plain text</li>
<li>Once we have finished setting up our resources, we want to destroy them &quot;terraform destroy&quot;</li>
</ol>
<p>tf is the file extension that Terraform uses</p>
<p>Note that you can find all available information on a data source.</p>
<p>Common question: What is the difference between Ansible and Terraform</p>
<ol>
<li>They are both used for provisioning the infrastructure</li>
<li>However, Terraform also has the power to provision the infrastructure</li>
<li>Ansible is better for configuring the infrastructure and deploying the application</li>
<li>Both could be used in combination</li>
</ol>
<p>I am doing this example on my local kind cluster — have a look at one of the previous videos to see how to set-up kind</p>
<p>We are using a mixture of the following tutorials</p>
<ul>
<li><a href="https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/guides/getting-started">https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs/guides/getting-started</a></li>
<li><a href="https://learn.hashicorp.com/tutorials/terraform/kubernetes-provider?in=terraform/use-case">https://learn.hashicorp.com/tutorials/terraform/kubernetes-provider?in=terraform/use-case</a></li>
</ul>
<p>Create your cluster e.g. with kind</p>
<p>kind-config.yaml</p>
<pre><code class="language-jsx">kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  extraPortMappings:
  - containerPort: 30201
    hostPort: 30201
    listenAddress: &quot;0.0.0.0&quot;
</code></pre>
<p>Then run</p>
<pre><code class="language-jsx">kind create cluster --name terraform-learn --config kind-config.yaml
</code></pre>
<p>For our terraform.tfvar file, we need the following information </p>
<ul>
<li><code>[host](https://learn.hashicorp.com/tutorials/terraform/kubernetes-provider?in=terraform/use-case#host)</code> corresponds with <code>clusters.cluster.server</code>.</li>
<li><code>[client_certificate](https://learn.hashicorp.com/tutorials/terraform/kubernetes-provider?in=terraform/use-case#client_certificate)</code> corresponds with <code>users.user.client-certificate</code>.</li>
<li><code>[client_key](https://learn.hashicorp.com/tutorials/terraform/kubernetes-provider?in=terraform/use-case#client_key)</code> corresponds with <code>users.user.client-key</code>.</li>
<li><code>[cluster_ca_certificate](https://learn.hashicorp.com/tutorials/terraform/kubernetes-provider?in=terraform/use-case#cluster_ca_certificate)</code> corresponds with <code>clusters.cluster.certificate-authority-data</code>.</li>
</ul>
<p>To get those, run:
&quot;kubectl config view --minify --flatten --context=kind-terraform-learn&quot;</p>
<pre><code class="language-jsx"># terraform.tfvars

host                   = &quot;https://127.0.0.1:32768&quot;
client_certificate     = &quot;LS0tLS1CRUdJTiB...&quot;
client_key             = &quot;LS0tLS1CRUdJTiB...&quot;
cluster_ca_certificate = &quot;LS0tLS1CRUdJTiB...&quot;
</code></pre>
<p>This is our terraform .tf file</p>
<pre><code class="language-jsx">terraform {
  required_providers {
    kubernetes = {
      source = &quot;hashicorp/kubernetes&quot;
    }
  }
}

variable &quot;host&quot; {
  type = string
}

variable &quot;client_certificate&quot; {
  type = string
}

variable &quot;client_key&quot; {
  type = string
}

variable &quot;cluster_ca_certificate&quot; {
  type = string
}

provider &quot;kubernetes&quot; {
  host = var.host

  client_certificate     = base64decode(var.client_certificate)
  client_key             = base64decode(var.client_key)
  cluster_ca_certificate = base64decode(var.cluster_ca_certificate)
}

resource &quot;kubernetes_namespace&quot; &quot;test&quot; {
  metadata {
    name = &quot;nginx&quot;
  }
}
resource &quot;kubernetes_deployment&quot; &quot;test&quot; {
  metadata {
    name      = &quot;nginx&quot;
    namespace = kubernetes_namespace.test.metadata.0.name
  }
  spec {
    replicas = 2
    selector {
      match_labels = {
        app = &quot;MyTestApp&quot;
      }
    }
    template {
      metadata {
        labels = {
          app = &quot;MyTestApp&quot;
        }
      }
      spec {
        container {
          image = &quot;nginx&quot;
          name  = &quot;nginx-container&quot;
          port {
            container_port = 80
          }
        }
      }
    }
  }
}
resource &quot;kubernetes_service&quot; &quot;test&quot; {
  metadata {
    name      = &quot;nginx&quot;
    namespace = kubernetes_namespace.test.metadata.0.name
  }
  spec {
    selector = {
      app = kubernetes_deployment.test.spec.0.template.0.metadata.0.labels.app
    }
    type = &quot;NodePort&quot;
    port {
      node_port   = 30201
      port        = 80
      target_port = 80
    }
  }
}
</code></pre>
<p>Now we can run</p>
<pre><code class="language-jsx">terraform init

terraform plan

terraform apply
</code></pre>
<p>Make sure that all your resources are within the right path and it should work.</p>
<h1 id="crossplane"><a class="header" href="#crossplane">Crossplane</a></h1>
<h1 id="100days-resources-17"><a class="header" href="#100days-resources-17">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/Dw0SMLHZvXM">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-17"><a class="header" href="#learning-resources-17">Learning Resources</a></h1>
<ul>
<li>The Website <a href="https://crossplane.io/">https://crossplane.io/</a></li>
<li>The Docs <a href="https://crossplane.io/docs/v1.0/">https://crossplane.io/docs/v1.0/</a></li>
</ul>
<h1 id="example-notes-17"><a class="header" href="#example-notes-17">Example Notes</a></h1>
<p>Some Highlights from their website:</p>
<blockquote>
<p>Crossplane is an open source Kubernetes add-on that supercharges your Kubernetes clusters enabling you to provision and manage infrastructure, services, and applications from kubectl.</p>
</blockquote>
<p>Crossplane is a CNCF sandbox project which extends the Kubernetes API to manage and compose infrastructure.</p>
<p>Crossplane introduces multiple building blocks that enable you to provision, compose, and consume infrastructure using the Kubernetes API. These individual concepts work together to allow for powerful separation of concern between different personas in an organization, meaning that each member of a team interacts with Crossplane at an appropriate level of abstraction.</p>
<p>Crossplane uses Kubernetes Custom Resource Definitions to manage your infrastructure and resources. Basically, all of the resources used are Kubernetes resources themselves, making it possible for Crossplane to interact with any other Kubernetes Resources.</p>
<p>In short, Crossplane manages all of your resources through a Kubernetes API.</p>
<p>The Benefits:</p>
<ol>
<li>Manage your infrastructure directly through kubectl</li>
<li>Supports infrastructure from all major cloud providers + the maintainers are consistently working on new providers</li>
<li>You can build your own infrastructure abstraction on top of Crossplane</li>
<li>Crossplane is based on CRD, so you can run it anywhere + it is extensible</li>
<li>One place of truth for your infrastructure</li>
<li>Use custom APIs to manage policies, hiding infrastructure complexity and safely customise application</li>
<li>Declarative Infrastructure Configuration —  infrastructure managed through Crossplane is accessible via kubectl, configurable with YAML, and self-healing right out of the box.</li>
</ol>
<p><strong>GitOps best practices</strong></p>
<p>Crossplane can be combined with CI/CD pipelines, this allows to implement GitOps best practices. GitOps is a deployment strategy, whereby everything, the desired state of the application is defined in git.</p>
<blockquote>
<p>infrastructure configuration that can be versioned, managed, and deployed using your favorite tools and existing processes</p>
</blockquote>
<p>To use crossplane you initially need any type of Kubernetes Cluster — it could be Minikube, where you then install it! Once you have access to the Crossplane Kubernetes resources </p>
<blockquote>
<p>On their own, custom resources let you store and retrieve structured data. When you combine a custom resource with a custom controller, custom resources provide a true declarative API.</p>
</blockquote>
<h2 id="install-crossplane"><a class="header" href="#install-crossplane">Install Crossplane</a></h2>
<p>Crossplane combines custom resource definitions and custom controllers — those are both Kubernetes resources. So you install Kubernetes resources that are used to manage Crossplane.</p>
<p><a href="https://crossplane.io/docs/v1.0/getting-started/install-configure.html#start-with-a-self-hosted-crossplane">https://crossplane.io/docs/v1.0/getting-started/install-configure.html#start-with-a-self-hosted-crossplane</a></p>
<p>Prerequisites</p>
<ol>
<li>Have kubectl installed</li>
<li>Have Helm installed</li>
<li>Have a local Kubernetes cluster, or any Kubernetes cluster</li>
<li>Have access to the account e.g. Azure or Google where you want to provision your infrastructure</li>
</ol>
<p>First, you need to install Corssplane on a Kubernetes cluster. Note that this can be any cluster, it does not have to be the cluster that you will be using Crossplane on to provision Infrastructure. This just provides the Corssplane interface for you to be able to use Corssplane. For instance, in my case, I am going to be using Crossplane on my Docker Cluster and then povision Infrastructure on Azure.</p>
<p>Note that we are going the self-provisioned route:</p>
<pre><code class="language-jsx">kubectl create namespace crossplane-system

helm repo add crossplane-stable https://charts.crossplane.io/stable
helm repo update

helm install crossplane --namespace crossplane-system crossplane-stable/crossplane
</code></pre>
<p>Check that all resources are running properly</p>
<pre><code class="language-jsx">helm list -n crossplane-system

kubectl get all -n crossplane-system
</code></pre>
<p>Next, we can install the Crossplane CLI that will be used in combination with kubectl: </p>
<pre><code class="language-jsx">curl -sL https://raw.githubusercontent.com/crossplane/crossplane/release-1.0/install.sh | sh
</code></pre>
<p>Depending on the provider that you want to use, you can select Azure, GCP, AWS, or Alibaba (as of the time of writing)</p>
<p>On the same cluster that you installed the previous resources on:</p>
<pre><code class="language-jsx">kubectl crossplane install provider crossplane/provider-azure:v0.14.0
</code></pre>
<p>Now wait until the Provider becomes healthy:</p>
<pre><code class="language-jsx">kubectl get provider.pkg --watch
</code></pre>
<p>Now it will get a bit tricks. If you are using Azure, make sure that you are logged into your account.  For that, first install the Azure CLI :</p>
<pre><code class="language-jsx">curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
</code></pre>
<p>And then login with:</p>
<pre><code class="language-jsx">az login
</code></pre>
<p>Once you are logged in, you can run the following commands</p>
<pre><code class="language-jsx">**# create service principal with Owner role
az ad sp create-for-rbac --sdk-auth --role Owner &gt; &quot;creds.json&quot;

# we need to get the clientId from the json file to add Azure Active Directory
# permissions.
if which jq &gt; /dev/null 2&gt;&amp;1; then
  AZURE_CLIENT_ID=$(jq -r &quot;.clientId&quot; &lt; &quot;./creds.json&quot;)
else
  AZURE_CLIENT_ID=$(cat creds.json | grep clientId | cut -c 16-51)
fi

RW_ALL_APPS=1cda74f2-2616-4834-b122-5cb1b07f8a59
RW_DIR_DATA=78c8a3c8-a07e-4b9e-af1b-b5ccab50a175
AAD_GRAPH_API=00000002-0000-0000-c000-000000000000

az ad app permission add --id &quot;${AZURE_CLIENT_ID}&quot; --api ${AAD_GRAPH_API} --api-permissions ${RW_ALL_APPS}=Role ${RW_DIR_DATA}=Role
az ad app permission grant --id &quot;${AZURE_CLIENT_ID}&quot; --api ${AAD_GRAPH_API} --expires never &gt; /dev/null
az ad app permission admin-consent --id &quot;${AZURE_CLIENT_ID}&quot;**
</code></pre>
<p>Note that some of them might not work if you are not at least an owner within your Azure account.</p>
<p>Lastly we need to set-up a Provider Secret and our Provider</p>
<pre><code class="language-jsx">**kubectl create secret generic azure-creds -n crossplane-system --from-file=key=./creds.json**
</code></pre>
<p>Now <strong>either</strong> create the following file ProviderConfig.yaml</p>
<pre><code class="language-jsx">apiVersion: azure.crossplane.io/v1beta1
kind: ProviderConfig
metadata:
  name: default
spec:
  credentials:
    source: Secret
    secretRef:
      namespace: crossplane-system
      name: azure-creds
      key: key
</code></pre>
<p>And then run </p>
<pre><code class="language-jsx">kubectl apply --file ./ProviderConfig.yaml
</code></pre>
<p>Note that you might have to modify the file path depending on where the file is stored at.</p>
<p>Alternatively, you can run the following command</p>
<pre><code class="language-jsx">kubectl apply -f https://raw.githubusercontent.com/crossplane/crossplane/release-1.0/docs/snippets/configure/azure/providerconfig.yaml
</code></pre>
<h2 id="provision-infrastructure"><a class="header" href="#provision-infrastructure">Provision Infrastructure</a></h2>
<p>This step is pretty straight forward — we can set-up a Kubernetes Yaml file that allows us to create a Kubernetes Service i.e. a Cluster on Azure:</p>
<p>E.g. we will name is aks in our case</p>
<pre><code class="language-jsx">apiVersion: azure.crossplane.io/v1alpha3
kind: ResourceGroup
metadata:
  name: CHANGE_ME_RESOURCE_GROUP
spec:
  location: eastus

---

apiVersion: compute.azure.crossplane.io/v1alpha3
kind: AKSCluster
metadata:
  name: anais-demo
spec:
  location: eastus
  version: &quot;1.19.7&quot;
  nodeVMSize: Standard_D2_v2
  resourceGroupNameRef:
    name: CHANGE_ME_RESOURCE_GROUP
  dnsNamePrefix: dt
  nodeCount: 3
</code></pre>
<p>Have a look at the Azure documentation on how the YAML has to be defined to create different services.</p>
<p>Once we apply the YAML</p>
<pre><code class="language-jsx">kubectl apply --filename ./aks.yaml
</code></pre>
<p>We can go ahead and have a look at the created resources</p>
<pre><code class="language-jsx">kubectl get resourcegroups
</code></pre>
<p>We can watch the provisioning of our cluster through</p>
<pre><code class="language-jsx">watch kubectl get aksclusters

kubectl describe aksclusters
</code></pre>
<p>And connect to our Azure cluster as well</p>
<pre><code class="language-jsx">az aks get-credentials --resource-group myResourceGroup --name myAKSCluster

az aks get-credentials --resource-group anais-resource --name anais-crossplane-demo
</code></pre>
<p>Something that Terraform, Pulumi etc. does not have — this is a feature of Crossplane</p>
<ol>
<li>Correct cluster manually — do what should not be done </li>
<li>Crossplane will keep whatever state we defined before, even if we change the state of our cluster through the UI</li>
</ol>
<p>Add a node pool to see how crossplane will downscale the pool</p>
<pre><code class="language-jsx">az aks nodepool add \
    --resource-group myResourceGroup \
    --cluster-name myAKSCluster \
    --name mynodepool \
    --node-count 3
</code></pre>
<p>The next step is using GitOps to deploy our infrastructure</p>
<h3 id="install-and-application"><a class="header" href="#install-and-application">Install and application</a></h3>
<p>I have already set-up the example Azure voting app. Now I will deploy those resources</p>
<pre><code class="language-jsx">kubectl apply -f ./app/voting.yaml
</code></pre>
<p>And we can get the service resource with</p>
<pre><code class="language-jsx">kubectl get service azure-vote-front --watch
</code></pre>
<p>Since this is a service of type LoadBalancer, we can access it through the External-IP</p>
<h2 id="uninstall-provider"><a class="header" href="#uninstall-provider">Uninstall Provider</a></h2>
<p>Let’s check whether there are any managed resources before deleting the
provider.</p>
<p><code>kubectl get managed</code></p>
<p>If there are any, please delete them first, so you don’t lose the track of them.
Then delete all the <code>ProviderConfig</code>s you created. An example command if you used
AWS Provider:</p>
<p><code>kubectl delete providerconfig.aws --all</code></p>
<p>List installed providers:</p>
<p><code>kubectl get provider.pkg</code></p>
<p>Delete the one you want to delete:</p>
<p><code>kubectl delete provider.pkg &lt;provider-name&gt;</code></p>
<h2 id="uninstall-crossplane"><a class="header" href="#uninstall-crossplane">Uninstall Crossplane</a></h2>
<p>`helm delete crossplane --namespace crossplane-system</p>
<p>kubectl delete namespace crossplane-system`</p>
<p>Helm does not delete CRD objects. You can delete the ones Crossplane created with
the following commands:</p>
<p><code>kubectl patch lock lock -p '{&quot;metadata&quot;:{&quot;finalizers&quot;: []}}' --type=merge kubectl get crd -o name | grep crossplane.io | xargs kubectl delete</code></p>
<p>Connecting Crossplane with ArgoCD</p>
<p><a href="https://aws.amazon.com/blogs/opensource/connecting-aws-managed-services-to-your-argo-cd-pipeline-with-open-source-crossplane/">https://aws.amazon.com/blogs/opensource/connecting-aws-managed-services-to-your-argo-cd-pipeline-with-open-source-crossplane/</a></p>
<h1 id="helm"><a class="header" href="#helm">Helm</a></h1>
<h1 id="100days-resources-18"><a class="header" href="#100days-resources-18">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/hOaA_VYhKV8">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-18"><a class="header" href="#learning-resources-18">Learning Resources</a></h1>
<ul>
<li><a href="https://codefresh.io/helm-tutorial/getting-started-with-helm-3/">https://codefresh.io/helm-tutorial/getting-started-with-helm-3/</a></li>
<li><a href="https://helm.sh/docs/intro/install/">https://helm.sh/docs/intro/install/</a></li>
<li><a href="https://github.com/cdwv/awesome-helm">https://github.com/cdwv/awesome-helm</a></li>
<li>Tutorial with Helm example deployments with <a href="https://github.com/cncf/podtato-head">Mr Podtato Head</a></li>
</ul>
<h1 id="example-notes-18"><a class="header" href="#example-notes-18">Example Notes</a></h1>
<p>In its simplest form, Helm is a package manager for Kubernetes. It is basically used to manage Kubernetes Applications through Helm templates. </p>
<p>The combination of Helm templates and values is called a Helm Chart</p>
<p>The goal: Make Kubernetes resources more accessible.</p>
<p>Helm 3 is SUPER different from Helm 2 — NO MORE TILLER 
For more information, please refer to this guide.</p>
<p>Each release goes through multiple stages, including its installation, upgrade, removal, and rollback. With Helm, you can group multiple microservices together and treat them as one entity. You can either create your own chart or use an existing one from Helm Artifact Hub. Once your application is packaged into a Helm chart, it can easily be shared and deployed on Kubernetes. <a href="https://codefresh.io/helm-tutorial/getting-started-with-helm-3/">Source</a></p>
<p><strong>Charts make it super easy to:</strong></p>
<ul>
<li>Create Kubernetes Manifests,</li>
<li>Version your Kubernetes resources</li>
<li>Share your resources</li>
<li>And publish your resources</li>
</ul>
<p>&quot;Helm is a graduated project in the CNCF and is maintained by the Helm community.&quot;</p>
<p>You can also reuse existing charts, there is a separate site for Helm Charts <a href="https://artifacthub.io/">https://artifacthub.io/</a></p>
<p>Imagine this to be similar to the Docker Hub — just for Kubernetes in Chart format (don't quote me on this please)</p>
<p><strong>What you need to use Helm:</strong></p>
<ol>
<li>A machine</li>
<li>Kubernetes — kubectl installed</li>
<li>A Kubernetes cluster (ANY cluster should be fine for our purposes)</li>
<li>Install Helm</li>
<li>USE Helm</li>
</ol>
<p><strong>Who would I recommend to use Helm</strong></p>
<ol>
<li>If you are a completely beginner — you don't have to know Kubernetes — take complex business knowledge, pack it up so that other people can use it</li>
<li>If you are going advanced</li>
<li>Not necessarily for HUGE applications</li>
</ol>
<h2 id="we-are-using-helm"><a class="header" href="#we-are-using-helm">We are using Helm</a></h2>
<p>To install Helm, head over to the </p>
<ul>
<li>Installation Guide: <a href="https://helm.sh/docs/intro/install/">https://helm.sh/docs/intro/install/</a></li>
</ul>
<p>Ensure that Helm is actually available on your machine:</p>
<pre><code class="language-jsx">helm version
</code></pre>
<p>Now, you can get started super easy just by running</p>
<pre><code class="language-jsx">mkdir charts

cd charts

helm create example-chart
</code></pre>
<p>This will create a basic Helm boiler template. Have a look at the presentation that walked through the different files that we have within that Helm chart.</p>
<p><strong>The Chart Structure</strong></p>
<p>Now before we actually install our Helm Chart, we want to make sure that it is all set-up properly</p>
<pre><code class="language-jsx">helm install --dry-run --debug example-chart ./example-chart
</code></pre>
<p>This will populate our templates with Kubernetes manifests and display those within the console. The benefit is that if you are already familiar with Kubernetes manifests, you will be able to cross-check the Kubernetes resource — and, if needed, make changes to your Kubernetes resources.</p>
<p>Once we are happy with what Helm will create, we can run:</p>
<pre><code class="language-jsx">helm install example-chart ./example-chart
</code></pre>
<p>Note that in this case, the name of my chart is &quot;example-chart&quot; you can name your chart however, you want to; just make sure that your are specifying the right path to your Helm chart.</p>
<p>Now we can have a look at the installed chart:</p>
<pre><code class="language-jsx">helm ls
</code></pre>
<p>And the history of events of our installed Chart:</p>
<pre><code class="language-jsx">helm history example-chart
</code></pre>
<p>If you are using Helm for your deployments, you want to store your Helm charts within a Helm repository. This will allow you to access your Helm charts over time. </p>
<p>Once you have deployed a Helm Chart, you can also upgrade it with new values</p>
<pre><code class="language-jsx">helm upgrade [RELEASE] [CHART] [flags]
</code></pre>
<p>To remove our example chart:</p>
<pre><code class="language-jsx">helm delete example-chart
</code></pre>
<h2 id="second-helm-exercise"><a class="header" href="#second-helm-exercise">Second Helm Exercise</a></h2>
<h3 id="helm-commands"><a class="header" href="#helm-commands">Helm Commands</a></h3>
<p>Search for all helm repositories on the helm hub: <a href="https://artifacthub.io/">https://artifacthub.io/</a></p>
<pre><code class="language-jsx">helm search hub
</code></pre>
<p>Add a repositories</p>
<pre><code class="language-jsx">helm repo add &lt;name&gt; &lt;repository link&gt;
e.g. helm repo add bitnami https://charts.bitnami.com/bitnami
</code></pre>
<p>From <a href="https://artifacthub.io/packages/helm/bitnami/mysql">https://artifacthub.io/packages/helm/bitnami/mysql</a></p>
<p>List repositories all repositories that you have installed</p>
<pre><code class="language-jsx">helm repo list
</code></pre>
<p>Search within a repository</p>
<pre><code class="language-jsx">helm search repo &lt;name&gt;
</code></pre>
<p>Instead of using the commands, you can also search the chart repository online.</p>
<p>To upgrade the charts in your repositories</p>
<pre><code class="language-jsx">helm repo update
</code></pre>
<p>Install a specific chart repository</p>
<pre><code class="language-jsx">helm install stable/mysql --generate-name

</code></pre>
<p>Note that you can either ask Helm to generate a name with the —generate-name flag,</p>
<p>or you can provide the name that you want to give the chart by defining it after install</p>
<pre><code class="language-jsx">helm install say-my-name stable/mysql
</code></pre>
<p>Check the entities that got deployed within a specific cluster:</p>
<p>List all the charts that you have deployed with the following command</p>
<pre><code class="language-jsx">helm ls
</code></pre>
<p>To remove a chart use</p>
<pre><code class="language-jsx">helm uninstall &lt;name of the chart&gt;
</code></pre>
<p>In the next section, we are going to look at ways that you can customize your chart.</p>
<h1 id="helm-part-2"><a class="header" href="#helm-part-2">Helm Part 2</a></h1>
<h1 id="setting-up-and-modifying-helm-charts"><a class="header" href="#setting-up-and-modifying-helm-charts">Setting up and modifying Helm Charts</a></h1>
<h1 id="100days-resources-19"><a class="header" href="#100days-resources-19">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/dvQErXPnjtI">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-19"><a class="header" href="#learning-resources-19">Learning Resources</a></h1>
<ul>
<li>https://helm.sh/docs/chart_template_guide/getting_started/</li>
</ul>
<h1 id="example-notes-19"><a class="header" href="#example-notes-19">Example Notes</a></h1>
<p>Charts are the packages that Helm works with — these are then turned into Kubernetes manifests and installed on your cluster.</p>
<p>You can easily go ahead and create a chart template</p>
<pre><code class="language-jsx">helm create &lt;name of your choice&gt;
</code></pre>
<blockquote>
<p>Chart names must be lower case letters and numbers. Words may be separated with dashes (-) <br />
YAML files should be indented using two spaces (and never tabs).</p>
</blockquote>
<p>This chart is based on the nginx image.</p>
<p>This chart template is especially useful if you are developing a stateless chart — we will cover other starting points later on.</p>
<p>With the following command, you can go ahead and install the chart</p>
<pre><code class="language-jsx">helm install &lt;chart name&gt; &lt;location of your chart&gt;
</code></pre>
<p>as we have seen in the previous days.</p>
<p>Note that your Kubernetes cluster needs to have access to the Docker Hub to pull the image from there and use it.</p>
<p>This chart runs Nginx out of the box — the templates within the charts are used to set-up the Kubernetes manifests.</p>
<ul>
<li>Helm Template Syntax and creating Templates</li>
</ul>
<blockquote>
<p>Helm uses the Go text template engine provided as part of the Go standard library. The syntax is used in kubectl (the command-line application for Kubernetes) templates, Hugo (the static site generator), and numerous other applications built in Go.</p>
</blockquote>
<p>Note that you do not have to know Go to create templates.</p>
<p>To pass custom information from the Chart file to the template file, you have to use annotation.</p>
<ul>
<li>
<p>You can control the flow of the template generation using:</p>
<ul>
<li>
<p><code>if/else</code> for creating conditional blocks.</p>
<pre><code class="language-jsx">{{- if .Values.ingress.enabled -}}
...
{{- else -}}
# Ingress not enabled
{{- end }}
</code></pre>
<p>The syntax is providing an <strong>&quot;if/then&quot;</strong> logic — which makes it possible to have default values if no custom values are provided within the values.yaml file.</p>
</li>
<li>
<p><code>with</code> to set a scope to a particular object</p>
<pre><code class="language-jsx">containers:
  - name: {{ .Release.name }}
    {{- with .Values.image }}
    image:
      repository: .repo
      pullPolicy: .pullPolicy
      tag: .tag
    {{- end }}
</code></pre>
<p>while the values.yaml file contains</p>
<pre><code class="language-yaml">image:
  repo: my-repo
  pullPolicy: always
  tag: 1.2.3
</code></pre>
<p>In this example the scope is changed from the current scope which is <code>.</code> to another object which is <code>.Values.image</code>.
Hence, <code>my-repo</code>, <code>always</code> and <code>1.2.3</code> were referenced without specifying <code>.Values.images.</code>.</p>
<blockquote>
<p>WARNING: Other objects can not be accessed using <code>.</code> from within a restricted scope. A solution to this scenario will be
using variables <code>$</code>.</p>
</blockquote>
</li>
<li>
<p><code>range</code>, which provides a &quot;for each&quot;-style loop</p>
<pre><code class="language-jsx">cars: |-
  {{- range .Values.cars }}
  - {{ . }}
  {{- end }}
</code></pre>
<p>with the values.yaml file contains</p>
<pre><code class="language-yaml">cars:
  - BMW
  - Ford
  - Hyundai
</code></pre>
<p>Note that <code>range</code> too changes the scope. But to what?
In each loop the scope becomes a member of the list. In this case, <code>.Values.cars</code> is a list of strings, so each iteration the
scope becomes a string. In the first iteration <code>.</code> is set to <code>BMW</code>. The second iteration, it is set to <code>Ford</code> and in the third
it is set to <code>Hyundai</code>. Therefore, each item is referenced using <code>.</code></p>
<pre><code class="language-jsx">containers:
  - name: {{ .Release.name }}
    env:
      {{- range .Values.env }}
      - name: {{ .name }}
        value: {{ .value | quote }}
      {{- end  }}
</code></pre>
<p>while the values.yaml file contains</p>
<pre><code class="language-yaml">env:
  - name: envVar1
    value: true
  - name: envVar2
    value: 5
  - name: envVar3
    value: helmForever
</code></pre>
<p>In this case, <code>.Values.env</code> is a list of dictonary, so each iteration the scope becomes a dictionary.
For example, in the first iteration, the <code>.</code> is the first dictionary <code>{name: envVar1, value: true}</code>.
In the second iteration the scope is the dictionary <code>{name: envVar2, value: 5}</code> and so on.
In addition, you can perform a pipeline on the value of <code>.name</code> or <code>.value</code> as shown.</p>
</li>
</ul>
</li>
<li>
<p>Special Functions</p>
<ul>
<li>With the <strong>include</strong> function, objects from one template can be used within another template; the first argument is the name of the template, the &quot;.&quot; that is passed in refers to the second argument in the root object.</li>
</ul>
<pre><code class="language-yaml">metadata:
  name: {{ include &quot;helm-example.fullname&quot; . }}
</code></pre>
<ul>
<li>With the <strong>required</strong> function, you can set a parameter as required. In case it's not passed a custom error message will be prompted.</li>
</ul>
<pre><code class="language-yaml">image:
    repository: {{ required &quot;An image repository is required&quot; .Values.image.repository }}
</code></pre>
</li>
<li>
<p>You can also add your own variables to tempaltes</p>
</li>
</ul>
<pre><code class="language-jsx">{{ $var := .Values.character }}
</code></pre>
<ul>
<li>The &quot;toYaml&quot; function turns data into YAML syntax</li>
</ul>
<blockquote>
<p>Helm has the ability to build a chart archive. Each chart archive is a gzipped TAR file with the extension .tgz. Any tool that can create, extract, and otherwise work on gzipped TAR files will work with Helm’s chart archives. Source. Learning Helm Book</p>
</blockquote>
<ul>
<li>Pipelines</li>
</ul>
<p>In Helm, a pipeline is a chain of variables, commands and functions which is used to alter a value before placing it in the values.yaml file.</p>
<blockquote>
<p>The value of a variable or the output of a function is used as the input to the next function in a pipeline. The output of the final element of a pipeline is the output of the pipeline. The following illustrates a simple pipeline:
character: <code>{{ .Values.character | default &quot;Sylvester&quot; | quote }}</code></p>
</blockquote>
<ul>
<li>Writing maintainable templates, here are the suggestions by the maintainers</li>
</ul>
<blockquote>
<p>You may go long periods without making structural changes to the templates in a chart and then come back to it. Being able to quickly rediscover the layout will make the processes faster.</p>
</blockquote>
<p>Other people will look at the templates in charts. This may be team members who create the chart or those that consume it. Consumers can, and sometimes do, open up a chart to inspect it prior to installing it or as part of a process to fork it.</p>
<p>When you debug a chart, which is covered in the next section, it is easier to do so with some structure in the templates.</p>
<ul>
<li>You can package your Helm chart with the following command.</li>
</ul>
<pre><code class="language-jsx">helm package &lt;chart name&gt;
</code></pre>
<p>It is important to think of a Helm chart as a package.
This package can be published to a public or private repository.
Helm repositories can also be hosted in GitHub, GitLab pages, object storage and using Chartmuseum.
You can also find charts hosted in many distributed repositories hosted by numerous people and organizations through <em>Helm Hub</em> (aka <a href="https://artifacthub.io/">Artifact Hub</a>).</p>
<h1 id="k9s"><a class="header" href="#k9s">K9s</a></h1>
<h1 id="100days-resources-20"><a class="header" href="#100days-resources-20">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/5Nhbl6LwP2o">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-20"><a class="header" href="#learning-resources-20">Learning Resources</a></h1>
<ul>
<li><a href="https://github.com/derailed/k9s">The GitHub repository</a></li>
<li><a href="https://medium.com/flant-com/k9s-terminal-ui-for-kubernetes-aeead8b0303f">Overview Medium Post</a></li>
</ul>
<h1 id="example-notes-20"><a class="header" href="#example-notes-20">Example Notes</a></h1>
<p>Managing your kubernetes cluster through kubectl commands is difficult. </p>
<ul>
<li>You have to understand how the different components interact, specifically, what resources is linked to which other resource</li>
<li>Using a UI usually abstracts all the underlying resources, not allowing for the right learning experience to take place + forcing your to click buttons</li>
<li>You do not have full insights on the resources within your cluster.</li>
</ul>
<p>These are some of the aspects that K9s can help with.</p>
<p>This is a short intro and tutorial with examples on how you can use K9s to navigate your cluster</p>
<h2 id="what-is-k9s"><a class="header" href="#what-is-k9s">What is k9s?</a></h2>
<p>K9s is a terminal-based tool that visualises the resources within your cluster and the connection between those. It helps you to access, observe, and manage your resources. &quot;K9s continually watches Kubernetes for changes and offers subsequent commands to interact with your observed resources.&quot; — Taken from their GitHub repository</p>
<p><strong>Installation options:</strong></p>
<ul>
<li>Homebrew MacOS and Linux</li>
<li>OpenSUSE</li>
<li>Arch Linux</li>
<li>Chocolatey for Windows</li>
<li>Install via Go</li>
<li>Install from source</li>
<li>Run from Docker container</li>
</ul>
<p>As you can see there are multiple different options and I am sure you will find the right one for you and your OS.</p>
<p><strong>Some additional features:</strong></p>
<ol>
<li>Customise the color settings</li>
<li><a href="https://github.com/derailed/k9s#key-bindings">Key Bindings</a></li>
<li>Node Shell</li>
<li>Command Aliases</li>
<li>HotKeySpport</li>
<li>Resource Custom Columns</li>
<li>Plugins</li>
<li>Benchmark your Application</li>
</ol>
<p>Have a look at their website for more comprehensive information</p>
<h2 id="getting-up-and-running"><a class="header" href="#getting-up-and-running">Getting up and running</a></h2>
<p>The configuration for K9s is kept in your home directory under .k9s $HOME/.k9s/config.yml.</p>
<p>You can find a detailed explanation on what the file is here: <a href="https://k9scli.io/topics/config/">https://k9scli.io/topics/config/</a></p>
<p>Note that the definitions may change over time with new releases.</p>
<p>To enter the K9s, just type k9s into your terminal.</p>
<p>Show everything that you can do with K9s, just type</p>
<pre><code class="language-jsx">?
</code></pre>
<p>Or press:</p>
<pre><code class="language-jsx">ctrl-a
</code></pre>
<p>Which will show a more comprehensive list.</p>
<p>Search for specific resources type</p>
<pre><code class="language-jsx">:&lt;name&gt;
</code></pre>
<p>The name could for instance refer to &quot;pods&quot;, &quot;nodes&quot;, &quot;rs&quot; (for ReplicaSet) and other Kubernetes resources that you have already been using. Once you have selected a resource, for instance, a namespace, you can search for specific namespaces using &quot;/&quot;</p>
<p>Have a look at deployments</p>
<pre><code class="language-jsx">:dp
</code></pre>
<p>To switch between your Kubernetes context type:</p>
<pre><code class="language-jsx">:ctx
</code></pre>
<p>You can also add the context name after the command if you want to view your Kubernetes context and then switch.</p>
<p>To delete a resource type press: ctrl-d</p>
<p>To kill a resource, use the same command but with k: ctrl-k</p>
<p>Change how resourc4es are displayed:</p>
<pre><code class="language-jsx">:xray RESOURCE
</code></pre>
<p>To exist K9s either type </p>
<pre><code class="language-jsx">:q
</code></pre>
<p>Or press: ctrl-c</p>
<h2 id="k9s-interaction-with-kubernetes"><a class="header" href="#k9s-interaction-with-kubernetes">k9s interaction with Kubernetes</a></h2>
<p>If you are changing the context or the namespace with kubectl, k9s will automatically know about it and show you the resources within the namespace.</p>
<p>Alternatively, you can also specify the namespace through K9s like detailed above.</p>
<h2 id="k9s-for-further-debugging-and-benchmarking"><a class="header" href="#k9s-for-further-debugging-and-benchmarking">k9s for further debugging and benchmarking</a></h2>
<p>K9s integrates with Hey, which is a CLI tool used to benchmark HTTP endpoints. It currently supports benchmarking port-forwards and services (<a href="https://k9scli.io/topics/bench/">Source</a>)</p>
<p>To port forward, you will need to selects a pod and container that exposes a specific port within the PodView. </p>
<p>With SHIFT-F a dialog will pop up and allows you to select the port to forward to.</p>
<p>Once you have selected that, you can use </p>
<pre><code class="language-jsx">:pf
</code></pre>
<p>to navigate to the PortForward view and list out all active port-forward.</p>
<p>Selecting port-forward + using CTRL-B will run a benchmark on that http endpoint.</p>
<p>You can then view the results of the benchmark through</p>
<pre><code class="language-jsx">:be
</code></pre>
<p>Keep in mind that once you exit the K9s session, the port-forward will be removed, forwards only last for the duration of the session.</p>
<p>Each cluster has its own bench-config that can be found at $HOME/.k9s/bench-&lt;my_context&gt;.yml</p>
<p>You can find further information <a href="https://k9scli.io/topics/bench/">here</a>.</p>
<p>You can debug processes using</p>
<pre><code class="language-jsx">k9s -l debug
</code></pre>
<h2 id="change-look"><a class="header" href="#change-look">Change Look</a></h2>
<p>You can change the look of your K9s by changing the according YAML in your .k9s folder.</p>
<p>Here is where the default skin lives: skin.yml</p>
<p>You can find example skin files in the skins directory: <a href="https://github.com/derailed/k9s/tree/master/skins">https://github.com/derailed/k9s/tree/master/skins</a></p>
<p>View all the color definitions here: <a href="https://k9scli.io/topics/skins/">https://k9scli.io/topics/skins/</a></p>
<p>For further information on how to <a href="https://k9scli.io/topics/video/">optimise K9s</a>, check-out their video tutorials.</p>
<h1 id="knative"><a class="header" href="#knative">Knative</a></h1>
<h1 id="100days-resources-21"><a class="header" href="#100days-resources-21">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/rZKaju9h7fQ">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-21"><a class="header" href="#learning-resources-21">Learning Resources</a></h1>
<ul>
<li><a href="https://knative.dev/docs/">Their documentation</a></li>
</ul>
<h1 id="example-notes-21"><a class="header" href="#example-notes-21">Example Notes</a></h1>
<p>Kubernetes-based platform to deploy and manage modern serverless workloads. Knative's serving component incorporates Istio, which is an open source tool developed by IBM, Google, and ridesharing company Lyft to help manage tiny, container-based software services known as microservices.</p>
<p>Introduce event-driven and serverless capabilities to Kubernetes clusters. Knative combines two interesting concepts Serverless and Kubernetes Container Orchestration. With Kubernetes, developers have to set-up a variety of different tools to make everything work together, this is time consuming and difficult for many. Knative wants to bring the focus back on writing code instead of managing infrastructure.</p>
<p>Knative allows us to make it super easy to deploy long-running stateless application on top of Kubernetes.</p>
<p><strong>What is Knative?</strong></p>
<p>A Kubernetes extension consistent of custom controllers and custom resource definitions that enable new use cases on top of Kubernetes. </p>
<p>A platform installed on top of Kubernetes that brings serverless capabilities to Kubernetes — with its additional features, it makes it super easy to go serverless on top of Kubernetes.</p>
<p><strong>The Goal:</strong> Making microservice deployments on Kubernetes really easy.+</p>
<p>Serverless style user experience that lives on top of Kubernetes.</p>
<p><strong>It consists of three major components:</strong></p>
<ol>
<li>Note that this part has been deprecated but you will still find it in a lot of tutorials. Build: Every developer has code — then turn it into a container — either one step or consistent of multiple step. Next, push the image to a cloud registry. These are a lot of steps — Knative can do all of this within the cluster itself, making iterative development possible. </li>
<li>Serve: Knative comes with Istio components, traffic management, automatic scaling. It consists of the Route and the Config — every revision of our service is managed by the config</li>
<li>Event: You need triggers that are responded to by the platform itself — it allows you to set-up triggers. Also, you can integrate the eventing with your ci/cd flow to kick off your build and serve stages.</li>
</ol>
<p>Note that you can use other Kubernetes management tools together with Knative</p>
<p><strong>Features:</strong></p>
<ul>
<li>Focused API with higher level abstractions for common app use-cases.</li>
<li>Stand up a scalable, secure, stateless service in seconds.</li>
<li>Loosely coupled features let you use the pieces you need.</li>
<li>Pluggable components let you bring your own logging and monitoring, networking, and service mesh.</li>
<li>Knative is portable: run it anywhere Kubernetes runs, never worry about vendor lock-in.</li>
<li>Idiomatic developer experience, supporting common patterns such as GitOps, DockerOps, ManualOps.</li>
<li>Knative can be used with common tools and	frameworks such as Django, Ruby on Rails, Spring, and many more.</li>
</ul>
<p>Knative offers several benefits for Kubernetes users wanting to take their use of containers to the next level:</p>
<ul>
<li><strong>Faster iterative development:</strong> Knative cuts
valuable time out of the container building process, which enables you
to develop and roll out new container versions more quickly. This makes
it easier to develop containers in small, iterative steps, which is a
key tenet of the agile development process.</li>
<li><strong>Focus on code:</strong> <a href="https://www.ibm.com/cloud/learn/devops-a-complete-guide">DevOps</a> may empower developers to administer their own environments, but at the end of the day, coders want to code. You want to focus on building
bug-free software and solving development problems, not on configuring
message bus queues for event triggering or managing container
scalability. Knative enables you to do that.</li>
<li><strong>Quick entry to serverless computing:</strong> Serverless
environments can be daunting to set up and manage manually. Knative
allows you to quickly set up serverless workflows. As far as the
developers are concerned, they’re just building a container—it’s Knative that runs it as a service or a serverless function behind the scenes.</li>
</ul>
<p><strong>There are two core Knative components that can be installed and used together or independently to provide different functions:</strong></p>
<ul>
<li><a href="https://knative.dev/docs/serving/">Knative Serving</a>: Provides request-driven compute that can scale to 0. The Serving component is responsible for running/hosting your application.
Easily manage stateless services on Kubernetes by reducing the developer effort required for autoscaling, networking, and rollouts.</li>
<li><a href="https://knative.dev/docs/eventing/">Knative Eventing</a>: Management and delivery of events — manage the event infrastructure of your application.
Easily route events between on-cluster and off-cluster components by
exposing event routing as configuration rather than embedded in code.</li>
</ul>
<p>These components are delivered as Kubernetes custom resource 
definitions (CRDs), which can be configured by a cluster administrator 
to provide default settings for developer-created applications and event
workflow components.</p>
<p>Additionally, knative keeps track of your revisions.</p>
<p><strong>Revisions</strong></p>
<ol>
<li>Revisions of your application are used to scale up the resources once you receive more requests</li>
<li>If you are deploying a change/update, revisions can also be used to gradually move traffic from revision 1 to revision 2</li>
<li>You can also have revisions that are not part of the networking scheme — in which case, they have a dedicate name and endpoint.</li>
</ol>
<p><strong>Prerequisites</strong></p>
<ul>
<li>Kubernetes cluster with v1.17 or newer, note that most have 1.18 already by default but you might want to check</li>
<li>Kubectl that is connected to your cluster</li>
</ul>
<p><strong>The resources that are going to be deployed through Serving</strong></p>
<p>Knative Serving defines a set of objects as Kubernetes Custom Resource
Definitions (CRDs). These objects are used to define and control how your
serverless workload behaves on the cluster:</p>
<ul>
<li><a href="https://knative.dev/docs/serving/spec/knative-api-specification-1.0#service">Service</a>:
The <code>service.serving.knative.dev</code> resource automatically manages the whole
lifecycle of your workload. It controls the creation of other objects to
ensure that your app has a route, a configuration, and a new revision for each
update of the service. Service can be defined to always route traffic to the
latest revision or to a pinned revision.</li>
<li><a href="https://knative.dev/docs/serving/spec/knative-api-specification-1.0#route">Route</a>:
The <code>route.serving.knative.dev</code> resource maps a network endpoint to one or
more revisions. You can manage the traffic in several ways, including
fractional traffic and named routes.</li>
<li><a href="https://knative.dev/docs/serving/spec/knative-api-specification-1.0#configuration">Configuration</a>:
The <code>configuration.serving.knative.dev</code> resource maintains the desired state
for your deployment. It provides a clean separation between code and
configuration and follows the Twelve-Factor App methodology. Modifying a
configuration creates a new revision.</li>
<li><a href="https://knative.dev/docs/serving/spec/knative-api-specification-1.0#revision">Revision</a>:
The <code>revision.serving.knative.dev</code> resource is a point-in-time snapshot of the
code and configuration for each modification made to the workload. Revisions
are immutable objects and can be retained for as long as useful. Knative
Serving Revisions can be automatically scaled up and down according to
incoming traffic. See
<a href="https://knative.dev/docs/serving/autoscaling">Configuring the Autoscaler</a> for more
information.</li>
</ul>
<p><img src="https://github.com/knative/serving/raw/master/docs/spec/images/object_model.png" alt="https://github.com/knative/serving/raw/master/docs/spec/images/object_model.png" /></p>
<p>With the Service resource, a deployed service will automatically have a matching route and configuration created. Each time the Service is updated, a new revision is created.</p>
<p>Configuration creates and tracks snapshots of how to run the user's software called Revisions. Since it keeps track of revisions, it allows you to roll back to prevision revisions should you encounter an issue within new deployments.</p>
<p>Data-path/Life of a Request</p>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<p>Installing the knative serving component</p>
<pre><code class="language-jsx">kubectl apply --filename https://github.com/knative/serving/releases/download/v0.20.0/serving-crds.yaml
</code></pre>
<p>Installing the core serving component</p>
<pre><code class="language-jsx">kubectl apply --filename https://github.com/knative/serving/releases/download/v0.20.0/serving-core.yaml
</code></pre>
<p>Installing Istio for Knative <a href="https://knative.dev/docs/install/installing-istio/">https://knative.dev/docs/install/installing-istio/</a></p>
<p>Install the Knative Istio controller:</p>
<pre><code class="language-jsx">kubectl apply --filename https://github.com/knative/net-istio/releases/download/v0.20.0/release.yaml
</code></pre>
<p>Fetch the External IP or CNAME:</p>
<pre><code class="language-jsx">kubectl --namespace istio-system get service istio-ingressgateway
</code></pre>
<p>Issue to delete webhooks</p>
<p><a href="https://github.com/knative/serving/issues/8323">https://github.com/knative/serving/issues/8323</a></p>
<p><strong>Configure DNS</strong> </p>
<p>Head over to the docs</p>
<p><a href="https://knative.dev/docs/install/">https://knative.dev/docs/install/</a> </p>
<p>Monitor the Knative components until all of the components show a STATUS of Running or Completed:</p>
<pre><code class="language-jsx">kubectl get pods --namespace knative-serving
</code></pre>
<p>You can also use knative with their cli-tool.</p>
<p>Note: Make sure that you have enough resources/capacity of your cluster. If you do receive an error message, increase the capacity of your cluster and rerun commands.</p>
<p><strong>So the process is now</strong></p>
<ol>
<li>The client opens the application</li>
<li>Which will then forward the request to the Loadbalancer that has been created when we installed Istio (this will only be created on a 'proper cluster')</li>
<li>The LoadBalancer will then forward our request to the Istio Gateway — which is responsible for fulfilling the request connected to our application.</li>
</ol>
<p><strong>For a stateless application, there should be as a minimum, the following resources:</strong></p>
<ol>
<li>Deployment</li>
<li>ReplicaSet</li>
<li>Pod</li>
<li>Pod Scalar to ensure the adequate number of pods are running</li>
<li>We need a Service so that other Pods/Services can access the application</li>
<li>If the application should be used outside of the cluster, we need an Ingress or similar</li>
</ol>
<p><strong>So what makes our application serverless?</strong></p>
<p>When Knative realises that our application is not being used for a while, it will remove the pods needed to run the application ⇒ Scaling the app to 0 Replicas</p>
<p>Knative is a solution for Serverless workloads, it not only scales our application but also queues our requests if there are no pods to handle our requests.</p>
<h2 id="resources-1"><a class="header" href="#resources-1">Resources</a></h2>
<ul>
<li>Website <a href="https://knative.dev/">https://knative.dev/</a></li>
<li>Documentation <a href="https://knative.dev/docs/">https://knative.dev/docs/</a></li>
<li>YouTube video <a href="https://youtu.be/28CqZZFdwBY">https://youtu.be/28CqZZFdwBY</a></li>
</ul>
<p>If you set a limit of how many requests one application can serve, you can easier see the scaling functionality of Knative in your cluster.</p>
<pre><code class="language-jsx">kn service update &lt;nameofservice&gt; --concurrency-limit=1
</code></pre>
<h1 id="gitops-and-argo"><a class="header" href="#gitops-and-argo">GitOps and Argo</a></h1>
<h1 id="100days-resources-22"><a class="header" href="#100days-resources-22">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/c4v7wGqKcEY">Video by Anais Urlichs on ArgoCD</a></li>
<li><a href="https://youtu.be/c3qJr6L8nHg">Video by Anais Urlichs on Argo Workflows</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-22"><a class="header" href="#learning-resources-22">Learning Resources</a></h1>
<ul>
<li><a href="https://thenewstack.io/understanding-gitops-the-latest-tools-and-philosophies/">&quot;Understanding GitOps: The Latest Tools and Philosophies&quot;</a></li>
<li><a href="https://www.youtube.com/c/DevOpsToolkit/videos">The DevOps Toolkit</a> has several great explanations and tutorials around GitOps and ArgoCD</li>
</ul>
<h1 id="example-notes-22"><a class="header" href="#example-notes-22">Example Notes</a></h1>
<p>This is the website of Argo <a href="https://argoproj.github.io/">https://argoproj.github.io/</a></p>
<p>Argo is currently a CNCF incubating project. Part of the Argo project family are three different projects:</p>
<ol>
<li><a href="tools/./argo.html#argo-cd">Continuous Delivery</a></li>
<li><a href="tools/./argo.html#argo-workflows">Workflows and Pipelines</a></li>
<li><a href="tools/./argo.html#argo-events">Events</a></li>
<li>Rollouts</li>
</ol>
<p><strong>So why would you want to learn about Argo and all of its projects?</strong></p>
<p>Argo follows best-practices for deploying and monitoring your applications as well as any resources that your application may depend on. Additionally, it is open source, so it will allow you to become familiar with best practices for deployments without having to pay for an expensive tool.</p>
<p>A lot of employers want to see your experience with real-world tools and scenarios. If you do not have access directly to the paid tools that are used in production, your best chance is to get really familiar with the open source tools that are used and available.</p>
<h2 id="argo-cd"><a class="header" href="#argo-cd">Argo CD</a></h2>
<p>A few months ago, I wrote an article <a href="https://thenewstack.io/understanding-gitops-the-latest-tools-and-philosophies/">&quot;Understanding GitOps: The Latest Tools and Philosophies&quot;</a> for the New Stack. In that blog post, I went over the basics of what GitOps is all about and the key facts as of today. </p>
<p>So what is GitOps and why would we want to use it? In its simplest form, GitOps is the principle of defining all of our resources in Git. Meaning, anything that we could possibly push to a Git repository and keep version controlled, we should.</p>
<p><strong>Why do we want to learn about GitOps and implement it in our organisation?</strong></p>
<p>There are different ways to duplicate steps on different machines. We could either go ahead and press different buttons, and detail the process of us setting everything up, or we could go ahead and define our Infrastructure as Code, Kubernetes resources through declarative YAML files and so on.</p>
<p>Using the declarative way to define our resources makes our infrastructure pro-active rather than reactive. This about, instead of zipping through different TV channels, you tell your TV that every Saturday morning at 10 AM you want to watch a cooking show by person x. Ultimately, your TV is going to take care of making that happen.</p>
<p>The main problem with the imperative approach of setting up our resources is that we cannot accurately recreate the same steps. UIs change, buttons move, so how can we ensure that we press the same buttons always.</p>
<p>Additionally, setting up resources the manual way is extremely risky. How do you ensure people do not break things, that the security is correct and the networking is in place?</p>
<p><strong>So how does GitOps actually work?</strong></p>
<p>The two most popular tools right now for GitOps best practices are Flux CD and Argo CD. In both cases, you will install an agent inside your cluster. This agent is then responsible for monitoring your resources.</p>
<p>The desired state of your resources will be defined in Git. Whenever anything changes to your desired state, the Agent will pull the new resources from Git and deploy them to your Kubernetes Cluster. You can call it magic or really good engineering work 🙂</p>
<p><strong>Now why is there a pull model instead of a push model?</strong> </p>
<p>The pull model makes it possible for the agent to deploy new images from inside the cluster; they are pulling information into the cluster rather than the cluster having to allow new information being pushed in from the outside. The latter would open the cluster for new security risks.</p>
<h2 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h2>
<p>In this example, we are going to be using Argo CD to install resources in our cluster. Why are we using Argo and not Flux? I am familiar with Argo but not so familiar with Flux. Additionally, Argo has a really nice UI that will help us understand what is actually happening within our cluster.</p>
<p>Separately, ArgoCD just launched its version 2.0. Note that I will be trying out their 2.0 version for the first time. <a href="https://www.cncf.io/blog/2021/04/07/argo-cd-2-0-released/">https://www.cncf.io/blog/2021/04/07/argo-cd-2-0-released/</a></p>
<p>Documentation: <a href="https://argoproj.github.io/argo-cd/">https://argoproj.github.io/argo-cd/</a></p>
<p>So, let's get started.</p>
<p>Create a new cluster</p>
<pre><code class="language-jsx">kind create cluster --name argocd
</code></pre>
<p>Let's create a namespace for agrocd</p>
<pre><code class="language-jsx">kubectl create namespace argocd
</code></pre>
<p>And install the CRDs that Argo is based on </p>
<pre><code class="language-jsx">kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.0.0-rc3/manifests/install.yaml
</code></pre>
<p>Argo is installed as CRD inside our cluster</p>
<pre><code class="language-jsx">kubectl get all -n argocd
</code></pre>
<p>Now that we have that, we want to access the UI.</p>
<pre><code class="language-jsx">kubectl port-forward svc/argocd-server -n argocd 8080:443
</code></pre>
<p>You can get the password through the following command. Make sure that you port-forwarded beforehand.</p>
<pre><code class="language-jsx">kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=&quot;{.data.password}&quot; | base64 -d
</code></pre>
<p>And then log into </p>
<pre><code class="language-jsx">argocd login localhost:8080
</code></pre>
<p>This will allow us to update our password — the username is &quot;admin&quot;</p>
<pre><code class="language-jsx">argocd account update-password
</code></pre>
<p>We will open <a href="http://localhost">localhost</a> next and log into the UI.</p>
<p>Let's deploy an application. First, through the UI and then through the CLI. You can install the Argo CD CLI through the following commands:</p>
<pre><code class="language-jsx">VERSION=$(curl --silent &quot;https://api.github.com/repos/argoproj/argo-cd/releases/latest&quot; | grep '&quot;tag_name&quot;' | sed -E 's/.*&quot;([^&quot;]+)&quot;.*/\1/')
curl -sSL -o /usr/local/bin/argocd https://github.com/argoproj/argo-cd/releases/download/$VERSION/argocd-linux-amd64
chmod +x /usr/local/bin/argocd
</code></pre>
<p>Here is the documentation: <a href="https://argoproj.github.io/argo-cd/cli_installation/">https://argoproj.github.io/argo-cd/cli_installation/</a></p>
<p>We are going to be using the following repository</p>
<p><a href="https://github.com/AnaisUrlichs/react-article-display">https://github.com/AnaisUrlichs/react-article-display</a></p>
<p>Now we will go ahead and tell the Argo UI about my Helm Chart. If you already have a repository with a Helm Chart, YAML files, or any other YAML manifests, you could use that instead.</p>
<p>Alternatively, you can tell Argo CD about the repository through the following command or similar:</p>
<pre><code class="language-jsx">argocd app create react-app --repo https://github.com/anais-codefresh/react-article-display.git --path charts/example-chart --dest-server https://kubernetes.default.svc --dest-namespace default
</code></pre>
<p>Once the app has been deployed, we can take a look </p>
<pre><code class="language-jsx">kubectl get all
</code></pre>
<pre><code class="language-jsx">argocd app get react-app
</code></pre>
<p>We can also ensure that our desired state is synced with the actual state in our cluster</p>
<pre><code class="language-jsx">argocd app sync react-app
</code></pre>
<p>So what happens if we make any manual changes with the resources within our cluster?</p>
<p>Let's go ahead and patch the service into type NodePort</p>
<pre><code class="language-jsx">kubectl patch svc react-app-example-chart --type='json' -p '[{&quot;op&quot;:&quot;replace&quot;,&quot;path&quot;:&quot;/spec/type&quot;,&quot;value&quot;:&quot;NodePort&quot;}]'
</code></pre>
<p>And check the status again</p>
<pre><code class="language-jsx">argocd app get react-app
</code></pre>
<h2 id="additional-resources"><a class="header" href="#additional-resources">Additional Resources</a></h2>
<p>If you do not have enough time for all of this, here is a video in which I provide an overview of GitOps in about 1 min</p>
<p><a href="https://youtu.be/H7wex_SmtrI">https://youtu.be/H7wex_SmtrI</a></p>
<h2 id="argo-events"><a class="header" href="#argo-events">Argo Events</a></h2>
<p>Documentation: <a href="https://argoproj.github.io/argo-events/">https://argoproj.github.io/argo-events/</a></p>
<p>Argo events is an event-driven workflow automation framework. This means that there is a trigger/event happening that causes a workflow to run and to use or modify K8s objects in response to the workflow. </p>
<p>Events can come from a variety of sources including webhooks, s3, schedules, messaging queues, gcp pubsub, sns, sqs — and by now I am lost between what these short forms mean. The point being, you can set-up a lot of different events to trigger a workflow.</p>
<p>Here are some of the features that are supported by Argo Events</p>
<ul>
<li>Supports events from 20+ event sources.</li>
<li>Ability to customize business-level constraint logic for workflow automation.</li>
<li>Manage everything from simple, linear, real-time to complex, multi-source events.</li>
<li>Supports Kubernetes Objects, Argo Workflow, AWS Lambda, Serverless, etc. as triggers.</li>
<li><a href="https://cloudevents.io/">CloudEvents</a> compliant.</li>
</ul>
<p>Once the event has been registered, it can then trigger either or multiple of the following:</p>
<ol>
<li>Argo Workflows</li>
<li>Standard K8s Objects</li>
<li>HTTP Requests / Serverless Workloads (OpenFaas, Kubeless, KNative etc.)</li>
<li>AWS Lambda</li>
<li>NATS Messages</li>
<li>Kafka Messages</li>
<li>Slack Notifications</li>
<li>Argo Rollouts</li>
<li>Custom Trigger / Build Your Own Trigger</li>
<li>Apache OpenWhisk</li>
</ol>
<p>Overall, there are 22 different events that can trigger 11 different actions.</p>
<p><strong>Event Source</strong></p>
<p>Listens to an event insides and/or outside of the cluster and once an event has been recorded, it will send it to an eventbus.</p>
<p><strong>Sensor</strong></p>
<p>The sensor listens to the event bus for certain events and conditional triggers actions.</p>
<p><strong>Trigger</strong></p>
<p>What will ultimately happen - the action e.g. our Argo workflow</p>
<p>The eventbus acts as the transport layer of Argo-Events by connecting the event-sources and sensors.</p>
<p>Event-Sources publish the events while the sensors subscribe to the events to execute triggers.</p>
<p>The current implementation of the eventbus is powered by NATS streaming.</p>
<h2 id="argo-workflows"><a class="header" href="#argo-workflows">Argo Workflows</a></h2>
<p>Documentation <a href="https://argoproj.github.io/argo-workflows/">https://argoproj.github.io/argo-workflows/</a></p>
<p>You would not want to use Argo Events solely by itself because by itself it does not provide a lot of functionality. Instead, you want to set-up Argo events with Argo workflows. </p>
<p>Argo Workflows is used to orchestrate parallel jobs on Kubernetes. A workflow in itself is a series of steps that follow each other but can also put in parallel. </p>
<p>In Argo Workflows, each step of the Workflows is its own container. Meaning, you can put anything that you can put into a container image into an Argo Workflow. </p>
<p>Additionally, Argo Worklfows makes it possible to model multi-step workflows as a sequence of tasks and capture dependencies between tasks using a directed acyclic graph (DAG). Meaning, you can have different steps being dependent on one another and tell Argo Worklfows the order of steps.</p>
<p>Making it possible to run CI/CD pipelines natively on Kubernetes is definitely a big plus since that means that everything that we set-up will be placed directly into Kubernetes.</p>
<h2 id="installation-1"><a class="header" href="#installation-1">Installation</a></h2>
<p>The installation of Argo events is similar to all other Argo projects. Namely, we will start out by </p>
<ol>
<li>Creating a namespace</li>
<li>Installing the CRDs and controllers that the specific Argo tool depends on e.g. Argo events </li>
<li>Argo events requires a specific eventbus to be installed </li>
</ol>
<p>First, we want to set-up Argo Workflows by following the getting started guide. </p>
<p>Note that Argo workflows does not work on all clusters. You can try it out on different clusters but you might need a cluster of a cloud provider.</p>
<p>Next, we want to create a namespace for Argo Workflows that we can use to set-up our </p>
<pre><code class="language-jsx">kubectl create ns argo
kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo-workflows/stable/manifests/quick-start-postgres.yaml
</code></pre>
<p>You can then access the Argo Workflows UI though the following command on localhost:2746</p>
<pre><code class="language-jsx">kubectl -n argo port-forward deployment/argo-server 2746:2746
</code></pre>
<p>Then we are going to go ahead and set-up Argo events — we are going to go with the cluster wide installation but you can also choose the namespace specific installation. Please have a look at the documentation for that.</p>
<pre><code class="language-jsx">kubectl create namespace argo-events
kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/manifests/install.yaml
</code></pre>
<p>Then, we will need the eventbus</p>
<pre><code class="language-jsx">kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/eventbus/native.yaml
</code></pre>
<p>Before you move to the next step, please make sure to have all the eventbus related pods running in your argo-events namespace</p>
<pre><code class="language-jsx">kubectl get all -n argo-events
</code></pre>
<p>Now we will set-up an event source for webhooks. Note that you can also set-up other event sources</p>
<pre><code class="language-jsx">kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/webhook.yaml
</code></pre>
<p>Here is the webhook YAML <a href="https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/webhook.yaml">https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/event-sources/webhook.yaml</a> </p>
<p>Now create a sensor for the webhook</p>
<pre><code class="language-jsx">kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/webhook.yaml
</code></pre>
<p>Have a look at the way the sensor defines the workflow here <a href="https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/webhook.yaml">https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/sensors/webhook.yaml</a></p>
<p>Access the event-source pod via port forward to consumer requests over HTTP</p>
<pre><code class="language-jsx">kubectl -n argo-events port-forward $(kubectl -n argo-events get pod -l eventsource-name=webhook -o name) 12000:12000 &amp;
</code></pre>
<p>Now send a request to localhost:12000</p>
<pre><code class="language-jsx">curl -d '{&quot;message&quot;:&quot;this is my first webhook&quot;}' -H &quot;Content-Type: application/json&quot; -X POST http://localhost:12000/example
</code></pre>
<p>And verify that an Argo workflows was triggered</p>
<pre><code class="language-jsx">kubectl -n argo-events get workflows | grep &quot;webhook&quot;
</code></pre>
<pre><code class="language-jsx">kubectl -n argo-events get wf
</code></pre>
<h1 id="linkerd"><a class="header" href="#linkerd">Linkerd</a></h1>
<h1 id="100days-resources-23"><a class="header" href="#100days-resources-23">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/4Y6TR-sg0mQ">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-23"><a class="header" href="#learning-resources-23">Learning Resources</a></h1>
<ul>
<li><a href="https://linkerd.io/2.10/getting-started/">The Linkerd getting started guide</a></li>
<li><a href="https://youtu.be/QGZO2V_N3ts">Video by Saiyam Pathak</a></li>
<li><a href="tools/../networking/servicemesh.html">Have a look at the intro to Service Mesh</a></li>
</ul>
<h1 id="example-notes-23"><a class="header" href="#example-notes-23">Example Notes</a></h1>
<p>If you want to get started with Linkerd, for the most part, just follow their Documentation. :)</p>
<p>In some of the previous videos, we looked as Service Mesh, and specifically at Istio. Since many people struggle to set-up Istio I thought it would be great to take another look at a different Service Mesh. In this case, at Linkerd. </p>
<p>Note that Istio is still one of the most popular. Thus, it is always good to still know about that but as an alternative, let's also take a look at Linkerd.</p>
<p><strong>Benefits of Linkerd</strong></p>
<blockquote>
<p>It works as it is — Saiyam</p>
</blockquote>
<ul>
<li>First service mesh project that introduced the term</li>
<li>In production for about 4 years</li>
<li>Open governance model and hosted by the CNCF</li>
<li>Super extensible and easy to install add-ons</li>
<li>Easy to install</li>
<li>It is a community project</li>
<li>Kept small, you should spend the least amount of resources</li>
</ul>
<p><strong>More information</strong></p>
<p>Control plane written in Go and data plane written in Rust.</p>
<h2 id="installation-2"><a class="header" href="#installation-2">Installation</a></h2>
<p>LinkerD has a really neat <a href="https://linkerd.io/2.10/getting-started/">getting-started documentation</a> that you can follow step-by-step.</p>
<p>First, we are going to follow the onboarding documentation. Once we have everything set-up and working, we want to see metrics of our application through Prometheus and Grafana. </p>
<p>Usually I would copy paste commands to make sure everyone knows the order but since their documentation is pretty straight-forward, I am not going to do that this time.</p>
<ol>
<li>Let's follow the steps detailed in the getting-started documentation <a href="https://linkerd.io/2.10/getting-started/">https://linkerd.io/2.10/getting-started/</a></li>
</ol>
<p>In our case, we are going to try it out on our Docker for Desktop Kubernetes cluster but you could also use another cluster such as KinD.</p>
<p>Small side-note, I absolutely love that they have a checker command to see whether LinkerD can actually be installed on your cluster</p>
<pre><code class="language-jsx">linkerd check --pre
</code></pre>
<p>How amazing is that?</p>
<p>And then, they have a separate command to see whether all the resources are ready</p>
<pre><code class="language-jsx">linkerd check
</code></pre>
<p>I am already in love with them!</p>
<p>We are going to keep following the guide, because we want to have a fancy dashboard with metrics in the end</p>
<pre><code class="language-jsx">linkerd viz install | kubectl apply -f -
</code></pre>
<pre><code class="language-jsx">linkerd jaeger install | kubectl apply -f -
</code></pre>
<p>Then we are going to check that everything is ready</p>
<pre><code class="language-jsx">linkerd check
</code></pre>
<p>Access the dashboard</p>
<pre><code class="language-jsx">**linkerd viz dashboard &amp;**
</code></pre>
<p>this will give you the <a href="http://localhost">localhost</a> port to a beautiful Dashboard</p>
<p>We are going to use the following GitHub respository to install our application</p>
<p><a href="https://github.com/AnaisUrlichs/react-article-display">https://github.com/AnaisUrlichs/react-article-display</a></p>
<p>After clone, cd into react-article-display</p>
<p>Now you want to have Helm istalled since this application relies on a Helm Chart</p>
<pre><code class="language-jsx">helm install react-article ./charts/example-chart
</code></pre>
<p>Once installed, we can port-forward to access our application</p>
<pre><code class="language-jsx">kubectl por-forward service/react-article-example-chart 5000:80
</code></pre>
<p>Now we just have to connect our Service Mesh with the application</p>
<pre><code class="language-jsx">kubectl get deploy -o yaml \
  | linkerd inject - \
  | kubectl apply -f -
</code></pre>
<p>Then we can go ahead and run checks again</p>
<pre><code class="language-jsx">linkerd check --proxy
</code></pre>
<p>We can see the live stats of our application through</p>
<pre><code class="language-jsx">linkerd viz stat deploy
</code></pre>
<p>And see the stream of requests to our services throug</p>
<pre><code class="language-jsx">linkerd viz tap deploy/web
</code></pre>
<p>Now let's go ahead and see how we can do traffic splitting for Canary deployments with Linkerd. For this, we are going to follow the documentation <a href="https://linkerd.io/2.10/tasks/canary-release/">https://linkerd.io/2.10/tasks/canary-release/</a></p>
<p>Clone the following repository</p>
<p>Apply the first application</p>
<pre><code class="language-jsx">kubectl apply -f app-one.yaml
</code></pre>
<p>You can access it through kubectl port forwarding</p>
<pre><code class="language-jsx">kubectl port-forward service/app-one 3000:80
</code></pre>
<p>Install Flagger</p>
<pre><code class="language-jsx">kubectl apply -k github.com/fluxcd/flagger/kustomize/linkerd
</code></pre>
<p>Deploy the deployment resource</p>
<pre><code class="language-jsx">kubectl create ns test &amp;&amp; \
  kubectl apply -f https://run.linkerd.io/flagger.yml
</code></pre>
<p>Ensure they are ready</p>
<pre><code class="language-jsx">kubectl -n test rollout status deploy podinfo
</code></pre>
<p>Access the service</p>
<pre><code class="language-jsx">kubectl -n test port-forward svc/podinfo 9898
</code></pre>
<pre><code class="language-jsx">kubectl -n test get ev --watch
</code></pre>
<h1 id="prometheus"><a class="header" href="#prometheus">Prometheus</a></h1>
<h1 id="prometheus-exporter"><a class="header" href="#prometheus-exporter">Prometheus Exporter</a></h1>
<h1 id="100days-resources-24"><a class="header" href="#100days-resources-24">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/qItg7P1H7uw">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-24"><a class="header" href="#learning-resources-24">Learning Resources</a></h1>
<ul>
<li><a href="https://youtu.be/QoDqxm7ybLc">Setup Prometheus Monitoring on Kubernetes using Helm and Prometheus Operator | Part 1</a></li>
</ul>
<h1 id="example-notes-24"><a class="header" href="#example-notes-24">Example Notes</a></h1>
<h2 id="install-prometheus-helm-chart-with-operators-etc"><a class="header" href="#install-prometheus-helm-chart-with-operators-etc">Install Prometheus Helm Chart with Operators etc.</a></h2>
<p>First off, we are going to follow the commands provided in the previous day — Day 28 — on Prometheus to have our Helm Chart with all the Prometheus related resources installed.</p>
<p><a href="observability/prometheus.html">Day 28: What is Prometheus</a></p>
<h2 id="have-a-look-at-the-prometheus-resources-to-understand-those-better"><a class="header" href="#have-a-look-at-the-prometheus-resources-to-understand-those-better">Have a look at the Prometheus Resources to understand those better</a></h2>
<p>Prometheus uses ServiceMonitors to discover endpoints. You can get a list of all the ServiceMonitors through:</p>
<pre><code class="language-jsx">kubectl get servicemonitor
</code></pre>
<p>Now have a look at one of those ServiceMonitors:</p>
<pre><code class="language-jsx">kubectl get servicemonitor prometheus-kube-prometheus-grafana -o yaml
</code></pre>
<p>This will display the ServiceMonitor definition in pure YAML inside our terminal. Look through the YAML file and you will find a label called:</p>
<p>&quot;release: prometheus&quot;</p>
<p>This label defines the ServiceMonitors that Prometheus is supposed to scrape.</p>
<p>Like mentioned in the Previous video, operators, such as the Prometheus Operator rely on CRD. Have a look at the CRDs that Prometheus uses through:</p>
<pre><code class="language-jsx">kubectl get crd
</code></pre>
<p>You can take a look at a specific one as follows.</p>
<pre><code class="language-jsx">kubectl get prometheus.monitoring.coreos.com -o yaml
</code></pre>
<h2 id="set-up-mongodb"><a class="header" href="#set-up-mongodb">Set-up MongoDB</a></h2>
<p>Now, we want to install a MongoDB image on our cluster and tell Prometheus to monitor it's endpoint. However, MongoDB is one of those images that relies on an exporter for its service to be visible to Prometheus. Think about it this way, Prometheus needs the help of and Exporter to know where MongoDB is in our cluster — like a pointer.</p>
<p>You can learn more about those concepts in my previous videos</p>
<ol>
<li><a href="https://youtu.be/n4eF8EcgpZQ">Prometheus on Kubernetes: Day 28 of #100DaysOfKubernetes</a></li>
<li><a href="https://youtu.be/C_mlWhSrqEY">Kubernetes Operators: Day 29 of #100DaysOfKubernetes</a></li>
</ol>
<p>First off, we are going to install the MongoDB deployment and the MongoDB service; here is the YAML needed for this:</p>
<p>deployment.yaml</p>
<pre><code class="language-jsx">apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb-deployment
  labels:
    app: mongodb
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongodb
        image: mongo
        ports:
        - containerPort: 27017
</code></pre>
<p>service.yaml</p>
<pre><code class="language-jsx">apiVersion: v1
kind: Service
metadata:
  name: mongodb-service
spec:
  selector:
    app: mongodb
  ports:
  - protocol: TCP
    port: 27017
    targetPort: 27017
</code></pre>
<p>Use the follow to apply both to your cluster</p>
<pre><code class="language-jsx">kubectl apply -f deployment.yaml
kubectl apply -f service.yaml
</code></pre>
<p>You can check that both are up and running through</p>
<pre><code class="language-jsx">kubectl get all 
# or

kubectl get service
kubectl get pods
</code></pre>
<p>Now we want to tell Prometheus to monitor that endpoint — for this, we are going to use the following Helm Chart <a href="https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-mongodb-exporter">https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-mongodb-exporter</a></p>
<p>You can find a list of Prometheus exporters and integrations here:</p>
<p><a href="https://prometheus.io/docs/instrumenting/exporters/">https://prometheus.io/docs/instrumenting/exporters/</a> </p>
<p>Next, we are going to add the Helm Mongo DB exporter</p>
<pre><code class="language-jsx">helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
</code></pre>
<pre><code class="language-jsx">helm show values rometheus-community/prometheus-mongodb-exporter &gt; values.yaml
</code></pre>
<p>We need to mondify the values provided in the values.yaml file as follows:</p>
<pre><code class="language-jsx">mongodb:
  uri: &quot;mongodb://mongodb-service:27017&quot;

serviceMonitor:
  enabled: true
  additionalLabels:
    release: prometheus
</code></pre>
<p>Basically, replace the values.yaml file created in the helm show command above with this file. In this case, we are going to tell the helm chart the mongodb endpoint and then the additional label for the ServiceMonitor.</p>
<p>Next, let's install the Helm Chart and pass in the values.yaml</p>
<pre><code class="language-jsx">helm install mongodb-exporter prometheus-community/prometheus-mongodb-exporter -f values.yaml
</code></pre>
<p>You can see whether the chart got installed correctly through</p>
<pre><code class="language-jsx">helm ls
</code></pre>
<p>Now have a look at the pods and the services again to see whether everything is running correctly:</p>
<pre><code class="language-jsx">kubectl get all 
# or

kubectl get service
kubectl get pods
</code></pre>
<p>Lastly make sure that we have the new ServiceMonitor in our list:</p>
<pre><code class="language-jsx">kubectl get servicemonitor
</code></pre>
<p>Have a look at the prometheus label within the ServiceMonitor</p>
<pre><code class="language-jsx">kubectl get servicemonitor mongodb-exporter-prometheus-mongodb-exporter -o yaml
</code></pre>
<p>Now access the service of the mongodb-exporter to see whether it scrapes the metrics of our MongoDB properly:</p>
<pre><code class="language-jsx">kubectl port-forward service/mongodb-exporter-prometheus-mongodb-exporter 9216
</code></pre>
<p>and open the Prometheus service:</p>
<pre><code class="language-jsx">kubectl port-forward service/prometheus-kube-prometheus-prometheus 9090
</code></pre>
<p>On <a href="http://localhost:9090">localhost:9090</a>, go to Status - Targets and you can see all of our endpoints that Prometheus currently knows about.</p>
<h1 id="kubernetes-operators"><a class="header" href="#kubernetes-operators">Kubernetes Operators</a></h1>
<h1 id="100days-resources-25"><a class="header" href="#100days-resources-25">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/C_mlWhSrqEY">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-25"><a class="header" href="#learning-resources-25">Learning Resources</a></h1>
<ul>
<li><a href="https://youtu.be/ha3LjlD6g7g">https://youtu.be/ha3LjlD6g7g</a></li>
<li><a href="https://www.redhat.com/en/topics/containers/what-is-a-kubernetes-operator">https://www.redhat.com/en/topics/containers/what-is-a-kubernetes-operator</a></li>
<li><a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/">https://kubernetes.io/docs/concepts/extend-kubernetes/operator/</a></li>
</ul>
<h1 id="example-notes-25"><a class="header" href="#example-notes-25">Example Notes</a></h1>
<p>There is so much great content around Kubernetes Operators, and I have mentioned it several times across the previous videos. However, we have not looked at Kubernetes Operators yet. Today, we are going to explore </p>
<ul>
<li>What are Kubernetes Operators</li>
<li>How do Operators work</li>
<li>Why are they important</li>
</ul>
<p>Operators are mainly used for Stateful Applications</p>
<p>Operators are software extensions to Kubernetes that make use of custom resources to manage applications and their components. Operators follow Kubernetes principles, notably the control loop — <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/">Source</a></p>
<h2 id="managing-stateful-applications-without-an-operator-vs-with-an-operator"><a class="header" href="#managing-stateful-applications-without-an-operator-vs-with-an-operator">Managing Stateful Applications without an operator vs. with an operator.</a></h2>
<p>If you are deploying an application, you usually use </p>
<ul>
<li>Service</li>
<li>Deployment</li>
<li>ConfigMap</li>
</ul>
<p>Kubernetes knows what the desired state of our cluster is because we told it through our configuration files. It aims to match the actual state of our cluster to our desired state.</p>
<p>Now for a stateful application the process is a bit more difficult — it does not allow Kubernetes to automatically scale etc. </p>
<p>E.g. SQL databases are not identical replicas. There has to be a constant communication for the data to be consistent + other factors.</p>
<p>Each database has its own workaround —&gt; this makes it difficult to use Kubernetes to automate any workaround.</p>
<p>A lot of times stateful applications will require manual intervention i.e. human operators. However, having to manually update resources in Kubernetes goes against its </p>
<p>So there is a need for an alternative to manage stateful applications. This alternative is a Kubernetes Operator. </p>
<h2 id="what-is-an-operator"><a class="header" href="#what-is-an-operator">What is an Operator?</a></h2>
<p>Replace all the manual tasks that the human operator would do. It takes care of</p>
<ul>
<li>Deploying the app</li>
<li>Creating replicas</li>
<li>Ensuring recovery in case of failure</li>
</ul>
<p>With this, an Operator is basically an application-specific controller that extends the functionality of the Kubernetes API to take care of the management of complex applications. This is making tasks automated and reusable.</p>
<p><strong>Operators rely on a control loop mechanism.</strong></p>
<p>If one replica of our database dies, it will create a new one. If the image version of the database get updated, it will deploy the new version.</p>
<p>Additionally, Operators rely on Kubernetes Custom Resource Definitions (CRDs). CRDs are custom resources created on top of Kubernetes. CRDs allow Operators to have specific knowledge of the application that they are supposed to manage.</p>
<p>You can find a list of Kubernetes Operators in the Kubernetes Operator Hub <a href="https://operatorhub.io/">https://operatorhub.io/</a></p>
<p>AND you can find several awesome operators in the wild <a href="https://github.com/operator-framework/awesome-operators">https://github.com/operator-framework/awesome-operators</a> 😄</p>
<p>Once you have created an operator, it will take high-level inputs and translate them into low level actions and tasks that are needed to manage the application.</p>
<p>Once the application is deployed, the Operator will continue to monitor the application to ensure that it is running smoothly.</p>
<h2 id="who-is-creating-operators"><a class="header" href="#who-is-creating-operators"><strong>Who is creating Operators?</strong></a></h2>
<p>Those if the insights and know-how of the application that the operator is supposed to run.</p>
<p>There is an Operator Framework that basically provides the building blocks that can be used to</p>
<p>The Operator Framework includes:</p>
<ul>
<li><strong>Operator SDK:</strong> Enables developers to build operators based on their expertise without requiring knowledge of Kubernetes API complexities.</li>
<li><strong>Operator Lifecycle Management:</strong> Oversees installation, updates, and management of the lifecycle of all of the operators running across a Kubernetes cluster.</li>
<li><strong>Operator Metering:</strong> Enables usage reporting for operators that provide specialized services.</li>
</ul>
<p>Some of the things that you can use an operator to automate include:</p>
<ul>
<li>deploying an application on demand</li>
<li>taking and restoring backups of that application's state</li>
<li>handling upgrades of the application code alongside related changes such
as database schemas or extra configuration settings</li>
<li>publishing a Service to applications that don't support Kubernetes APIs to
discover them</li>
<li>simulating failure in all or part of your cluster to test its resilience</li>
<li>choosing a leader for a distributed application without an internal
member election process</li>
</ul>
<h2 id="practical-example-1"><a class="header" href="#practical-example-1">Practical Example</a></h2>
<p>In the case of Prometheus, I would have to deploy several separate components to get Prometheus up and running, which is quite complex + deploying everything in the right order.</p>
<h1 id="serverless"><a class="header" href="#serverless">Serverless</a></h1>
<h1 id="100days-resources-26"><a class="header" href="#100days-resources-26">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/LLqICWKbP5I">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-26"><a class="header" href="#learning-resources-26">Learning Resources</a></h1>
<p>TODO</p>
<h1 id="example-notes-26"><a class="header" href="#example-notes-26">Example Notes</a></h1>
<blockquote>
<p>There is no such thing as serverless, there is always a server</p>
</blockquote>
<p><strong>How do we think about Serverless?</strong></p>
<p>In the cloud, functions written in JavaScript, response to event/triggers ⇒ highly narrowed view</p>
<p><strong>Why would you want Serverless?</strong></p>
<ol>
<li>You want to try out cool things</li>
<li>You want to stop over- or under-provisioning of your infrastructure</li>
<li>You want to try out cool things</li>
</ol>
<h2 id="serverless-1"><a class="header" href="#serverless-1">Serverless</a></h2>
<p>Serverless: Popular operating model — infrastructure requirements are provisioned just before the Serverless workload is executed — The infrastructure resources are just needed during the execution, so there is no need to keep the infrastructure during low to now usage.</p>
<p>The serverless platform will likely run on containers. </p>
<ol>
<li>No Server Management is necessary</li>
<li>You only pay for the execution time</li>
<li>They can scale to 0 = no costs to keep the infrastructure up</li>
<li>Serverless functions are stateless, this promotes scalability</li>
<li>Auto-scalability ⇒ usually taken care of by the cloud provider</li>
<li>Reduced operational costs</li>
</ol>
<p>It does not mean that there are no servers involved. Instead it is the process of ensuring that the infrastructure is provisioned automatically. In this case, the developer is less involved to managing infrastructure. </p>
<p>FAAS is a compute platform for your service — they are essentially processing your code. These functions are called by events, events in this case are any trigger. </p>
<p>Within Serverless, cloud providers provision event-driven architecture. When you build your app, you look for those events and you add them to your code. </p>
<p><strong>Issues with serverless</strong></p>
<ol>
<li>Functions/processes of your code will not finish before the infrastructure downgrades</li>
<li>Latency issues i.e. because of cold starts</li>
</ol>
<p>Moving from IAAS to PAAS to FAAS</p>
<p><strong>Two distinctions</strong> </p>
<ol>
<li>Not using Servers</li>
<li>FaaS (Function as a Service): Is essentially small pieces of code that are run in the cloud on stateless containers</li>
</ol>
<p>One is about hiding operations from us</p>
<p><strong>Serverless options</strong></p>
<ol>
<li>Google: Google Cloud Functions</li>
<li>AWS: Lambda </li>
<li>For Kubernetes: Knative</li>
</ol>
<p>What is a cold start?</p>
<p>Serverless functions can be really slow the first time that they start. This can be a problem. These articles offer some comparisons</p>
<ol>
<li><a href="https://mikhail.io/serverless/coldstarts/aws/">https://mikhail.io/serverless/coldstarts/aws/</a> </li>
<li><a href="https://mikhail.io/serverless/coldstarts/big3/">https://mikhail.io/serverless/coldstarts/big3/</a> </li>
<li>Discussion <a href="https://youtu.be/AuMeockiuLs">https://youtu.be/AuMeockiuLs</a></li>
</ol>
<p><strong>AWS has something called Provisioned Concurrenc</strong>y that allows you to keep on x functions that are always on and ready. </p>
<p>When would you not want to use Serverless:</p>
<ol>
<li>When you have a consistent traffic</li>
</ol>
<p>Be careful,</p>
<ol>
<li>Different serverless resources have different pricing models, meaning it could easily happen that you accidentally leave your serverless function running and it will eat up your pocket</li>
<li>If you depend on a Serverless feature, it is easy to get vendorlocked.</li>
</ol>
<h2 id="resources-2"><a class="header" href="#resources-2">Resources</a></h2>
<ol>
<li>How to think about Serverless <a href="https://youtu.be/_1-5YFfJCqM">https://youtu.be/_1-5YFfJCqM</a></li>
</ol>
<p>I am going to be looking in the next days at</p>
<ol>
<li>Knative: Kubernetes based open source building blocks for serverless</li>
<li>Faasd: <a href="https://github.com/openfaas/faasd">https://github.com/openfaas/faasd</a></li>
</ol>
<h1 id="ingress-from-scratch"><a class="header" href="#ingress-from-scratch">Ingress from scratch</a></h1>
<h1 id="100days-resources-27"><a class="header" href="#100days-resources-27">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/13y1tWEK2ZY">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-27"><a class="header" href="#learning-resources-27">Learning Resources</a></h1>
<p>TODO</p>
<h1 id="example-notes-27"><a class="header" href="#example-notes-27">Example Notes</a></h1>
<p>If you are new to 100 Days Of Kubernetes, have a look at the previous days to fill in the gaps and fundamentals for today:</p>
<p><a href="https://www.notion.so/Day-4-Looking-at-Services-b0e7a6aa09fc4de39ad107ea133c708f">Day 4: Looking at Services</a></p>
<p><a href="https://www.notion.so/Day-11-More-exercises-on-Services-0f98b7b9620841e8b26ad3f507849683">Day 11: More exercises on Services</a></p>
<p><a href="https://www.notion.so/Day-13-Ingress-89d34d81c5a549aeba6ad10c3d3d9d9c">Day 13: Ingress</a></p>
<p><a href="https://www.notion.so/Day-31-Service-Mesh-60e08e4d07d6484e92e50457f5c6b31a">Day 31: Service Mesh</a></p>
<h2 id="resources-3"><a class="header" href="#resources-3">Resources</a></h2>
<ol>
<li>Do I need a service mesh or is Ingress enough? <a href="https://www.nginx.com/blog/do-i-need-a-service-mesh/">https://www.nginx.com/blog/do-i-need-a-service-mesh/</a></li>
<li>Check the specific installation guide <a href="https://kubernetes.github.io/ingress-nginx/deploy/#microk8s">https://kubernetes.github.io/ingress-nginx/deploy/</a></li>
<li>Tutorial setup <a href="https://medium.com/analytics-vidhya/configuration-of-kubernetes-k8-services-with-nginx-ingress-controller-5e2c5e896582">https://medium.com/analytics-vidhya/configuration-of-kubernetes-k8-services-with-nginx-ingress-controller-5e2c5e896582</a></li>
<li>Comprehensive Tutorial by Red Hat <a href="https://redhat-scholars.github.io/kubernetes-tutorial/kubernetes-tutorial/ingress.html">https://redhat-scholars.github.io/kubernetes-tutorial/kubernetes-tutorial/ingress.html</a></li>
<li>Using Ingress on Kind cluster <a href="https://kind.sigs.k8s.io/docs/user/ingress/#ingress-nginx">https://kind.sigs.k8s.io/docs/user/ingress/#ingress-nginx</a></li>
<li>More on installing ingress on kind <a href="https://dustinspecker.com/posts/test-ingress-in-kind/">https://dustinspecker.com/posts/test-ingress-in-kind/</a></li>
</ol>
<h2 id="install"><a class="header" href="#install">Install</a></h2>
<p>The installation will slightly depend on the type of cluster that you are using. </p>
<p>For this tutorial, we are going to be using the docker-desktop cluster</p>
<p>We need two main components</p>
<ol>
<li>
<p>Ingress Controller</p>
</li>
<li>
<p>Ingress resources</p>
<p>Note that depending on the Ingress controller that you are using for your Ingress, the Ingress resource that has to be applied to your cluster will be different. In this tutorial, I am using the Ingress Nginx controller.</p>
</li>
</ol>
<h2 id="lets-set-everything-up"><a class="header" href="#lets-set-everything-up">Let's set everything up.</a></h2>
<p>We are going to be using the NGINX Ingress Controller. With the NGINX Ingress Controller for Kubernetes, you get basic load balancing, SSL/TLS termination, support for URI rewrites, and upstream SSL/TLS encryption</p>
<p>First off, let's clone this repository: <a href="https://github.com/AnaisUrlichs/ingress-example">https://github.com/AnaisUrlichs/ingress-example</a></p>
<pre><code class="language-jsx">git clone &lt;repository&gt;
</code></pre>
<pre><code class="language-jsx">cd ingress-example
</code></pre>
<pre><code class="language-jsx">cd app-one
</code></pre>
<p>now build your Docker image</p>
<pre><code class="language-jsx">docker build -t anaisurlichs/flask-one:1.0 .
</code></pre>
<p>You can test it out through</p>
<pre><code class="language-jsx">docker run -p 8080:8080 anaisurlichs/flask-one:1.0
</code></pre>
<p>And then we are going to push the image to our Docker Hub</p>
<pre><code class="language-jsx">docker push anaisurlichs/flask-one:1.0
</code></pre>
<p>We are going to do the same in our second example application</p>
<pre><code class="language-jsx">cd ..
cd app-two
</code></pre>
<p>build the docker image</p>
<pre><code class="language-jsx">docker build -t anaisurlichs/flask-two:1.0 .
</code></pre>
<p>push it to your Docker Hub</p>
<pre><code class="language-jsx">docker push anaisurlichs/flask-two:1.0
</code></pre>
<p>Now apply the deployment-one.yaml and the deployment-tow.yaml</p>
<pre><code class="language-jsx">kubectl apply -f deployment-one.yaml
kubectl apply -f deployment-two.yaml
</code></pre>
<p>Make sure they are running correctly </p>
<pre><code class="language-jsx">kubectl get all
</code></pre>
<p>Installing Ingress Controller</p>
<pre><code class="language-jsx">kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.44.0/deploy/static/provider/cloud/deploy.yaml
</code></pre>
<p><strong>Installing Ingress Resource</strong></p>
<pre><code class="language-jsx">kubectl apply -f ingress.yaml
</code></pre>
<p>make sure that everything is running correctly</p>
<pre><code class="language-jsx">kubectl get all -n ingress-nginx
</code></pre>
<p>Making paths happen <a href="https://github.com/kubernetes/ingress-nginx/issues/3762">https://github.com/kubernetes/ingress-nginx/issues/3762</a></p>
<h1 id="setup-istio-from-scratch"><a class="header" href="#setup-istio-from-scratch">Setup Istio from scratch</a></h1>
<h1 id="100days-resources-28"><a class="header" href="#100days-resources-28">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/-X9babxSxK4">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-28"><a class="header" href="#learning-resources-28">Learning Resources</a></h1>
<ul>
<li>
<p>Istio cli &quot;istioctl&quot; <a href="https://istio.io/latest/docs/setup/install/istioctl/">https://istio.io/latest/docs/setup/install/istioctl/</a></p>
</li>
<li>
<p>To use kind and ingress together have a look at this blog post <a href="https://mjpitz.com/blog/2020/10/21/local-ingress-domains-kind/">https://mjpitz.com/blog/2020/10/21/local-ingress-domains-kind/</a></p>
</li>
<li>
<p>Using the first part of this blog post <a href="https://www.arthurkoziel.com/running-knative-with-istio-in-kind/">https://www.arthurkoziel.com/running-knative-with-istio-in-kind/</a> (if you want to set-up knative on your kind cluster, be my guest, I also have a whole video on using kind)</p>
<p><a href="https://www.notion.so/Day-27-Knative-edd3b828562f4ce08f42a47393e3b473">Day 27: <strong>Knative</strong></a></p>
<p>However, this is not the focus today.</p>
</li>
</ul>
<p>Have a look at the previous day on Service Mesh</p>
<p><a href="https://www.notion.so/Day-31-Service-Mesh-60e08e4d07d6484e92e50457f5c6b31a">Day 31: Service Mesh</a></p>
<h1 id="example-notes-28"><a class="header" href="#example-notes-28">Example Notes</a></h1>
<h2 id="service-mesh-vs-ingress"><a class="header" href="#service-mesh-vs-ingress">Service Mesh vs Ingress</a></h2>
<p>We would not want to create a LoadBalancer for all of our Services to access each and every individual Service. This is where Ingress comes in. Ingress allows us to configure the traffic to all of our microservice applications. This way, we can easier manage the traffic.</p>
<p>Now to manage the connection between services, we would want to configure a Service Mesh such as Istio. Istio will then take care of the communication between our microservice applications within our cluster. However, it will not take care of external traffic automatically. Instead, what Istio will do is to use an Istio Ingress Gateway that will allow users to access applications from outside the Kubernetes cluster.</p>
<p>The difference here is using the Kubernetes Ingress Resource vs. the Istio Ingress Resource. Have a look at the following blog post that provides comprehensive detail on the differences:</p>
<p><a href="https://medium.com/@zhaohuabing/which-one-is-the-right-choice-for-the-ingress-gateway-of-your-service-mesh-21a280d4a29c">https://medium.com/@zhaohuabing/which-one-is-the-right-choice-for-the-ingress-gateway-of-your-service-mesh-21a280d4a29c</a></p>
<h2 id="installation-3"><a class="header" href="#installation-3">Installation</a></h2>
<p><strong>Prerequisites</strong></p>
<ul>
<li>Docker desktop installed <a href="https://www.docker.com/products/docker-desktop">https://www.docker.com/products/docker-desktop</a></li>
<li>Kind binary installed so that you can run kind commands <a href="https://kind.sigs.k8s.io/docs/user/quick-start/">https://kind.sigs.k8s.io/docs/user/quick-start/</a></li>
</ul>
<p><strong>Set-up the kind cluster for using Ingress</strong></p>
<p>In the previous tutorial on Ingress, we used Docker Desktop as our local cluster. You can also set-up Ingress with a kind cluster.</p>
<p>You can configure your kind cluster through a yaml file. In our case, we want to be able to expose some ports through the cluster, so we have to specify this within the config.</p>
<p>Before we create a kind cluster, save the following configurations in a kind-config.yaml file.</p>
<pre><code class="language-jsx">kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: &quot;ingress-ready=true&quot;
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
    protocol: TCP
  - containerPort: 443
    hostPort: 443
    protocol: TCP
</code></pre>
<p>This file is going to be used in the following command when we create out kind cluster.</p>
<pre><code class="language-jsx">kind create cluster --config kind-config.yaml --name istio-test
</code></pre>
<p>Now you should be on your cluster istio-test. In your kubectl config context the cluster will be called kind-istio-test.</p>
<pre><code class="language-jsx">kubectl config get-contexts
kind get clusters
</code></pre>
<p>You can then follow the documentation on setting up different Ingress Controllers on top </p>
<p><strong>Install Istio</strong></p>
<p>Install Istio with the following commands if you are on Linux, check out other installation options.</p>
<pre><code class="language-jsx">curl -L https://istio.io/downloadIstio | sh -
</code></pre>
<p>Now you have to move the package into your usr/bin directory so you can access it through the command line.</p>
<pre><code class="language-jsx">sudo mv /Downloads/istio-1.9.1/bin/istioctl /usr/local/bin/
</code></pre>
<p>When you run now the following, you should see the version of istioctl</p>
<pre><code class="language-jsx">istioctl version
</code></pre>
<p>Awesome! we are ready to install istio on our kind cluster.</p>
<p>We are going to use the following Istio configurations and then going to use istioctl to install everything on our kind cluster.</p>
<pre><code class="language-jsx">istioctl install --set profile=demo
</code></pre>
<p>Note that you can use another istio profile, other than the default one.</p>
<p>Let's check that everything is set-up correctly</p>
<pre><code class="language-jsx">kubectl get pods -n istio-system
</code></pre>
<p>You should have two pods running right now.</p>
<p>You can check whether everything is working properly through the following command</p>
<pre><code class="language-jsx">istioctl manifest generate --set profile=demo | istioctl verify-install -f -
</code></pre>
<p>By default, Istio will set the Ingress type to Loadbalancer. However, on a local kind cluster, we cannot use Loadbalancer. Instead, we have to use NodePort and set the host configurations. Create a file called patch-ingressgateway-nodeport.yaml with the following content:</p>
<pre><code class="language-jsx">spec:
  type: NodePort
  ports:
  - name: http2
    nodePort: 32000
    port: 80
    protocol: TCP
    targetPort: 80
</code></pre>
<p>And then apply the patch with</p>
<pre><code class="language-jsx">kubectl patch service istio-ingressgateway -n istio-system --patch &quot;$(cat patch-ingressgateway-nodeport.yaml)&quot;
</code></pre>
<p>However, in our Docker Desktop cluster, we are connected to <a href="http://localhost">localhost</a> automatically so in that case, we do not have to change anything to NodePort.</p>
<p>Next, we have to allow Istio to communicated with our default namespace:</p>
<pre><code class="language-jsx">kubectl label namespace default istio-injection=enabled
</code></pre>
<p><strong>Set-up Ingress Gateway</strong></p>
<p>From here onwards, we are just following the documentation</p>
<p>Setting-up the example application:</p>
<p><a href="https://istio.io/latest/docs/examples/bookinfo/">https://istio.io/latest/docs/examples/bookinfo/</a></p>
<p>Setting-up metrics:</p>
<p><a href="https://istio.io/latest/docs/tasks/observability/metrics/tcp-metrics/">https://istio.io/latest/docs/tasks/observability/metrics/tcp-metrics/</a></p>
<p>If you feel ready for a challenge, try out canary updates with Istio</p>
<p><a href="https://istio.io/latest/docs/setup/upgrade/canary/">https://istio.io/latest/docs/setup/upgrade/canary/</a></p>
<h1 id="deploy-an-app-to-civo-k3s-cluster-from-scratch"><a class="header" href="#deploy-an-app-to-civo-k3s-cluster-from-scratch">Deploy an app to CIVO k3s cluster from scratch</a></h1>
<h1 id="100days-resources-29"><a class="header" href="#100days-resources-29">100Days Resources</a></h1>
<ul>
<li><a href="https://www.youtube.com/watch?v=HpAlAzOjgpI">Video by Rajesh Radhakrishnan</a></li>
<li><a href="https://github.com/rajeshradhakrishnanmvk/kitchen.git">Rajesh' Kitchen repo</a></li>
</ul>
<h1 id="learning-resources-29"><a class="header" href="#learning-resources-29">Learning Resources</a></h1>
<ol>
<li><a href="https://www.youtube.com/watch?v=HpAlAzOjgpI">Video by Rajesh Radhakrishnan</a></li>
<li><a href="https://github.com/civo/cli">CIVO cli essentials</a></li>
<li><a href="https://www.civo.com/learn/deploying-applications-through-the-civo-kubernetes-marketplace">CIVO marketplace</a></li>
<li><a href="https://www.civo.com/learn/user_guides/446">Part 1 -  Full Stack deployment</a></li>
<li><a href="https://www.civo.com/learn/user_guides/549">Part 2 -  MicroFrontend deployment</a></li>
</ol>
<h1 id="civo-notes"><a class="header" href="#civo-notes">CIVO Notes</a></h1>
<blockquote>
<p>Ideas to production, CIVO helped me to make it happen...</p>
</blockquote>
<p><strong>A rough sketch on the application</strong>
I came up with an application that help us create a book cover and add chapters to it. In order to build this, I thought of having an angular frontend that communicates to three .net core backend services. </p>
<p><strong>Login and Create a K3s cluster in CIVO</strong>
I am using my Windows Linux System to learn K8s, also in CIVO creating a k3s cluster is really fast. Once you have the cluster, using the CIVO cli, merge the kubconfig to interact with the cluster using 'kubectl' command. Also K9s tools is also very useful to navigate through the custer. A a few handy commands I will share it to set it up. Once you download the cli, refer to the Learning resource section with useful links:</p>
<pre><code>civo apikey add &lt;clustername&gt; &lt;apikey&gt;
civo apikey current &lt;clustername&gt;
civo region current NYC1
civo kubernetes ls
civo kubernetes config  &quot;PROJECT&quot;-infra --save --merge

kubectl config get-contexts
kubectl config set-context &quot;PROJECT&quot;-infra
kubectl config use-context &quot;PROJECT&quot;-infra
</code></pre>
<p><strong>Dockerize the microservices</strong></p>
<pre><code>1. Create a docker file in each microservices.
2. Build &amp; push the images to docker Hub.
3. Create a CI/CD pipeline to deploy to CIVO

docker build . -f Dockerfile -t PROJECT-web:local
docker tag &quot;PROJECT&quot;-web:local &lt;tag&gt;/ &quot;PROJECT&quot;-web:v.0.2
docker push &lt;TAG&gt;/ PROJECT-web:v.0.2
</code></pre>
<p><strong>Deploy using openfaas</strong></p>
<pre><code>1. Install the Openfaas from the CIVO marketplace.
2. Download the openfaas cli and connect to CIVO repo.
3. Push the image to the openfaas repo.

curl -sLSf https://cli.openfaas.com | sudo sh
export OPENFAAS_PREFIX=&quot;&lt;tag&gt;/&quot;
export DNS=&quot;&lt;YOUR_CIVO_CLUSTER_ID&gt;.k8s.civo.com&quot; # As per dashboard
export OPENFAAS_URL=http://$DNS:31112
PASSWORD=$(kubectl get secret -n openfaas basic-auth -o jsonpath=&quot;{.data.basic-auth-password}&quot; | base64 --decode; echo)
echo -n $PASSWORD | faas-cli login --username admin --password-stdin

faas-cli new --lang dockerfile api
faas-cli build
faas-cli push -f stack.yml # Contains all the image deployment
faas-cli deploy -f stack.idserver.yml # individual deployment
faas-cli deploy -f stack.web.yml
</code></pre>
<p><strong>Deploy using helm</strong></p>
<pre><code>1. Create a helm chart
2. Mention the docker image for the installation
3. Helm install to the CIVO cluster

helm upgrade --install &quot;PROJECT&quot;-frontend /&quot;PROJECT&quot;-web/conf/charts/&quot;PROJECT&quot;-ui --namespace PROJECT --set app.image=&lt;TAG&gt;/&quot;PROJECT&quot;-web:latest
helm uninstall &quot;PROJECT&quot;-frontend -n &quot;PROJECT&quot;
</code></pre>
<p><strong>Setup the SSL certificates us Let's Encrypt</strong></p>
<pre><code>1. Create an LetsEncrypt PROD issuer.
2. Deploy the ingress.
3. Troubleshoot and verify the certificated is issued.

kubectl apply -f ./cert-manager/civoissuer.stage.yaml
issuer.cert-manager.io/letsencrypt-stage created
kubectl apply -f ingress-cert-civo.yaml

kubectl get issuer -n kitchen 
kubectl get ing -n kitchen
kubectl get certificates -n kitchen
kubectl get certificaterequest -n kitchen 
kubectl describe order  -n kitchen
kubectl describe challenges -n kitchen
</code></pre>
<p><strong>Show off</strong>
I used to document the process and steps I have done during the development &amp; deployment. It will help us to review and refine it as we progress through our development</p>
<h2 id="rancher-shared-service-using-kubernauts--popeye-setup"><a class="header" href="#rancher-shared-service-using-kubernauts--popeye-setup">Rancher Shared Service using Kubernauts &amp; Popeye setup</a></h2>
<p><strong>Get the free life time access to RaaS from https://kubernautic.com/</strong></p>
<pre><code>kubectl apply -f https://rancher.kubernautic.com/v3/import/&lt;youraccesskey&gt;.yaml
clusterrole.rbac.authorization.k8s.io/proxy-clusterrole-kubeapiserver created
clusterrolebinding.rbac.authorization.k8s.io/proxy-role-binding-kubernetes-master created
namespace/cattle-system created
serviceaccount/cattle created
clusterrolebinding.rbac.authorization.k8s.io/cattle-admin-binding created
secret/cattle-credentials-c9b86c5 created
clusterrole.rbac.authorization.k8s.io/cattle-admin created
deployment.apps/cattle-cluster-agent created

wget https://github.com/derailed/popeye/releases/download/v0.9.0/popeye_Linux_x86_64.tar.gz
tar -xvf  popeye_Linux_x86_64.tar.gz
mv popeye /usr/local/bin/

POPEYE_REPORT_DIR=/mnt/e/Kubernetes/ popeye --save --out html --output-file report.html
</code></pre>
<h2 id="resources-4"><a class="header" href="#resources-4">Resources</a></h2>
<ul>
<li>
<p><a href="https://kubernautic.com/">Rancher Shared Service</a></p>
</li>
<li>
<p><a href="https://popeyecli.io/">popeye</a></p>
</li>
</ul>
<h1 id="crashloopbackoff"><a class="header" href="#crashloopbackoff">CrashLoopBackOff</a></h1>
<h1 id="100days-resources-30"><a class="header" href="#100days-resources-30">100Days Resources</a></h1>
<ul>
<li><a href="https://youtu.be/-ypljeZrSgc">Video by Anais Urlichs</a></li>
<li>Add your blog posts, videos etc. related to the topic here!</li>
</ul>
<h1 id="learning-resources-30"><a class="header" href="#learning-resources-30">Learning Resources</a></h1>
<ul>
<li>What is Kubernetes: <a href="https://youtu.be/VnvRFRk_51k">https://youtu.be/VnvRFRk_51k</a></li>
<li>Kubernetes architecture explained: <a href="https://youtu.be/umXEmn3cMWY">https://youtu.be/umXEmn3cMWY</a></li>
</ul>
<h1 id="example-notes-29"><a class="header" href="#example-notes-29">Example Notes</a></h1>
<p>Today I actually spent my time first learning about ReplicaSets. However, when looking at my cluster, I have noticed something strange.</p>
<p>There was one pod still running that I thought I had deleted yesterday.</p>
<p>The status of the pod indicated &quot;CrashLoopBackOff&quot;; meaning the pod would fail every time it was trying to start. It will start, fail, start fail, start f...</p>
<p>This is quite common, and in case the restart policy of the pod is set to always, Kubernetes will try to restart the pod every time it has an error.</p>
<p><strong>There are several reasons why the pod would end up in this poor state:</strong></p>
<ol>
<li>Something is wrong within our Kubernetes cluster</li>
<li>The pod is configured correctly</li>
<li>Something is wrong with the application</li>
</ol>
<p>In this case, it was easier to identify what had gotten wrong that resulted in this misbehaved pod.</p>
<p>Like mentioned in previous days, there are multiple ways that one can create a pod. This can be categorized roughly into </p>
<ul>
<li>Imperative: We have to tell Kubernetes each step that it has to do within our cluster</li>
<li>Declarative: We provide Kubernetes with any resource definition that we want to set-up within our pod and it will figure out the steps that are needed to make it happen</li>
</ul>
<p>As part of yesterdays learning, I tried to set-up a container image inside a pod and inside our cluster with the following command:</p>
<p><em>kubectl create deployment --image=<name of the image></em></p>
<p>this will create a deployment resource, based on which a pod is created, based on which a container is run within the created pod.</p>
<p>Going back to reasons this might have resulted in a CrashLoopBackOff; I don't have reasons to believe that something is wrong within our cluster since the same image was created using a pod definition in a declarative format — that worked.</p>
<p>Next, the pod could be configured correctly. This could have been the case — However, since we did not tell Kubernetes explicitly how it should create the pod, we don't have much control over this aspect.</p>
<p>Lastly, the application inside of the container is wrong. I have reason to believe that this is the case. When parsing the container image into the Kubernetes cluster, we did not provide any arguments. However, the container image would have needed an argument to know which image it is actually supposed to run. Thus, I am settling for this explanation.</p>
<p>Now how do we get rid of the pod or correct this?</p>
<p>I first tried to delete the pod with</p>
<pre><code class="language-jsx">kubectl delete pod &lt;name of the pod&gt;
</code></pre>
<p>However, this just meant that the current instance of the pod was deleted and a new one created each time — counting the restarts from 0.</p>
<p>So it must be that there is another resource that tells Kubernetes &quot;create this pod and run the container inside&quot;</p>
<p>Let's take a look at the original command:</p>
<p><em>kubectl create deployment --image=<name of the image></em></p>
<p>I had literally told Kubernetes to create a deployment. SO let's check for deployment:</p>
<pre><code class="language-jsx">kubectl get deployment
</code></pre>
<p>and tada, here is the fish.</p>
<p>Since we have already tried to delete the pod, we will now delete the deployment itself.</p>
<pre><code class="language-jsx">kubectl delete deployment &lt;name of the deployment&gt;
</code></pre>
<p>When we are now looking at our pods</p>
<pre><code class="language-jsx">kubectl get pods
</code></pre>
<p>We should not see any more pods listed.</p>
<h1 id="terminology-primer"><a class="header" href="#terminology-primer">Terminology Primer</a></h1>
<p>This section is intended as a quick primer to get you up to speed with the various terms and acronyms that you are likely to encounter as you begin your Kubernetes journey. This is by no means an exhaustive list and it will certainly evolve.</p>
<ul>
<li>
<p><strong>Containers:</strong> Containers are a standard package of software that allows you to bundle (or package) an application's code, dependencies, and configuration into a single object which can then be deployed in any environment. Containers isolate the software from its underlying environment.</p>
</li>
<li>
<p><strong>Container Image:</strong> According to <a href="https://www.docker.com/resources/what-container">Docker</a>, a Container Image <em>&quot;is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries, and settings.&quot;</em> Container Images become Containers at runtime.</p>
</li>
<li>
<p><strong>Container Registry:</strong> A container registry is a repository for storing all of your container images. Examples of containers registries are: 'Azure Container Registry', 'Docker Hub', and 'Amazon Elastic Container Registry'.</p>
</li>
<li>
<p><strong>Docker:</strong> Docker is an open-source platform used for automating the deployment of containerized applications.</p>
</li>
<li>
<p><strong>Kubernetes:</strong> Also know as <strong>K8s</strong>, Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. Please see <a href="https://kubernetes.io/">kubernetes.io</a> for more details. It could also be thought of as a 'Container Orchestrator'.</p>
</li>
<li>
<p><strong>Control Plane:</strong> The control plane is the container orchestration layer. It exposes an API that allows you to manage your cluster and its resources.</p>
</li>
<li>
<p><strong>Namespaces:</strong> Namespaces are units of the organization. They allow you to group related resources.</p>
</li>
<li>
<p><strong>Nodes:</strong> Nodes are worker machines in Kubernetes. Nodes can be virtual or physical. Kubernetes runs workloads by placing containers into Pods that run on Nodes. You will typically have multiple nodes in your Kubernetes cluster.</p>
</li>
<li>
<p><strong>Pods:</strong> Pods are the smallest deployable unit of computing that you can create and manage in Kubernetes. A pod is a group of one or more containers.</p>
</li>
<li>
<p><strong>Service:</strong> A Service in Kubernetes is a networking abstraction for Pod access. It handles the network traffic to a Pod or set of Pods.</p>
</li>
<li>
<p><strong>Cluster:</strong> A Kubernetes Cluster is a set of nodes.</p>
</li>
<li>
<p><strong>Replica Sets:</strong> A Replica Set works to ensure that the defined number of Pods are running in the Cluster at all times.</p>
</li>
<li>
<p><strong>Kubectl:</strong> Kubectl is a command-line tool for interacting with a Kubernetes API Server to manage your Cluster.</p>
</li>
<li>
<p><strong>etcd:</strong> Etcd is a key-value store that Kubernetes uses to store Cluster data.</p>
</li>
<li>
<p><strong>Ingress:</strong> An object that manages external access to the services running on the Cluster.</p>
</li>
<li>
<p><strong>K3s:</strong> K3s is a lightweight Kubernetes distribution designed for IoT or Edge computing scenarios.</p>
</li>
<li>
<p><strong>GitOps:</strong> According to <a href="https://codefresh.io/gitops/">Codefresh</a>, <em>&quot;GitOps is a set of best-practices where the entire code delivery process is controlled via Git, including infrastructure and application definition as code and automation to complete updates and rollbacks.&quot;</em></p>
</li>
<li>
<p><strong>Containerd:</strong> Containerd is a container runtime that manages the complete container lifecycle.</p>
</li>
<li>
<p><strong>Service Mesh:</strong> According to <a href="https://istio.io/latest/docs/concepts/what-is-istio/">Istio</a>, a Service Mesh describes the network of micro-services within an application and the communications between them.</p>
</li>
<li>
<p><strong>AKS:</strong> Azure Kubernetes Service (AKS) is a managed, hosted, Kubernetes service provided by Microsoft Azure. A lot of the management of your Kubernetes cluster is abstracted away and managed by Azure. Find out more <a href="https://docs.microsoft.com/en-us/azure/aks/intro-kubernetes">here</a>.</p>
</li>
<li>
<p><strong>EKS:</strong> Elastic Kubernetes Service (EKS), provided by Amazon AWS, is a managed, hosted, Kubernetes service. Similar to AKS, much of the management is abstracted away and managed by the cloud provider. Find out more <a href="https://aws.amazon.com/eks">here</a></p>
</li>
<li>
<p><strong>GKE:</strong> Google Kubernetes Engine (GKE), is another managed Kubernetes service. This time from Google. Find out more <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/kubernetes-engine-overview">here</a>.</p>
</li>
<li>
<p><strong>Helm:</strong> Helm can be thought of as a Package Manager for Kubernetes.</p>
</li>
<li>
<p><strong>Helm Chart:</strong> Helm Charts are YAML files that define, install and upgrade Kubernetes applications.</p>
</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
        
        

    </body>
</html>
